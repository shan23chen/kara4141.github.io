<h1>Chapter 13</h1>
<p class="chapter-subtitle"><span>General Discussion and Future Perspectives</span></p>
<h2>Summary</h2>
<p><span>Background<br/></span><span>This chapter synthesizes the findings across Parts I and II, integrating the demonstrated utilities and exposed vulnerabilities of large language models in clinical settings into a unified perspective on the future of medical AI.</span></p>
<p><span>Findings<br/></span><span>Part I showed that LLMs deliver measurable clinical value—from patient communication to autonomous trial safety agents—but require rigorous oversight. Part II revealed systemic failures: pre-training bias, lexical fragility, sycophantic behavior, and evaluation gaps that undermine reliability. Together, these findings argue that safe clinical AI requires last-mile alignment grounded in local data, domain-specific ontologies, and transparent evaluation, not simply larger models.</span></p>
<p><span>Interpretation<br/></span><span>The path to trustworthy clinical AI depends on interdisciplinary collaboration, robust evaluation frameworks, and purpose-built safeguards. This thesis provides foundational evidence, benchmarks, and datasets to guide development toward systems that strengthen the safety, equity, and integrity of healthcare.</span></p>
<h2>Part I: Potential Utilities of Language Models in Real Clinical Practice</h2><h3>Patient-Facing Information and Clinical Communication</h3><p><span>The first section of the thesis examines the most immediate interface between LLMs and public health: the generation of medical information for patients and the drafting of clinician messages. </span><span>Chapters 2 and 3</span><span> address these domains through complementary perspectives.</span></p><p><span>In </span><span>Chapter 2</span><span>, we demonstrate that although ChatGPT delivers guideline-concordant cancer treatment recommendations in 98% of cases, one-third of responses contain partially incorrect statements and 12% hallucinate entire treatment modalities. Performance varies markedly with prompt phrasing, highlighting the fragility of general-purpose LLMs when used by patients without expert oversight. These findings reinforce the need for verification layers, guardrails, and clinician involvement in any patient-facing deployment.</span></p><p><span>Chapter 3</span><span> transitions from public use to clinical workflows, evaluating GPT-4-assisted drafting of portal messages. While clinicians report increased efficiency in 77% of cases, unedited drafts introduce a 7.7% risk of severe harm due to misclassification of urgency or inappropriate clinical advice. Across both chapters, a central theme emerges: LLMs provide value primarily as tools that augment expert decision-making. Their safe and effective use depends on human vigilance, not autonomous output.</span></p><h3>Unlocking Unstructured Clinical Data</h3><p><span>Chapters 4 and 5</span><span> shift focus from communication to structured extraction of clinically meaningful but routinely underutilized information.</span></p><p><span>Radiotherapy-induced esophagitis severity, a toxicity with direct survival implications, is recorded only in narrative notes. Our fine-tuned models achieved macro-F1 scores up to 0.92 for CTCAE grade extraction, enabling large-scale real-world evidence generation where none previously existed.</span></p><p><span>Similarly, social determinants of health (SDoH), powerful and often ignored predictors of outcomes, remain largely invisible in structured data. Fine-tuned Flan-T5 models identified 94% of patients with adverse SDoH compared to only 2% using ICD-10 codes, and did so with less demographic bias than GPT-4 few-shot prompting.</span></p><p><span>These findings underscore a central message: </span><span>task-specific fine-tuning on domain data is not only more accurate but also more equitable</span><span>, surfacing disparities that generalist models miss. Future directions include expanding extraction to additional toxicity domains, embedding these pipelines in prospective registries, and evaluating their impact on clinical operations such as care navigation and resource allocation.</span></p><h3>Autonomous Agents in Trial Safety</h3><p><span>The final arc of Part I culminates in </span><span>Chapter 6</span><span>, where we introduce </span><span>iRAE Agent</span><span>, an agentic system piloted at Mass General Brigham for detecting immunotherapy-related adverse events in oncology trials. Unlike static classifiers, iRAE performs multi-step reasoning, structured error checking, and ontology-guided reporting—operations that map directly to real trial workflows and carry substantial regulatory significance.</span></p><p><span>This represents a true inflection point: moving from isolated extraction tasks to </span><span>full-fledged clinical agents</span><span> designed for deployment in safety-critical environments. Future work includes scaling iRAE across trial networks, integrating it with regulatory submission pipelines, and designing human-in-the-loop interfaces that preserve clinician oversight while automating routine surveillance. Robustness to linguistic variation and resistance to sycophantic agreement—fully explored in Part II—will be essential for trustworthy deployment.</span></p><h2>Part II: Potential Failures of Language Models in Medical Settings</h2><p><span>While Part I demonstrates the considerable promise of LLMs for clinical augmentation, Part II examines the structural vulnerabilities that emerge when these systems are used in safety-critical environments. These chapters collectively reveal that the same statistical learning mechanisms enabling LLMs to generalize across vast text corpora also produce brittle, easily perturbed behaviors when models are asked to perform medical reasoning.</span></p><p><span>Chapter 7</span><span> establishes a foundational principle for responsible deployment: generalist LLMs are not necessarily the right tool for most biomedical NLP tasks. Contrary to assumptions that larger models trained on broader corpora inherently perform better, our findings show that task-specific fine-tuned models not only outperform GPT-3.5 and GPT-4 in structured extraction tasks but do so while using dramatically less compute. These domain-tuned models demonstrate greater stability, interpretability, and consistency—qualities that are essential for clinical operations. The presumed “generalist advantage” dissolves under empirical scrutiny, highlighting the need to calibrate model choice to the specific constraints of healthcare settings rather than defaulting to the largest available system.</span></p><p><span>Chapters 8 through 10</span><span> reveal deeper epistemic failures rooted in pre-training data distributions. In </span><span>Chapter 8</span><span>, the Cross-Care study demonstrates that when LLMs are asked to estimate disease prevalence, their responses track the frequency with which diseases co-occur in The Pile—an English-dominant, internet-derived dataset—rather than actual epidemiological data. This pattern persists across languages and alignment strategies, suggesting that these models internally encode </span><span>statistical reflections of their training corpora rather than medically grounded representations of disease burden</span><span>. This representational bias carries profound implications for any task requiring epidemiological awareness or population-level reasoning.</span></p><p><span>The fragility exposed in </span><span>Chapter 9</span><span> underscores a parallel lexical vulnerability. Although LLMs nominally “know” that brand and generic drug names refer to the same pharmacologic substance, substituting one for the other reduces accuracy by up to 10%. This mismatch reflects a model’s reliance on lexical co-occurrence patterns rather than a robust conceptual representation of drug equivalence. In clinical practice, where generic versus brand names vary across notes, specialties, and institutions, such brittleness introduces unpredictable and potentially dangerous inconsistency.</span></p><p><span>Chapter 10</span><span> adds a behavioral dimension to these representational failures, demonstrating that LLMs frequently acquiesce to false claims about drug equivalence when such claims are embedded in user prompts. This sycophantic tendency—where models prioritize agreement and apparent helpfulness over factual accuracy—can lead to confident propagation of medical misinformation. These behaviors highlight the interface between alignment methods and clinical risk: systems optimized to sound polite, cooperative, or user-friendly can become hazardous when those attributes override epistemic integrity.</span></p><p><span>Together, the findings of Part II paint a cautionary but constructive picture: current LLMs are not yet reliable arbiters of medical knowledge. Their susceptibility to pre-training biases, lexical perturbations, not robust knowledge representation and misalignment indicates that naïve deployment in healthcare settings could amplify existing inequities and introduce new potential of harm. These vulnerabilities underscore the need for rigorous evaluation frameworks and purpose-built safeguards before clinical integration.</span></p><h3>Societal Impact and Future Directions</h3><p><span>The broader societal significance of this thesis lies in its insistence that clinical AI must be evaluated not only on technical performance but on its alignment with the practical, ethical, and equity-oriented demands of real-world medicine. By grounding every experiment in live workflows—patient messaging, toxicity surveillance, trial safety monitoring—this work demonstrates that LLMs tested on sanitized benchmarks or synthetic tasks provide less insight into their behavior under true clinical conditions. In contrast, models evaluated within authentic operational settings reveal the constraints, failure modes, and friction points that determine whether AI tools will meaningfully improve care.</span></p><p><span>A key contribution of this thesis is the demonstration that </span><span>last-mile alignment—not scale—is the decisive factor in achieving clinically meaningful AI</span><span>. While foundation models provide broad linguistic competence, their raw form is insufficient for safe deployment in real care settings. The most clinically relevant value emerges when models undergo </span><span>last-mile alignment</span><span>: targeted fine-tuning, calibration, and behavioral optimization using local data, domain-specific ontologies, and institution-defined safety constraints. This final stage transforms general capabilities into reliable, workflow-ready tools that reflect the actual decision environments clinicians navigate.</span></p><p><span>Crucially, last-mile alignment is inherently </span><span>interdisciplinary</span><span>. It requires domain experts to define what “correct” looks like, informaticians to validate model behavior across heterogeneous EHR data streams, ethicists to assess potential inequities or normative risks, and regulatory specialists to ensure compliance with evolving standards. Smaller, fine-tuned models enable this collaboration in ways that large generalist models cannot: they are transparent enough to audit, modular enough to modify, and lightweight enough to iterate on quickly and safely. In this sense, the path to trustworthy clinical AI is not dominated by ever-larger pre-trained systems but by </span><span>collaboratively aligned models that undergo rigorous, locally grounded last-mile refinement</span><span>.</span></p><p><span>The societal implications of the extraction work presented in Part I are substantial. By automating the identification of radiation toxicities and social determinants of health, this research enables the generation of real-world evidence that had previously remained inaccessible. These capabilities can support more equitable distribution of clinical resources, improve trial design, and uncover disparities hidden within documentation practices. They also create opportunities to inform public health interventions, policy design, and health equity efforts at scale.</span></p><p><span>In parallel, the failure patterns exposed in Part II offer concrete pathways for developers, regulators, and healthcare organizations to identify, mitigate, and monitor sources of clinical risk. Pre-training bias, drug-name brittleness, and sycophantic behavior are not abstract deficiencies; they are failure modes with direct implications for patient safety and public trust. By systematically characterizing these vulnerabilities, the thesis provides a roadmap for model improvement and governance frameworks that prioritize factual grounding, robustness, and ethical responsibility.</span></p><p><span>Finally, the release of </span><span>WorldMedQA-V</span><span> and </span><span>MedBrowseComp</span><span> contributes durable, publicly accessible infrastructure for assessing multilingual performance and hard yet realistic medical reasoning. These benchmarks allow the community to measure whether future models genuinely progress toward equitable, evidence-grounded clinical intelligence—or merely optimize for English-dominant, shallow tasks that reproduce existing inequities. In a field where three-quarters of published studies still lack reproducibility or diverse validation, such shared infrastructure is essential for elevating scientific standards and enabling meaningful regulatory oversight.</span></p><p><span>Looking forward, the future of clinical AI will depend not on scale alone but on rigorous scientific methodology, transparent evaluation pipelines, and thoughtful integration with human expertise. LLMs that meaningfully improve healthcare will be those that earn trust—through robustness, fairness, and verifiable correctness—rather than those that simply demonstrate linguistic sophistication. This thesis lays foundational evidence, frameworks, and datasets to guide the development of such systems, and points toward a future in which clinical AI strengthens—not destabilizes—the safety, equity, and integrity of global health.</span></p>