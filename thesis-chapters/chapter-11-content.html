<h1>Chapter 11</h1><p class="chapter-subtitle"><span>WorldMedQA-V: a Multilingual, Multimodal Medical Examination Dataset for Multimodal Language Models Evaluation</span></p><hr/><p><span>Shan Chen*</span><span>, João Matos*, Siena Kathleen V. Placino, Yingya Li, Juan Carlos Climent Pardo, Daphna Idan, Takeshi Tohyama, David Restrepo, Luis Filipe Nakayama, José María Millet Pascual-Leone, Guergana K Savova, Hugo Aerts, Leo Anthony Celi, An-Kwok Ian Wong, Danielle Bitterman, Jack Gallifant</span></p><p class="chapter-meta"><a href="https://aclanthology.org/2025.findings-naacl.402/" rel="noopener noreferrer" target="_blank"><em>Findings of the Association for Computational Linguistics: NAACL 2025</em></a></p><h2>Summary</h2><p><span>Background<br/></span><span>Vision–language models (VLMs) are increasingly employed in healthcare settings, yet existing evaluation benchmarks are mostly monolingual, text-only, and often derived from a small number of countries. This limits our understanding of how well VLMs perform in diverse clinical and linguistic contexts.</span></p><p><span>Methods<br/></span><span>We present WorldMedQA‑V — a multilingual, multimodal medical exam dataset comprising 568 clinician-validated multiple-choice questions, each paired with a medical image, collected from four countries (Brazil, Israel, Japan, Spain) in both the local language and native English translations. They evaluated ten state-of-the-art VLMs (including GPT4o, Gemini, LLaVA variants) on both text-only and text+image inputs, measuring accuracy and consistency across languages, and calculating agreement (Cohen’s Kappa) between predictions in the local and English versions.</span></p><p><span>Findings<br/></span><span>Performance was substantially lower than prior single-language medical benchmarks: even the best model, GPT4o, failed to pass the ‘exam’ threshold for several datasets (e.g., scoring 58% on the Hebrew original-language subset). Models showed improved accuracy when provided with images (though gains were typically modest ­— ~1-3% for top models) and greater cross-language consistency when images were included. Moreover, performance varied dramatically by country and language: models often did better on English translations than on original languages, and the least-represented languages (e.g., Hebrew) showed the greatest performance drop.</span></p><p><span>Interpretation<br/></span><span>WorldMedQA-V highlights substantial gaps in current VLMs’ multilingual and multimodal medical reasoning capabilities. The disparity across languages and reliance on mostly English data underscore risks for equitable deployment in global healthcare contexts. The dataset provides a rigorous new benchmark to drive the development of more inclusive, clinically relevant AI systems tracking real progress.</span></p><h2>Introduction</h2><p><span>Generative artificial-intelligence (AI) models are increasingly being adopted in healthcare, highlighting the need for robust benchmarks to assess their safety, efficacy, and fairness</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/CaJ8+e3VB+yuJy+Za8R" rel="noopener noreferrer" target="_blank">1–4</a></sup></span><span>.</span></p><p><span>One of the key evaluation tasks in Natural Language Processing (NLP) is Question Answering (QA)</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/2r7Z+ZGgZ" rel="noopener noreferrer" target="_blank">5,6</a></sup></span><span>, which involves building systems that can automatically respond to human queries in natural language by combining language understanding with information retrieval</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/v2iZ" rel="noopener noreferrer" target="_blank">7</a></sup></span><span>. Multiple-choice QA benchmarks have become essential not only for evaluating large language models (LLMs) but also for assessing vision–language models (VLMs) in medicine</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/s70K" rel="noopener noreferrer" target="_blank">8</a></sup></span><span>.</span></p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image4.png" title=""/><figcaption>Figure 1. WorldMedQA-V dataset generation and evaluation workflows.</figcaption></figure><p><span>Recent research has explored the performance of LLMs in medical exams, with ChatGPT being the first AI system to pass the USMLE</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/S2t1" rel="noopener noreferrer" target="_blank">9</a></sup></span><span>, prompting further studies</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/2H7i+Mv3A+s70K" rel="noopener noreferrer" target="_blank">8,10,11</a></sup></span><span>. A recent review identified 45 studies on ChatGPT’s performance in medical exams</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/s70K" rel="noopener noreferrer" target="_blank">8</a></sup></span><span>, but VLMs remain under-explored in medical tasks</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/I2wK+WsIY" rel="noopener noreferrer" target="_blank">12,13</a></sup></span><span>. Despite progress, current models face limitations such as context fragility, biases, and inconsistent multilingual performance</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/54Vq+SrkJ+X0L4" rel="noopener noreferrer" target="_blank">14–16</a></sup></span><span>. There is also a need for more diverse datasets to ensure equitable AI evaluation in healthcare</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/4YMq" rel="noopener noreferrer" target="_blank">17</a></sup></span><span>. </span></p><p><span>Key gaps include:</span></p><p><span>• Real-world validity: studies reveal errors in existing medical-QA datasets</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/7JuZ" rel="noopener noreferrer" target="_blank">18</a></sup></span><span>. </span></p><p><span>• Linguistic diversity: many datasets lack language representation (Appendix Table 1). </span></p><p><span>• Imaging data: most medical-QA benchmarks exclude multimodal data (Appendix Table 1). </span></p><p><span>• Training-data contamination: outdated datasets may overlap with LLM/VLM training corpora</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/xt4L+Z1jA+54Vq" rel="noopener noreferrer" target="_blank">14,19,20</a></sup></span><span>.</span></p><p><span>To address these issues, we introduce WorldMedQA-V, a multilingual, multimodal dataset for evaluating language and vision models. Key contributions include:</span></p><p><span>• Multimodal medical exams from four countries, supporting local languages and English. </span></p><p><span>• Previously unseen multimodal exam questions with clinical validation by medical professionals. </span></p><p><span>• Baseline performance reporting of current state-of-the-art VLMs across languages, including an evaluation of performance differentials between local languages and English. </span></p><p><span>• An investigation into the impact of adding image data to model performance and stability across language translations.</span></p><p><span>Related Work</span></p><p><span>Recent benchmarks such as MMMU</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/1Ajy" rel="noopener noreferrer" target="_blank">21</a></sup></span><span>, MMMU-pro</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/AR7u" rel="noopener noreferrer" target="_blank">22</a></sup></span><span>, EXAMS-V</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/C99w" rel="noopener noreferrer" target="_blank">23</a></sup></span><span>, and VQArt-Bench</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/NqcC" rel="noopener noreferrer" target="_blank">24</a></sup></span><span> evaluate vision-language models (VLMs) across multiple languages and disciplines. These evaluations reveal notable performance gaps across linguistic and cultural contexts. Studies indicate that VLMs tend to perform better in English, likely due to the predominance of English-language training data</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/BOpQ" rel="noopener noreferrer" target="_blank">25</a></sup></span><span>. These findings underscore the need to improve VLM performance in diverse linguistic and cultural settings, particularly in specialized domains. Moreover, previous benchmarks contain only a limited number of health- or medical-related questions, and these are not clinically verified.</span></p><p><span>Medical datasets from these regions highlight challenges in large language model (LLM) performance within healthcare. In Asia, notable datasets originate from China, Taiwan (MedQA), South Korea (KorMedMCQA), and India (MedMCQA). The MLEC-QA dataset from China, containing 136,236 multiple-choice questions, is the largest. Despite LLMs being pre-trained on vast corpora, performance in this domain is hindered by limited diversity and quality of training data—especially for two-step reasoning and biomedical concepts</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/H5i7" rel="noopener noreferrer" target="_blank">26</a></sup></span><span>. Similar trends are observed in Taiwan and South Korea, where English-pretrained models underperform on local medical exams.</span></p><p><span>In Europe, datasets from Spain, Sweden, and Poland (the latter not publicly available) underscore the difficulties LLMs face as question complexity increases</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/aa1y" rel="noopener noreferrer" target="_blank">27</a></sup></span><span>. Recent progress shows that GPT-3.5-Turbo and GPT-4 passed the Swedish medical licensing exam</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/tzW5" rel="noopener noreferrer" target="_blank">28</a></sup></span><span>, while GPT-4-Turbo slightly outperformed humans in Poland</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/coPcdt/Re9L" rel="noopener noreferrer" target="_blank">29</a></sup></span><span>.</span></p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image28.png" title=""/><figcaption>Figure 2. Accuracy in local language and English across models and countries. The red-shaded area highlights each country's exam-passing threshold. Passing score is a proxy here since our dataset is a subset. Detailed results appear in Appendix.</figcaption></figure><h2>Methods</h2><p><span>See Figure 1 shows the overall workflow of the study.</span></p><p><span>Data Collection </span></p><p><span>Our study uses medical-exam data from Brazil, Israel, Japan, and Spain, consisting of multiple-choice questions from national licensing or specialization exams. Brazil’s dataset includes 100 questions per exam from the 2011–2016 and 2020–2024 “Revalida” exams. Israel’s dataset contains 150 questions from Phase A of the resident-certification exam (2020–2023). Japan’s data come from the 116th–118th National Medical Licensing Examinations (2022–2024), while Spain’s dataset includes questions from specialization exams (2019–2023). Further details are provided in Appendix A.</span></p><p><span>Clinical Validation </span></p><p><span>A clinical-validation process was carried out for all collected and translated data to ensure their quality and relevance. Native-speaking clinicians from each country validated the three key stages of the process—data extraction, translation, and final QA review.</span></p><p><span>Evaluation </span></p><p><span>Models: We included open- and closed-source models across a range of sizes: GPT-4o-2024-05-13, GPT-4o-mini-2024-07-18, Gemini-Flash-1.5-May, Gemini-Pro-1.5-May, LLaVA-Next-Llama3 (8 B), LLaVA-Next-Yi-34B, LLaVA-Next-Mistral-7B, LLaVA-Next-Vicuna-7B, Yi-VL-34B, and Yi-VL-6B. All models were set to generate 512 tokens, with a temperature of 0 for reproducibility, and evaluated on an Nvidia GPU with CUDA &gt; 12.0.</span></p><p><span>Experiments: The VLMEvalKit evaluation framework was utilized to conduct experiments. We evaluated the ten models with and without image input, using accuracy as the metric. Cohen’s kappa coefficients were computed to assess each model’s reliability when answering the question in the original language versus the English translation.</span></p><h3>Results &amp; Discussions</h3><p><span>Dataset</span><span> </span></p><p><span>The complete WorldMedQA-V contains 726 question–answer pairs and 850 images across four countries: Brazil, Israel, Japan, and Spain. Every QA is paired with at least one image (some images are reused). After removing items with multiple images or multiple correct options, the final evaluation subset comprises 568 QAs, each with a single image and a single correct answer. Table 1 (Appendix) gives the country- and language-level breakdown; Box 1 (Appendix) shows a Brazilian example.</span></p><p><span>VLM performance </span></p><p><span>Figure 2 summarises model accuracy on each dataset in the local language and in English. All models score well below the ~90 % USMLE figure previously reported for GPT-4. GPT-4o is the strongest model overall; it fails to reach the 70 % pass mark only on the Hebrew-language Israel set (58 %), yet passes the English translation of the same set (63 %). The other non-Roman script in the benchmark is Japanese, where GPT-4o attains 88 %—above threshold—possibly because Japanese is better represented in pre-training corpora and shares characters with Chinese. </span></p><p><span>Models generally score higher on the English translations, especially for Israel and Spain; the English Israel set still trails the others, suggesting data-level differences beyond language. In Brazil, GPT-4o scores 75 % in Portuguese and 67 % in English; in Japan, GPT-4o, GPT-4o-mini, Gemini-Flash-1.5 and Gemini-Pro-1.5 all do better in Japanese than in English, indicating strong Japanese support. The LLaVA-NeXT family produces the lowest accuracies; on the Israel dataset several variants perform near-random (24–46 %) in both Hebrew and English.</span></p><p><span>Accuracy with and without image input </span></p><p><span>Across nearly all models, accuracy improved when the image was provided. This effect was most pronounced for models with weaker baseline text-only performance. GPT-family models, in contrast, showed only marginal differences—typically around one to three percentage points—between image and text-only settings, suggesting they rely less on visual information or can compensate via strong textual reasoning. Models from the Gemini family were the most sensitive to image removal; providing images improved their accuracy by approximately 4–27%, depending on the dataset.</span></p><p><span>Models with lower overall performance, such as Yi-VL and llava-next, displayed more unstable behavior, sometimes improving and other times worsening when images were removed. Notably, Yi-VL-34B showed almost no predictive ability when images were withheld, highlighting its dependence on the visual modality.</span></p><p><span>Cross-language consistency </span></p><p><span>To evaluate stability across languages, each model’s predictions in the original language were compared to its predictions on English translations using Cohen’s kappa. GPT-4o consistently showed the strongest cross-language agreement, especially for Brazil, Japan, and Spain, and agreement was generally higher when image information was available. The highest agreement (84%) occurred in the Spanish text-only condition, likely reflecting the model’s overall high accuracy on that dataset. GPT-4o-mini and Gemini Flash 1-5 also achieved strong agreement in Brazil and Spain, though they did not match GPT-4o.</span></p><p><span>Models such as Yi-VL displayed low agreement across all countries, indicating poor cross-linguistic stability. A particularly notable case is Gemini Pro 1-5 on the Spanish dataset: its agreement increased dramatically—from 16.3% to 69.3%—when images were included. This demonstrates that visual information can act as a stabilizing factor for cross-language semantic alignment.</span></p><h2>Conclusion</h2><p><span>In this work, we introduced WorldMedQA-V, a clinically validated, multilingual, and multimodal dataset containing medical QAs and images from Brazil, Israel, Japan, and Spain. We evaluated the performance of 10 vision-language models using both local languages and English translations, revealing performance disparities across languages and demonstrating how multimodal data can enhance accuracy. Despite improvements from image-based inputs, underrepresented languages like Hebrew proved particularly challenging. Throughout the data collection process, we engaged with 27 regions and collaborated closely with local physicians to ensure clinical and contextual relevance, ultimately focusing on the four regions that met our stringent criteria of high-quality translations and robust image-based multiple-choice questions. Each question underwent rigorous review by native-speaking clinicians to ensure linguistic precision and clinical validity, making WorldMedQA-V a gold-standard benchmark. Our methodology went beyond mere data aggregation, involving meticulous curation and validation to create a resource that is both scientifically robust and practically relevant. We adopted a single-correct-answer format consistent with standardized medical exams to simplify evaluation and ensure reproducibility, yet over 95\% of evaluated models still failed to achieve passing performance—underscoring the inherent difficulty of integrating multilingual, multimodal information. Although our current dataset focuses on four regions, we remain committed to expanding its geographic scope in future iterations, particularly to include underrepresented areas such as parts of Africa and the Americas, thereby continuing to address critical gaps in the evaluation of healthcare-focused VLMs.</span></p><h2>Limitations</h2><p><span>While WorldMedQA-V represents a significant step toward creating a multilingual, multimodal benchmark for evaluating VLMs in healthcare, several limitations must be acknowledged.</span></p><p><span>First, the dataset, while carefully curated by trained physicians to ensure the validity of both questions and answers, remains relatively small. As we evaluated 568 multiple-choice questions and images, the sample size is limited in comparison to larger text-based benchmarks.</span></p><p><span>Second, the dataset only includes data from four countries: Brazil, Israel, Japan, and Spain, spanning three continents. This geographic limitation results in an underrepresentation of certain regions, particularly Africa, North and Central America, Oceania, and other parts of Asia.</span></p><p><span>Furthermore, although the benchmark introduces multimodal elements, it pairs only one image per question. Real-world clinical scenarios often involve multiple images from different time points or modalities, such as a sequence of X-rays, CT scans, and pathology slides. Another limitation is that text that is within images were not translated or adapted. English translations, although validated by native-speaking clinicians from each country, require further cross-validations, as these are typically nontrivial tasks.</span></p><p><span>Additionally, the lack of open-source multimodal medical language models restricts our ability to comprehensively evaluate and compare state-of-the-art health AI using WorldMedQA-V. Furthermore, since the models we tested were not originally trained for the medical domain, some LLMs (e.g., Gemini) refused to respond when no image was provided for certain questions, resulting in lower scores. When evaluating model performance against a passing threshold, a limitation is that our analysis relies on a limited set of multiple-choice questions with images, which may not provide consistent difficulty levels across different questions within the same exam.</span></p><p><span>Lastly, we set the underlying assumption that each question had only one correct answer, excluding cases where multiple correct answers were possible. This decision was made to simplify evaluation, but it may not reflect the inherent ambiguity and complexity found in both medical examinations and real-world medical scenarios where multiple treatment options or diagnoses can be valid.</span></p><h2>References</h2><p><span>1. </span><span><a href="http://paperpile.com/b/coPcdt/CaJ8" rel="noopener noreferrer" target="_blank">Thirunavukarasu, A. J. </a></span><span><a href="http://paperpile.com/b/coPcdt/CaJ8" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/CaJ8" rel="noopener noreferrer" target="_blank"> Large language models in medicine. </a></span><span><a href="http://paperpile.com/b/coPcdt/CaJ8" rel="noopener noreferrer" target="_blank">Nat. Med.</a></span><span><a href="http://paperpile.com/b/coPcdt/CaJ8" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/coPcdt/CaJ8" rel="noopener noreferrer" target="_blank">29</a></sup></span><span><a href="http://paperpile.com/b/coPcdt/CaJ8" rel="noopener noreferrer" target="_blank">, 1930–1940 (2023).</a></span></p><p><span>2. </span><span><a href="http://paperpile.com/b/coPcdt/e3VB" rel="noopener noreferrer" target="_blank">Clusmann, J. </a></span><span><a href="http://paperpile.com/b/coPcdt/e3VB" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/e3VB" rel="noopener noreferrer" target="_blank"> The future landscape of large language models in medicine. </a></span><span><a href="http://paperpile.com/b/coPcdt/e3VB" rel="noopener noreferrer" target="_blank">Commun. Med. (Lond.)</a></span><span><a href="http://paperpile.com/b/coPcdt/e3VB" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/coPcdt/e3VB" rel="noopener noreferrer" target="_blank">3</a></sup></span><span><a href="http://paperpile.com/b/coPcdt/e3VB" rel="noopener noreferrer" target="_blank">, 141 (2023).</a></span></p><p><span>3. </span><span><a href="http://paperpile.com/b/coPcdt/yuJy" rel="noopener noreferrer" target="_blank">Abbasian, M. </a></span><span><a href="http://paperpile.com/b/coPcdt/yuJy" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/yuJy" rel="noopener noreferrer" target="_blank"> Foundation metrics for evaluating effectiveness of healthcare conversations powered by generative AI. </a></span><span><a href="http://paperpile.com/b/coPcdt/yuJy" rel="noopener noreferrer" target="_blank">NPJ Digit. Med.</a></span><span><a href="http://paperpile.com/b/coPcdt/yuJy" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/coPcdt/yuJy" rel="noopener noreferrer" target="_blank">7</a></sup></span><span><a href="http://paperpile.com/b/coPcdt/yuJy" rel="noopener noreferrer" target="_blank">, 82 (2024).</a></span></p><p><span>4. </span><span><a href="http://paperpile.com/b/coPcdt/Za8R" rel="noopener noreferrer" target="_blank">Wiggers, K. Hugging Face releases a benchmark for testing generative AI on health tasks. </a></span><span><a href="http://paperpile.com/b/coPcdt/Za8R" rel="noopener noreferrer" target="_blank">TechCrunch</a></span><span><a href="http://paperpile.com/b/coPcdt/Za8R" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>5. </span><span><a href="http://paperpile.com/b/coPcdt/2r7Z" rel="noopener noreferrer" target="_blank">Fan, L. </a></span><span><a href="http://paperpile.com/b/coPcdt/2r7Z" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/2r7Z" rel="noopener noreferrer" target="_blank"> A bibliometric review of large language models research from 2017 to 2023. </a></span><span><a href="http://paperpile.com/b/coPcdt/2r7Z" rel="noopener noreferrer" target="_blank">arXiv [cs.DL]</a></span><span><a href="http://paperpile.com/b/coPcdt/2r7Z" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>6. </span><span><a href="http://paperpile.com/b/coPcdt/ZGgZ" rel="noopener noreferrer" target="_blank">Yu, H. </a></span><span><a href="http://paperpile.com/b/coPcdt/ZGgZ" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/ZGgZ" rel="noopener noreferrer" target="_blank"> Large Language Models in biomedical and Health Informatics: A review with bibliometric analysis. </a></span><span><a href="http://paperpile.com/b/coPcdt/ZGgZ" rel="noopener noreferrer" target="_blank">arXiv [cs.DL]</a></span><span><a href="http://paperpile.com/b/coPcdt/ZGgZ" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>7. </span><span><a href="http://paperpile.com/b/coPcdt/v2iZ" rel="noopener noreferrer" target="_blank">Jin, D. </a></span><span><a href="http://paperpile.com/b/coPcdt/v2iZ" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/v2iZ" rel="noopener noreferrer" target="_blank"> What disease does this patient have? A large-scale open domain question answering dataset from medical exams. </a></span><span><a href="http://paperpile.com/b/coPcdt/v2iZ" rel="noopener noreferrer" target="_blank">Appl. Sci. (Basel)</a></span><span><a href="http://paperpile.com/b/coPcdt/v2iZ" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/coPcdt/v2iZ" rel="noopener noreferrer" target="_blank">11</a></sup></span><span><a href="http://paperpile.com/b/coPcdt/v2iZ" rel="noopener noreferrer" target="_blank">, 6421 (2021).</a></span></p><p><span>8. </span><span><a href="http://paperpile.com/b/coPcdt/s70K" rel="noopener noreferrer" target="_blank">Liu, M. </a></span><span><a href="http://paperpile.com/b/coPcdt/s70K" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/s70K" rel="noopener noreferrer" target="_blank"> Performance of ChatGPT across different versions in medical licensing examinations worldwide: Systematic review and meta-analysis. </a></span><span><a href="http://paperpile.com/b/coPcdt/s70K" rel="noopener noreferrer" target="_blank">J. Med. Internet Res.</a></span><span><a href="http://paperpile.com/b/coPcdt/s70K" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/coPcdt/s70K" rel="noopener noreferrer" target="_blank">26</a></sup></span><span><a href="http://paperpile.com/b/coPcdt/s70K" rel="noopener noreferrer" target="_blank">, e60807 (2024).</a></span></p><p><span>9. </span><span><a href="http://paperpile.com/b/coPcdt/S2t1" rel="noopener noreferrer" target="_blank">Kung, T. H. </a></span><span><a href="http://paperpile.com/b/coPcdt/S2t1" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/S2t1" rel="noopener noreferrer" target="_blank"> Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. </a></span><span><a href="http://paperpile.com/b/coPcdt/S2t1" rel="noopener noreferrer" target="_blank">PLOS Digit Health</a></span><span><a href="http://paperpile.com/b/coPcdt/S2t1" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/coPcdt/S2t1" rel="noopener noreferrer" target="_blank">2</a></sup></span><span><a href="http://paperpile.com/b/coPcdt/S2t1" rel="noopener noreferrer" target="_blank">, e0000198 (2023).</a></span></p><p><span>10. </span><span><a href="http://paperpile.com/b/coPcdt/2H7i" rel="noopener noreferrer" target="_blank">Rodrigues Alessi, M. </a></span><span><a href="http://paperpile.com/b/coPcdt/2H7i" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/2H7i" rel="noopener noreferrer" target="_blank"> Comparative performance of medical students, ChatGPT-3.5 and ChatGPT-4.0 in answering questions from a Brazilian national medical exam: Cross-sectional questionnaire study. </a></span><span><a href="http://paperpile.com/b/coPcdt/2H7i" rel="noopener noreferrer" target="_blank">JMIR AI</a></span><span><a href="http://paperpile.com/b/coPcdt/2H7i" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/coPcdt/2H7i" rel="noopener noreferrer" target="_blank">4</a></sup></span><span><a href="http://paperpile.com/b/coPcdt/2H7i" rel="noopener noreferrer" target="_blank">, e66552 (2025).</a></span></p><p><span>11. </span><span><a href="http://paperpile.com/b/coPcdt/Mv3A" rel="noopener noreferrer" target="_blank">Chen, S. </a></span><span><a href="http://paperpile.com/b/coPcdt/Mv3A" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/Mv3A" rel="noopener noreferrer" target="_blank"> Evaluating the ChatGPT family of models for biomedical reasoning and classification. </a></span><span><a href="http://paperpile.com/b/coPcdt/Mv3A" rel="noopener noreferrer" target="_blank">J. Am. Med. Inform. Assoc.</a></span><span><a href="http://paperpile.com/b/coPcdt/Mv3A" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/coPcdt/Mv3A" rel="noopener noreferrer" target="_blank">31</a></sup></span><span><a href="http://paperpile.com/b/coPcdt/Mv3A" rel="noopener noreferrer" target="_blank">, 940–948 (2024).</a></span></p><p><span>12. </span><span><a href="http://paperpile.com/b/coPcdt/I2wK" rel="noopener noreferrer" target="_blank">Yan, Z. </a></span><span><a href="http://paperpile.com/b/coPcdt/I2wK" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/I2wK" rel="noopener noreferrer" target="_blank"> Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V. </a></span><span><a href="http://paperpile.com/b/coPcdt/I2wK" rel="noopener noreferrer" target="_blank">arXiv [cs.CV]</a></span><span><a href="http://paperpile.com/b/coPcdt/I2wK" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>13. </span><span><a href="http://paperpile.com/b/coPcdt/WsIY" rel="noopener noreferrer" target="_blank">Wu, C. </a></span><span><a href="http://paperpile.com/b/coPcdt/WsIY" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/WsIY" rel="noopener noreferrer" target="_blank"> Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis. </a></span><span><a href="http://paperpile.com/b/coPcdt/WsIY" rel="noopener noreferrer" target="_blank">arXiv [cs.CV]</a></span><span><a href="http://paperpile.com/b/coPcdt/WsIY" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>14. </span><span><a href="http://paperpile.com/b/coPcdt/54Vq" rel="noopener noreferrer" target="_blank">Gallifant, J. </a></span><span><a href="http://paperpile.com/b/coPcdt/54Vq" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/54Vq" rel="noopener noreferrer" target="_blank"> Language models are surprisingly fragile to drug names in biomedical benchmarks. </a></span><span><a href="http://paperpile.com/b/coPcdt/54Vq" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/coPcdt/54Vq" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>15. </span><span><a href="http://paperpile.com/b/coPcdt/SrkJ" rel="noopener noreferrer" target="_blank">Zack, T. </a></span><span><a href="http://paperpile.com/b/coPcdt/SrkJ" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/SrkJ" rel="noopener noreferrer" target="_blank"> Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a model evaluation study. </a></span><span><a href="http://paperpile.com/b/coPcdt/SrkJ" rel="noopener noreferrer" target="_blank">Lancet Digit. Health</a></span><span><a href="http://paperpile.com/b/coPcdt/SrkJ" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/coPcdt/SrkJ" rel="noopener noreferrer" target="_blank">6</a></sup></span><span><a href="http://paperpile.com/b/coPcdt/SrkJ" rel="noopener noreferrer" target="_blank">, e12–e22 (2024).</a></span></p><p><span>16. </span><span><a href="http://paperpile.com/b/coPcdt/X0L4" rel="noopener noreferrer" target="_blank">Chen, S. </a></span><span><a href="http://paperpile.com/b/coPcdt/X0L4" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/X0L4" rel="noopener noreferrer" target="_blank"> Cross-Care: Assessing the healthcare implications of pre-training data on language model bias. </a></span><span><a href="http://paperpile.com/b/coPcdt/X0L4" rel="noopener noreferrer" target="_blank">Neural Inf Process Syst</a></span><span><a href="http://paperpile.com/b/coPcdt/X0L4" rel="noopener noreferrer" target="_blank"> </a></span><span><a href="http://paperpile.com/b/coPcdt/X0L4" rel="noopener noreferrer" target="_blank">abs/2405.05506</a></span><span><a href="http://paperpile.com/b/coPcdt/X0L4" rel="noopener noreferrer" target="_blank">, 23756–23795 (2024).</a></span></p><p><span>17. </span><span><a href="http://paperpile.com/b/coPcdt/4YMq" rel="noopener noreferrer" target="_blank">Restrepo, D. </a></span><span><a href="http://paperpile.com/b/coPcdt/4YMq" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/4YMq" rel="noopener noreferrer" target="_blank"> Analyzing Diversity in Healthcare LLM Research: A Scientometric Perspective. </a></span><span><a href="http://paperpile.com/b/coPcdt/4YMq" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/coPcdt/4YMq" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>18. </span><span><a href="http://paperpile.com/b/coPcdt/7JuZ" rel="noopener noreferrer" target="_blank">Saab, K. </a></span><span><a href="http://paperpile.com/b/coPcdt/7JuZ" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/7JuZ" rel="noopener noreferrer" target="_blank"> Capabilities of Gemini models in medicine. </a></span><span><a href="http://paperpile.com/b/coPcdt/7JuZ" rel="noopener noreferrer" target="_blank">arXiv [cs.AI]</a></span><span><a href="http://paperpile.com/b/coPcdt/7JuZ" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>19. </span><span><a href="http://paperpile.com/b/coPcdt/xt4L" rel="noopener noreferrer" target="_blank">Zhang, A. K. </a></span><span><a href="http://paperpile.com/b/coPcdt/xt4L" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/xt4L" rel="noopener noreferrer" target="_blank"> Language model developers should report train-test overlap. </a></span><span><a href="http://paperpile.com/b/coPcdt/xt4L" rel="noopener noreferrer" target="_blank">arXiv [cs.LG]</a></span><span><a href="http://paperpile.com/b/coPcdt/xt4L" rel="noopener noreferrer" target="_blank"> (2025).</a></span></p><p><span>20. </span><span><a href="http://paperpile.com/b/coPcdt/Z1jA" rel="noopener noreferrer" target="_blank">Zhang, H. </a></span><span><a href="http://paperpile.com/b/coPcdt/Z1jA" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/Z1jA" rel="noopener noreferrer" target="_blank"> A careful examination of large language model performance on grade school arithmetic. </a></span><span><a href="http://paperpile.com/b/coPcdt/Z1jA" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/coPcdt/Z1jA" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>21. </span><span><a href="http://paperpile.com/b/coPcdt/1Ajy" rel="noopener noreferrer" target="_blank">Yue, X. </a></span><span><a href="http://paperpile.com/b/coPcdt/1Ajy" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/1Ajy" rel="noopener noreferrer" target="_blank"> MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. </a></span><span><a href="http://paperpile.com/b/coPcdt/1Ajy" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/coPcdt/1Ajy" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>22. </span><span><a href="http://paperpile.com/b/coPcdt/AR7u" rel="noopener noreferrer" target="_blank">Yue, X. </a></span><span><a href="http://paperpile.com/b/coPcdt/AR7u" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/AR7u" rel="noopener noreferrer" target="_blank"> MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark. in </a></span><span><a href="http://paperpile.com/b/coPcdt/AR7u" rel="noopener noreferrer" target="_blank">Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></span><span><a href="http://paperpile.com/b/coPcdt/AR7u" rel="noopener noreferrer" target="_blank"> 15134–15186 (Association for Computational Linguistics, Stroudsburg, PA, USA, 2025).</a></span></p><p><span>23. </span><span><a href="http://paperpile.com/b/coPcdt/C99w" rel="noopener noreferrer" target="_blank">Das, R. J. </a></span><span><a href="http://paperpile.com/b/coPcdt/C99w" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/C99w" rel="noopener noreferrer" target="_blank"> EXAMS-V: A multi-discipline multilingual multimodal exam benchmark for evaluating vision language models. </a></span><span><a href="http://paperpile.com/b/coPcdt/C99w" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/coPcdt/C99w" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>24. </span><span><a href="http://paperpile.com/b/coPcdt/NqcC" rel="noopener noreferrer" target="_blank">Alfarano, A., Venturoli, L. &amp; del Castillo, D. N. VQArt-Bench: A semantically rich VQA Benchmark for Art and Cultural Heritage. </a></span><span><a href="http://paperpile.com/b/coPcdt/NqcC" rel="noopener noreferrer" target="_blank">arXiv [cs.CV]</a></span><span><a href="http://paperpile.com/b/coPcdt/NqcC" rel="noopener noreferrer" target="_blank"> (2025) doi:</a></span><span><a href="http://dx.doi.org/10.48550/arXiv.2510.12750" rel="noopener noreferrer" target="_blank">10.48550/arXiv.2510.12750</a></span><span><a href="http://paperpile.com/b/coPcdt/NqcC" rel="noopener noreferrer" target="_blank">.</a></span></p><p><span>25. </span><span><a href="http://paperpile.com/b/coPcdt/BOpQ" rel="noopener noreferrer" target="_blank">Weidinger, L. </a></span><span><a href="http://paperpile.com/b/coPcdt/BOpQ" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/coPcdt/BOpQ" rel="noopener noreferrer" target="_blank"> Ethical and social risks of harm from Language Models. </a></span><span><a href="http://paperpile.com/b/coPcdt/BOpQ" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/coPcdt/BOpQ" rel="noopener noreferrer" target="_blank"> (2021).</a></span></p><p><span>26. </span><span><a href="http://paperpile.com/b/coPcdt/H5i7" rel="noopener noreferrer" target="_blank">Li, J., Zhong, S. &amp; Chen, K. MLEC-QA: A Chinese Multi-Choice Biomedical Question Answering Dataset. in </a></span><span><a href="http://paperpile.com/b/coPcdt/H5i7" rel="noopener noreferrer" target="_blank">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span><span><a href="http://paperpile.com/b/coPcdt/H5i7" rel="noopener noreferrer" target="_blank"> (Association for Computational Linguistics, Stroudsburg, PA, USA, 2021). doi:</a></span><span><a href="http://dx.doi.org/10.18653/v1/2021.emnlp-main.698" rel="noopener noreferrer" target="_blank">10.18653/v1/2021.emnlp-main.698</a></span><span><a href="http://paperpile.com/b/coPcdt/H5i7" rel="noopener noreferrer" target="_blank">.</a></span></p><p><span>27. </span><span><a href="http://paperpile.com/b/coPcdt/aa1y" rel="noopener noreferrer" target="_blank">Vilares, D. &amp; Gómez-Rodríguez, C. HEAD-QA: A Healthcare Dataset for Complex Reasoning. </a></span><span><a href="http://paperpile.com/b/coPcdt/aa1y" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/coPcdt/aa1y" rel="noopener noreferrer" target="_blank"> (2019).</a></span></p><p><span>28. </span><span><a href="http://paperpile.com/b/coPcdt/tzW5" rel="noopener noreferrer" target="_blank">Hertzberg, N. &amp; Lokrantz, A. MedQA-SWE - a Clinical Question &amp; Answer Dataset for Swedish. </a></span><span><a href="http://paperpile.com/b/coPcdt/tzW5" rel="noopener noreferrer" target="_blank">LREC</a></span><span><a href="http://paperpile.com/b/coPcdt/tzW5" rel="noopener noreferrer" target="_blank"> 11178–11186 (2024).</a></span></p><p><span>29. </span><span><a href="http://paperpile.com/b/coPcdt/Re9L" rel="noopener noreferrer" target="_blank">Bean, A. M., Korgul, K., Krones, F., McCraith, R. &amp; Mahdi, A. Exploring the landscape of large language models in medical question answering. </a></span><span><a href="http://paperpile.com/b/coPcdt/Re9L" rel="noopener noreferrer" target="_blank">arXiv</a></span><span><a href="http://paperpile.com/b/coPcdt/Re9L" rel="noopener noreferrer" target="_blank">.</a></span></p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image16.png" title=""/></figure>