<h1>Chapter 7</h1><p class="chapter-subtitle"><span>Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification </span></p><hr/><p><span>Shan Chen*</span><span>, Yingya Li*, Sheng Lu, Hoang Van, Hugo JWL Aerts, </span></p><p><span>Guergana K Savova, Danielle S Bitterman</span></p><p class="chapter-meta"><a href="https://doi.org/10.1093/jamia/ocad256" rel="noopener noreferrer" target="_blank"><em>Journal of the American Medical Informatics Association</em></a></p><h2>Summary</h2><h2>Background</h2><p><span>Large language models (LLMs) such as ChatGPT (GPT-3.5, GPT-4) demonstrate strong general question-answering ability but remain under-explored for specialized biomedical applications. Understanding their performance in structured biomedical tasks is essential before clinical or research deployment.</span></p><h2>Methods</h2><p><span>Using 11,122 samples, our study evaluated ChatGPT-family models on two core biomedical natural language processing tasks: </span><span>classification</span><span> of health advice in scientific literature (n = 8676) and </span><span>reasoning</span><span> via detection of causal relations (n = 2446). Prompts were developed under zero-shot and few-shot settings, with and without chain-of-thought (CoT) reasoning. The best prompts from each configuration were compared against two baselines: a simple bag-of-words (BoW) logistic regression model and fine-tuned BioBERT models.</span></p><h2>Findings</h2><p><span>Fine-tuned BioBERT achieved the highest F1 scores for both classification (0.800–0.902) and reasoning (0.851). Among LLMs, few-shot CoT prompting produced the strongest results—classification F1 0.671–0.770 and reasoning F1 0.682—comparable to the BoW baseline (F1 0.602–0.753 and 0.675, respectively). However, achieving optimal LLM performance required </span><span>78 hours</span><span>, compared with only </span><span>0.078 hours</span><span> for BioBERT and </span><span>0.008 hours</span><span> for BoW.</span></p><h2>Interpretation</h2><p><span>Despite the popularity of ChatGPT, task-specific fine-tuning of domain-adapted models like BioBERT remains the most effective and efficient strategy for biomedical NLP. LLM prompting can approach—but not surpass—traditional methods, while requiring substantially more computational time and engineering effort.</span></p><hr/><h2>Introduction</h2><p><span>The advancements in machine learning (ML) methods for natural language process (NLP), such as transformers</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/OA2QZ" rel="noopener noreferrer" target="_blank">1</a></sup></span><span> and reinforcement learning</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/EWdNo" rel="noopener noreferrer" target="_blank">2</a></sup></span><span>, in combination with abundant digital text and scaled-up hardware capabilities has led to many pretrained large language models (LLMs) — also referred to as foundation models. Coupling some of these LLMs with smart engineering gave the world the viral ChatGPT, which in turn popularized the technology and re-invigorated the artificial intelligence (AI)/artificial general intelligence (AGI) debate. Although most LLMs are trained as chatbots, some of the claims in the mainstream media go as far as stating that the LLMs are sentient, even able to solve tasks that previously required a high level of human expertise and specialized training. On the other hand, the scientific papers describing the LLMs are much more measured</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/yZqXo" rel="noopener noreferrer" target="_blank">3</a></sup></span><span> outlining limitations: </span><span>“… Aside from intentional misuse, there are many domains where large language models should be deployed only with great care, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying people based on protected characteristics, determining eligibility for credit, employment, or housing, generating political advertisements, and law enforcement.”</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/Ki4xv" rel="noopener noreferrer" target="_blank">4</a></sup></span><span> Therefore the scientific community bears the responsibility of understanding the LLMs’ strengths and weaknesses, how their limitations and risks can be managed, and implications for our future.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/bupsp+nlsHV" rel="noopener noreferrer" target="_blank">5,6</a></sup></span></p><p><span>Medicine is one of the highest-stakes domains for LLMs. The excitement surrounding the LLMs has penetrated the biomedical and clinical communities motivating various early use-case evaluations. For example, studies have shown that ChatGPT can pass the US Medical Licensing Examination (USMLE).</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/6p9xd+Kdn6v" rel="noopener noreferrer" target="_blank">7,8</a></sup></span><span> Zuccon and Koopman</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/HuOMB" rel="noopener noreferrer" target="_blank">9</a></sup></span><span> </span><span>investigate the effect of prompts on ChatGPT in answering complex health information questions. Chen et al.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/V4C6" rel="noopener noreferrer" target="_blank">10</a></sup></span><span> evaluate the performance and robustness of ChatGPT in providing cancer treatment recommendations that align with National Comprehensive Cancer Network (NCCN) guidelines. Lyu et al.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/LOqlU" rel="noopener noreferrer" target="_blank">11</a></sup></span><span> research the feasibility of using ChatGPT to translate radiology reports into plain language for patients and healthcare providers. A paper by Google Research and DeepMind</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/Ah0nd" rel="noopener noreferrer" target="_blank">12</a></sup></span><span> presents experiments with Google’s PaLM family of LLMs, suggesting the potential utility of LLMs in medicine but also revealing important limitations, reinforcing the importance of evaluation frameworks and methods development.</span></p><p><span>In parallel, the practical use of LLMs is limited by their huge size and computational requirements, limiting accessibility for most healthcare practices and researchers. Thus, researchers have been pursuing broader questions such as the utility of specialized clinical models, especially ones that are smaller and thus computationally affordable, in the LLM era. Lehman et al.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/lDUlL" rel="noopener noreferrer" target="_blank">13</a></sup></span><span> show that relatively small specialized clinical models substantially outperform bigger LLMs, even when fine-tuned on limited annotated data. In addition, they show that pretraining on clinical datasets allows for smaller, more parameter-efficient models that either match or outperform the much bigger computationally hungry LLMs. Wang et al.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/VfLjI" rel="noopener noreferrer" target="_blank">14</a></sup></span><span> focus on exploring ChatGPT robustness where a medical diagnosis dataset represents out-of-domain distributions. Results are consistent with Lehman at al.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/lDUlL" rel="noopener noreferrer" target="_blank">13</a></sup></span></p><p><span>We set out to contribute to the growing understanding of LLMs in the biomedical domain, with a focus on practical end-use. NLP research on the clinical narrative within the Electronic Medical Records (EMR) has direct applications to translational science</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/Q4Xhw" rel="noopener noreferrer" target="_blank">15</a></sup></span><span>, clinical decision support,</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/ECcXS" rel="noopener noreferrer" target="_blank">16</a></sup></span><span> and healthcare administration</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/9Zs2K" rel="noopener noreferrer" target="_blank">17</a></sup></span><span> in addition to direct patient care. Two fundamental NLP tasks to support these applications are classification (e.g. patient phenotyping) and reasoning (e.g. adverse events of medications), and understanding how LLMs handle these tasks will provide insight into optimal uses of these methods. Thus, we aim to evaluate the state-of-the-art (SOTA) LLM performance on classification and reasoning tasks requiring understanding of contextual nuances. While numerous LLMs have been trained in recent years, most are proprietary and not publicly available for a local download (e.g., GPT-3, ChatGPT), which precludes their evaluation on clinical datasets containing Protected Health Information (PHI) data, even if the data are de-identified. Therefore, we work with proxy biomedical data. We evaluate LLMs within the constraints of the typical user to understand their real-world utility, using the OpenAI API</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/l2nGi" rel="noopener noreferrer" target="_blank">18</a></sup></span><span> and LLMs that are computationally feasible for the IT capabilities of most hospitals and clinical practice.</span></p><h2>Method</h2><h3>2.1 Tasks and datasets</h3><p><span>We examine the performance of LLMs on two fundamental tasks in the clinical domain – classification and reasoning. Specifically, we select two open datasets annotated for health advice and causal language to test the ability of the models to classify and reason over medical literature findings, and their implications for health-related practices.</span></p><ul><li><span>Classification task: HealthAdvice</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/lfPHu" rel="noopener noreferrer" target="_blank">19</a></sup></span><span> is a dataset consisting of annotations of 10,000+ sentences extracted from abstracts and discussion/conclusion sections of medical research literature. The dataset adopts a multi-dimensional taxonomy and categorizes each sentence into “no advice”, “weak advice”, and “strong advice” to capture the occurrence and strength of clinical and policy recommendations. As health advice normally appears in either abstracts or discussion/conclusion sections and its language style may vary across different sections, the labels are further separated into three datasets: </span><span>advice in discussion sections, advice in unstructured abstracts, and advice in structured abstracts</span><span>.</span></li><li><span>Reasoning task: CausalRelation</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/uPu1p" rel="noopener noreferrer" target="_blank">20</a></sup></span><span> </span><span>is a multi-label reasoning dataset with the goal to identify correlational and causal claims in the findings of medical research literature. The annotated corpus includes over 3,000 PubMed research conclusion sentences extracted from abstracts. Each sentence is labeled as “correlational”, “conditional causal”, “direct causal”, and “no relationship” by its certainty and reasoning type.</span></li></ul><p><span>Appendix Table B1 and B2 show the dataset distributions. We split the two datasets into development and test sets. The development set is a proportionate sample of 20% the original dataset, while the remaining 80% of the dataset is used as the test set. Final evaluation is performed on the test set. </span></p><figure class="chapter-figure prompt-figure" style="margin: 2em 0;">
<figcaption><span>Figure 1. </span><span>Examples of the prompt templates used for the classification and reasoning tasks.</span><span> </span><span>Prompt templates were created for each of the prompting settings evaluated: </span><span>(A)</span><span> zero-shot, </span><span>(B) </span><span>zero-shot with Chain of Thought (CoT), </span><span>(C)</span><span> one-shot and few-shot, and </span><span>(D)</span><span> one-shot and few-shot with CoT. “…more exemplars…” (highlighted in green) were added only for the few-shot with and without CoT settings. All evaluated prompts are in the Appendix B. BoW = bag-of-words.</span></figcaption>
<pre class="prompt-example" style="background:#f4f4f4; padding:15px; border-radius:8px; white-space:pre-wrap; font-family:monospace; font-size:0.9em; margin-top: 1em; overflow-x: auto;">
[Original Context] = Georgian public health specialists working in the HIV field should prioritize implementation of such interventions among HIV patients.
[Question] = Is this a 0) no advice, 1) weak advice or 2) strong advice?
[Answer] = 2) Strong advice.
[CoT Solution] = 
1) The statement is a directive, so it is a strong advice. 
2) The statement is specific and clear, so it is not a weak advice. 
3) Therefore, the statement is a strong advice. 
Answer: 2) Strong advice.
</pre>
</figure><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image1.png" title=""/><figcaption>Figure: image1.</figcaption></figure><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image10.png" title=""/><figcaption>Figure: image10.</figcaption></figure><h3>2.1 Baseline models</h3><p><span>We compare the performance of LLMs with classic ML approaches and transformer-based pretrained language models. For the classic ML approach, we train logistic regression fitted with Stochastic Gradient Descent (SGD), using bag-of-words (BoW) representations with tf-idf as the vectorization method. We fine-tune BioBERT models, given that BERT-based pre-trained language models</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/nut9V" rel="noopener noreferrer" target="_blank">21</a></sup></span><span>, particularly BioBERT</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/QLAXk" rel="noopener noreferrer" target="_blank">22</a></sup></span><span>, have exhibited their efficacy on the aforementioned two tasks (Hyperparameter settings are in the Appendix Table A1). The baseline models are trained and fine-tuned on the full development set and tested on the test set. To further examine the effect of the amount of data on model performance, we trained and tested the BoW and BioBERT models with the 20%, 50%, and 100% of the development set. We track the time required to develop and evaluate the BoW and BioBERT models.</span></p><h3>2.2 ChatGPT Family of Models</h3><p><span>We evaluate GPT-4 and its predecessors, including GPT-3.5-Turbo (20B) and GPT-Davinci-003 (175B), on the two tasks. For </span><span>GPT-3.5 models</span><span>, we consider zero-shot, one-shot, and few-shot prompting with and without Chain-of-Thought (CoT). Given computational cost limits, for GPT-4, we consider zero-shot (the simplest prompting strategy mimicking an average user) and few-show with CoT (the most complex prompting strategy). CoT techniques explicitly outline the intermediate reasoning steps as prompts to LLMs to elicit multi-step reasoning behavior.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/bHjK3" rel="noopener noreferrer" target="_blank">23</a></sup></span></p><p><span>To design a prompt, we follow the prompt structure applied in prior studies.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/bHjK3" rel="noopener noreferrer" target="_blank">23</a></sup></span><span> Fig 1 shows an example of the prompt templates for the classification task. For the zero-shot settings, five of the authors each independently developed two prompts. For the one- and few-shot settings without CoT, </span><span>exemplars</span><span> are chosen directly from the development set. For the one- and few-shot settings with CoT, the same five authors independently wrote CoT prompts for </span><span>exemplars</span><span> from each of the datasets. To evaluate model efficiency, we track the total time spent on designing prompts. Given the different number of classes in the health advice (3 classes) and causal language (4 classes) datasets, we apply 3- and 4-shot </span><span>exemplars</span><span> for the few-shot settings for the classification and reasoning tasks respectively. We use regular expressions to match the model output to labels in the datasets, and conduct validation to verify the accuracy of the regular expressions. To assess the model’s performance, we compare the prediction results against the gold annotations in the datasets. Performance of the prompts was initially evaluated on the development set, and the best performing prompt was selected for the final evaluation on the test dataset. Performance across </span><span>exemplars</span><span> from different classes was also evaluated. The full set of evaluated prompts are included in the Appendix Table B3 and B4.</span></p><p><span>We use the OpenAI Application Programming Interface (API) to run the prediction and measure the performance of the models based on the averaged macro-F1 score on 4-fold cross validation on the test set. F1 score is a classic NLP metric representing the harmonic mean of recall/sensitivity and precision/positive predictive value. Macro F1 score is computed using the arithmetic mean of all per-class F1 scores. For comparison of model efficiency, we track the time and cost for running the inference using the API (Appendix Table A2-A4)</span></p><h3>2.3 Smaller LLMs</h3><p><span>The same settings (zero- and few-shot with and without CoT) were used to evaluate the performance</span><span> </span><span>of select smaller LLMs (less than 10B parameters) on the tasks, including GPT-J, GPT-JT, and Galactica</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/DU3ir" rel="noopener noreferrer" target="_blank">24</a></sup></span><span>. GPT-J, built on EleutherAI’s 6B parameter GPT-J-6B, is fine-tuned with 3.5 billion tokens. It performs very similarly to GPT-3 on various zero-shot downstream tasks. GPT-JT, a fork of GPT-J-6B, is fine-tuned on 3.53 billion tokens and has been shown to even outperform GPT-3 at some classification tasks. Galactica is trained on 48 million examples</span><span> </span><span>of scientific articles, websites, textbooks, lecture notes, and encyclopedias. Same evaluation procedure as for the GPT-family models is applied.</span></p><h3>2.4 Statistical Analysis</h3><p><span>Paired </span><span>t</span><span> tests are used to compare average macro-F1 score across tasks, and a two-sided </span><span>p</span><span>&lt;0.05 is considered statistically significant. Analyses are performed using python version 3.9.7 (Python Software Foundation).</span></p><h2>Results</h2><p>A summary of our findings is in Table 1; Figs 2-4 present multiple comparative analyses to provide context and facilitate interpretation of the outcomes.</p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image50.png" title=""/><figcaption>Figure 2. (A) Comparison of model performance on each dataset. Using fine-tuning BioBERT as a reference, average macro-F1 across all datasets was significantly better than all other models (p&lt;0.05 for all); there was no statistically significant difference in average performance between the other pair-wise model comparisons (Appendix Table A5). (B) Time (hours) required to obtain the best-performing results versus average performance across all datasets. BoW = bag-of-words; CoT = Chain of Thought.</figcaption></figure><h3>3.1 Effect of fine-tuning and prompt development on model performance, run time and time investment </h3><p><span>For the reasoning and classification tasks, fine-tuning BioBERT consistently outperforms the best GPT settings by a considerable margin (Δmacro F1 0.109 - 0.169) (Fig 2(A) and Table 1). Averaging performance on all datasets, BioBERT's macro F1 scores are significantly better than the other models (</span><span>p</span><span>&lt;0.05 for all, Fig 2(A). There is no statistical difference between the average performance of the BoW, best-performing GPT-3.5s, and GPT-4 models (Appendix Table A3). </span></p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image52.png" title=""/><figcaption>Figure 3. (A-D) Performance comparison of fine-tuned BioBERT (green bar) and BoW (yellow bar) models with different proportions (20%, 50% or 100%) of development set data versus the best GPTs settings (few-shot CoT in red-line) among four tasks. (A) Comparison on advice in discussion sections, (B) comparison on advice in unstructured abstracts, (C) comparison on advice in structured abstracts, (D) comparison on causal relation detection. BoW = bag-of-words; CoT = Chain of Thought</figcaption></figure><p><span>Furthermore, engineering the LLM prompts that yield the best results requires a substantial time investment. Fig 2(B) shows the time needed to achieve the best results, taking into account the time needed to develop and identify the prompting strategies for the few-shot settings. Even for the zero-shot setting, which does not require any prompt development, inference time — an indicator of run time and a consideration for compute budget— is longer and yields worse performance compared to BoW and BioBERT performance. Taken together, training a task-specific neural network through classic fine-tuning methodology is both faster and yields better performance.</span></p><h3>3.2 Effect of amount of training data on model performance </h3><p><span>In this experiment, we investigate the amount of training/fine-tuning data from the development set needed to achieve similar or better performing BoW and BioBERT models compared to the best GPT settings (few-shot CoT). As shown in Fig 3, using only 20% of the development set for the supervised fine-tuning of BioBERT surpasses the best GPTs in three of four datasets. Fine-tuning BioBERT on 50% of the development set outperforms the best GPTs on all datasets. Furthermore, the simple and computationally efficient BoW model using 100% of the development set outperforms the best GPTs in two of four datasets. </span></p><p><span>3.3 Effect of number of </span><span>exemplar</span><span> prompts and CoT prompts on model performance </span></p><p><span>We examine the relationship between the number of exemplars and CoT prompts and their impact on GPT settings performance. As shown in Fig 4(A), we observe a drop in performance when comparing one-shot to few-shot settings in three of the datasets. Reasoning for causal relation detection is the only task that consistently improves by adding prompt examples, without CoT. However, as demonstrated in Fig 4(B), incorporating more CoT </span><span>exemplars</span><span> results in consistent improvements across datasets. This observation highlights the value of adding CoT </span><span>exemplars</span><span>, albeit at substantial cost due to the time effort needed to create the CoT prompts.</span></p><p><span>Another observation from the one-shot experiments (both with and without CoT) among the LLMs evaluated is the impact of different exemplar prompt choices on performance. Variations in the text of the </span><span>exemplar</span><span> prompts for one-shot prompts led to notable variations in model outcomes (Appendix Table A8-A10).</span></p><p><span>3.4</span><span> Error Analysis</span></p><p><span>Of the investigated GPTs, GPT-4 with few-shot CoT settings yields the best performance for three of four datasets. Thus, we analyze GPT-4’s generated CoTs and identify common error patterns to better </span></p><p><span>Figure 4. </span><span>Comparison of</span><span> </span><span>GPT-3.5s performance on each dataset with increasing </span><span>exemplars</span><span> without </span><span>(A)</span><span> and with </span><span>(B)</span><span> Chain of Thought (CoT). For both plots, random is shown as a baseline and is the uniform distribution across class labels. Here, one-shot uses the majority class exemplar for each task and few-shot uses one exemplar per class. Few-shot = 3 </span><span>exemplars</span><span> for classification datasets and four </span><span>exemplars</span><span> for the reasoning dataset, reflecting the number of classes for each task.</span><span><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image47.png" title=""/><figcaption>Figure: image47.</figcaption></figure></span></p><p><span>understand their strengths and limitations. To maintain consistency, we randomly select 100 prediction errors of GPT-4 with few-shot CoT </span><span>setting</span><span> from the test set of each dataset. The selection of the error examples follows the same error type ratio from the confusion matrices of the prediction result. Two common error patterns are identified. Pattern A is an incorrect reasoning step based on one specific keyword. For example, the model classifies an input text as a strong advice if the word “importance” appears in the text (Appendix Table A13, row 4). Pattern B is a false positive due to the model incorrectly determining that there is health advice or a relationship for the classification and reasoning tasks, respectively, when in fact there is none. For instance, Appendix Table A16, row 3 presents a pattern B error where GPT-4 misclassifies a relationship between extracted entities as a clinical relationship. Appendix Tables A13-A16 show examples of the error patterns. </span></p><h2>Discussion</h2><p><span>In this study, we found that, even with the best in-context learning approaches, fine-tuning BioBERT consistently out-performed LLM performance by macro F1 &gt;0.100 for all datasets. In fact, fine-tuning on just 20% of the development dataset outperformed the best GPT-4 and GPT-3.5 performance for all of the classification datasets, and outperformed the best GPT-3.5 performance for the reasoning dataset. Surprisingly, the simple BoW models out-performed all LLM in-context learning approaches without CoT, and performed similarly to the best performing GPT-4 approach for classification of structured abstracts (macro F1 -0.017), and the reasoning task (macro F1 -0.007).</span></p><p><span>Our study emphasizes the performance, time, and computational trade-offs that should be taken into account when considering various approaches for clinical NLP tasks. At present, our results suggest that the overall balance is in favor of fine-tuning task-specific smaller models, consistent with Lehman et al</span><span>16</span><span>. SOTA LLMs such as GPT-3.5/GPT-4 are orders of magnitude larger than traditional language models and cannot be trained or fine-tuned without significant computational resources. For example, GPT-3 has 175 billion parameters and required several thousand petaflop/s-days for pre-training.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/Ua2Fh" rel="noopener noreferrer" target="_blank">25</a></sup></span><span> On the other hand, smaller, more accessible out-of-the-box LLMs without fine-tuning (&lt;10 billion parameters) performed very poorly on our tasks and did not demonstrate improvement with in-context learning, in line with the finding that emergent LLMs abilities scale with model size.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/95vn0" rel="noopener noreferrer" target="_blank">26</a></sup></span><span> </span></p><p><span>However, the computational requirements of LLMs could theoretically be offset if their zero- or few-shot performance was adequate. Our results clearly demonstrate that zero-short performance was poor. While few-shot CoT </span><span>prompting</span><span> improved performance, fine-tuning BioBERT, a 110 million parameter pre-trained language model, consistently provided the best performance. Prompt development to identify the best prompting strategies is itself resource-intensive, requiring human effort to design the prompts, and computational and time resources to evaluate. Ultimately, at most 50% of each dataset’s full development set was needed to fine-tune BioBERT models that exceeded LLM performance with the best prompting strategies identified using the full development set. Taking into account prompt development and evaluation, obtaining the best-performing LLM results required 100x the time needed to fine-tune our best performing BioBERT model. Of note, we did not include time needed to annotate the datasets, as all methods required the full development set for model development or prompt engineering, and the full test set for evaluation.</span></p><p><span>Despite under-performing fine-tuned BioBERT, in-context learning—especially CoT prompting—led to important improvements in performance for the classification and reasoning tasks. The ability of CoT to elicit reasoning and improve LLM performance in the general domain has previously been demonstrated.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/bHjK3+nrYzn" rel="noopener noreferrer" target="_blank">23,27</a></sup></span><span> Interestingly, while providing more exemplars with CoT prompting consistently improved performance, this was not always the case for prompting without CoT. For GPT-3.5, one-shot prompting provided the best results for classification, while few-shot prompting provided the best results for reasoning. This could be due to noise provided from including less informative prompts, and highlights the fragility of LLM performance based on the provided prompts. </span></p><p><span>This lack of robustness is also illustrated by the fact that the choice of exemplar for prompting had major impacts on performance. These findings are in line with Shi et al., who showed that, for arithmetic tasks, prompting may provide irrelevant context that reduces performance.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/eKzvW" rel="noopener noreferrer" target="_blank">28</a></sup></span><span> Clearly, simply providing more exemplars does not solve the challenge of LLMs robustness—a major area of concern and future research for the clinical domain, where robust performance is paramount. Concerningly, even small, seemingly non-substantive changes to prompts such as typos have been shown to impact performance.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/VfLjI+V4C6" rel="noopener noreferrer" target="_blank">10,14</a></sup></span><span> There is an emerging body of work on developing strategies to improve robustness and self-consistency.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/VfLjI+ktchQ" rel="noopener noreferrer" target="_blank">14,29</a></sup></span><span> Methods to improve performance will be especially important in the medical domain, where jargon, typos, abbreviations, and synonyms are common, limiting our ability to develop reliable prompting strategies and robustly assess real-world performance.</span></p><p><span>It should be noted that our tasks indirectly address the question of how LLMs perform on clinical NLP text, and use biomedical texts as a proxy for essential clinical NLP tasks. SOTA LLMs such as GPT-3.5/GPT-4 cannot be used with PHI, precluding an evaluation on real clinical data. Nevertheless, LLM performance is known to decrease on out-of-domain tasks, i.e. tasks that include text that does not reflect what it was trained on, including synthetic clinical text.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/VfLjI" rel="noopener noreferrer" target="_blank">14</a></sup></span><span> At the same time, classic language models trained on clinical text has been shown to out-perform LLMs with in-context learning.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/lDUlL" rel="noopener noreferrer" target="_blank">13</a></sup></span><span> Taken together with our finding that BioBERT, which is pre-trained on biomedical text, out-performs LLMs, it is reasonable to anticipate that findings would be similar on clinical datasets, but further evaluation will be needed once GPT-3.5/GPT-4 are safely and widely accessible for HIPAA-protected data. Further, non-clinical biomedical NLP has important implications in its own right, including in processing and transmitting medical information and knowledge outside of EMR contexts.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/4UG7r" rel="noopener noreferrer" target="_blank">30</a></sup></span></p><p><span>Most studies evaluating LLMs for clinical applications have focused on question-answering, with mixed results.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/6p9xd+Kdn6v+HuOMB+Ah0nd+V4C6" rel="noopener noreferrer" target="_blank">7–10,12</a></sup></span><span> For example, while LLMs have achieved passing scores in the USMLE,</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/6p9xd+Kdn6v" rel="noopener noreferrer" target="_blank">7,8</a></sup></span><span> several recent studies have shown sub-par performance of ChatGPT, including answering neonatal board exam questions,</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/tvjz" rel="noopener noreferrer" target="_blank">31</a></sup></span><span> answering questions about cancer treatment,</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/V4C6" rel="noopener noreferrer" target="_blank">10</a></sup></span><span> and providing assistance to laypeople.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/K2Wb" rel="noopener noreferrer" target="_blank">32</a></sup></span><span> However, question-answering is not representative of the range of NLP tasks needed to process clinical texts,</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/FgFYMN/4UG7r" rel="noopener noreferrer" target="_blank">30</a></sup></span><span> and in isolation has limited practical use for clinic and research. Further, such studies to date have largely focused on whether the LLM answers a question correctly but do not provide insight into why models may fail, which is critical to improving LLMs for medicine and moving the field forward. We chose our tasks—classification and reasoning—because they are fundamental to the development of NLP technologies that can support clinical care and research beyond question-answering, and are granular enough to reveal the specific language processing steps where LLMs may not perform well. </span></p><p><span>NLP for classification entails determining what category an input text belongs in. Classification methods identify if a patient’s EMR includes a characteristic or outcome of interest, which has implications for outcomes research, clinical trial matching, and identifying key events at the point-of-care. Reasoning entails determining the relationships between entities, which is to automatically identify how different events in a patient’s medical history relate to one another. Especially because nuanced information conveying medical reasoning can often only be expressed in free text, NLP methods for reasoning are needed to automate higher-level medical inferencing. Here, we investigated causative relationships, which is needed for tasks that require linking any clinical outcome with causative factors, such as associating adverse drug events with their inciting agent. Other reasoning tasks include temporal reasoning, which is determining the order of medical events over time. Another benefit of our task selection is that, compared to question-answering, they are more straight-forward to objectively evaluate, enabling a more direct evaluation of how LLMs perform in the biomedical domain and providing a clearer understanding into the types of performance gaps that need to be improved with additional research and development. In the future, evaluation of LLMs in the clinical domain for other classification and reasoning tasks, as well as other common NLP tasks such as relation extraction, named entity recognition, coreference resolution, word sense disambiguation, and machine translation, will be needed.</span></p><h2>Conclusion</h2><p><span>This study suggests an ongoing role for classic NLP models fine-tuned for specific tasks, while also providing guidance into strategies to optimize the LLMs for the biomedical domain. Fine-tuning BioBERT, a much smaller pretrained language model, out-performed SOTA huge LLMs for biomedical classification and reasoning tasks even after extensive prompt development. CoT prompting with multiple exemplars improved LLM performance compared to zero-shot prompting and prompting without CoT. However, developing CoT prompts was both time- and data-intensive, and BioBERT was more efficient with respect to both measures. In addition, LLMs were very sensitive to prompting strategy and the choice of prompt, raising concerns about the potential to develop LLM methods for medicine that are reliable and safe. Our work provides insight into the potential and pitfalls of these rapidly emerging methods for biomedical text processing. Future research could focus on developing more efficient prompting strategies and fine-tuning techniques for LLMs in the biomedical domain while ensuring their reliability and safety, as well as exploring hybrid approaches that combine the strengths of classic NLP models and LLMs to further enhance performance in biomedical text processing tasks.</span></p><h3>Data Availability Statement:</h3><p><span>All data generated/analysed during this study are available and can be found at </span><span><a href="https://github.com/shan23chen/HealthLLM_Eval" rel="noopener noreferrer" target="_blank">https://github.com/shan23chen/HealthLLM_Eval</a></span><span>.</span></p><p><span>Please view the full appendix at:</span><span> </span><span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10990500/" rel="noopener noreferrer" target="_blank">https://pmc.ncbi.nlm.nih.gov/articles/PMC10990500/</a></span></p><hr/><h2>References</h2><p><span>1. </span><span><a href="http://paperpile.com/b/FgFYMN/OA2QZ" rel="noopener noreferrer" target="_blank">Vaswani, A. </a></span><span><a href="http://paperpile.com/b/FgFYMN/OA2QZ" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/OA2QZ" rel="noopener noreferrer" target="_blank"> Attention is all you need. in </a></span><span><a href="http://paperpile.com/b/FgFYMN/OA2QZ" rel="noopener noreferrer" target="_blank">NeurIPS Proceedings</a></span><span><a href="http://paperpile.com/b/FgFYMN/OA2QZ" rel="noopener noreferrer" target="_blank"> (2017).</a></span></p><p><span>2. </span><span><a href="http://paperpile.com/b/FgFYMN/EWdNo" rel="noopener noreferrer" target="_blank">Sutton, R. S. &amp; Barto, A. G. </a></span><span><a href="http://paperpile.com/b/FgFYMN/EWdNo" rel="noopener noreferrer" target="_blank">Reinforcement Learning: An Introduction, 2nd Edition</a></span><span><a href="http://paperpile.com/b/FgFYMN/EWdNo" rel="noopener noreferrer" target="_blank">. (The MIT Press, Cambridge, MA, 2018).</a></span></p><p><span>3. </span><span><a href="http://paperpile.com/b/FgFYMN/yZqXo" rel="noopener noreferrer" target="_blank">Ouyang, L. </a></span><span><a href="http://paperpile.com/b/FgFYMN/yZqXo" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/yZqXo" rel="noopener noreferrer" target="_blank"> Training language models to follow instructions with human feedback. </a></span><span><a href="http://paperpile.com/b/FgFYMN/yZqXo" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/FgFYMN/yZqXo" rel="noopener noreferrer" target="_blank"> (2022).</a></span></p><p><span>4. </span><span><a href="http://paperpile.com/b/FgFYMN/Ki4xv" rel="noopener noreferrer" target="_blank">Ouyang, L. </a></span><span><a href="http://paperpile.com/b/FgFYMN/Ki4xv" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/Ki4xv" rel="noopener noreferrer" target="_blank"> Training language models to follow instructions with human feedback. in </a></span><span><a href="http://paperpile.com/b/FgFYMN/Ki4xv" rel="noopener noreferrer" target="_blank">NeurIPS Proceesings</a></span><span><a href="http://paperpile.com/b/FgFYMN/Ki4xv" rel="noopener noreferrer" target="_blank"> (2022).</a></span></p><p><span>5. </span><span><a href="http://paperpile.com/b/FgFYMN/bupsp" rel="noopener noreferrer" target="_blank">Lee, P., Bubeck, S. &amp; Petro, J. Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine. </a></span><span><a href="http://paperpile.com/b/FgFYMN/bupsp" rel="noopener noreferrer" target="_blank">N. Engl. J. Med.</a></span><span><a href="http://paperpile.com/b/FgFYMN/bupsp" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/FgFYMN/bupsp" rel="noopener noreferrer" target="_blank">388</a></sup></span><span><a href="http://paperpile.com/b/FgFYMN/bupsp" rel="noopener noreferrer" target="_blank">, 1233–1239 (2023).</a></span></p><p><span>6. </span><span><a href="http://paperpile.com/b/FgFYMN/nlsHV" rel="noopener noreferrer" target="_blank">Reardon, S. AI Chatbots Can Diagnose Medical Conditions at Home. How Good Are They? </a></span><span><a href="http://paperpile.com/b/FgFYMN/nlsHV" rel="noopener noreferrer" target="_blank">Scientific American</a></span><span><a href="http://paperpile.com/b/FgFYMN/nlsHV" rel="noopener noreferrer" target="_blank"> (By Sara Reardon on March 31 2023).</a></span></p><p><span>7. </span><span><a href="http://paperpile.com/b/FgFYMN/6p9xd" rel="noopener noreferrer" target="_blank">Gilson, A. </a></span><span><a href="http://paperpile.com/b/FgFYMN/6p9xd" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/6p9xd" rel="noopener noreferrer" target="_blank"> How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment. </a></span><span><a href="http://paperpile.com/b/FgFYMN/6p9xd" rel="noopener noreferrer" target="_blank">JMIR Med Educ</a></span><span><a href="http://paperpile.com/b/FgFYMN/6p9xd" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/FgFYMN/6p9xd" rel="noopener noreferrer" target="_blank">9</a></sup></span><span><a href="http://paperpile.com/b/FgFYMN/6p9xd" rel="noopener noreferrer" target="_blank">, e45312 (2023).</a></span></p><p><span>8. </span><span><a href="http://paperpile.com/b/FgFYMN/Kdn6v" rel="noopener noreferrer" target="_blank">Liévin, V., Hother, C. E. &amp; Winther, O. Can large language models reason about medical questions? </a></span><span><a href="http://paperpile.com/b/FgFYMN/Kdn6v" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/FgFYMN/Kdn6v" rel="noopener noreferrer" target="_blank"> (2022).</a></span></p><p><span>9. </span><span><a href="http://paperpile.com/b/FgFYMN/HuOMB" rel="noopener noreferrer" target="_blank">Zuccon, G. &amp; Koopman, B. Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness. </a></span><span><a href="http://paperpile.com/b/FgFYMN/HuOMB" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/FgFYMN/HuOMB" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>10. </span><span><a href="http://paperpile.com/b/FgFYMN/V4C6" rel="noopener noreferrer" target="_blank">Chen, S. </a></span><span><a href="http://paperpile.com/b/FgFYMN/V4C6" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/V4C6" rel="noopener noreferrer" target="_blank"> Use of Artificial Intelligence Chatbots for Cancer Treatment Information. </a></span><span><a href="http://paperpile.com/b/FgFYMN/V4C6" rel="noopener noreferrer" target="_blank">JAMA Oncol</a></span><span><a href="http://paperpile.com/b/FgFYMN/V4C6" rel="noopener noreferrer" target="_blank"> (2023) doi:</a></span><span><a href="http://dx.doi.org/10.1001/jamaoncol.2023.2954" rel="noopener noreferrer" target="_blank">10.1001/jamaoncol.2023.2954</a></span><span><a href="http://paperpile.com/b/FgFYMN/V4C6" rel="noopener noreferrer" target="_blank">.</a></span></p><p><span>11. </span><span><a href="http://paperpile.com/b/FgFYMN/LOqlU" rel="noopener noreferrer" target="_blank">Lyu, Q. </a></span><span><a href="http://paperpile.com/b/FgFYMN/LOqlU" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/LOqlU" rel="noopener noreferrer" target="_blank"> Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential. </a></span><span><a href="http://paperpile.com/b/FgFYMN/LOqlU" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/FgFYMN/LOqlU" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>12. </span><span><a href="http://paperpile.com/b/FgFYMN/Ah0nd" rel="noopener noreferrer" target="_blank">Singhal, K. </a></span><span><a href="http://paperpile.com/b/FgFYMN/Ah0nd" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/Ah0nd" rel="noopener noreferrer" target="_blank"> Large Language Models Encode Clinical Knowledge. </a></span><span><a href="http://paperpile.com/b/FgFYMN/Ah0nd" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/FgFYMN/Ah0nd" rel="noopener noreferrer" target="_blank"> (2022).</a></span></p><p><span>13. </span><span><a href="http://paperpile.com/b/FgFYMN/lDUlL" rel="noopener noreferrer" target="_blank">Lehman, E. </a></span><span><a href="http://paperpile.com/b/FgFYMN/lDUlL" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/lDUlL" rel="noopener noreferrer" target="_blank"> Do We Still Need Clinical Language Models? </a></span><span><a href="http://paperpile.com/b/FgFYMN/lDUlL" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/FgFYMN/lDUlL" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>14. </span><span><a href="http://paperpile.com/b/FgFYMN/VfLjI" rel="noopener noreferrer" target="_blank">Wang, J. </a></span><span><a href="http://paperpile.com/b/FgFYMN/VfLjI" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/VfLjI" rel="noopener noreferrer" target="_blank"> On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. </a></span><span><a href="http://paperpile.com/b/FgFYMN/VfLjI" rel="noopener noreferrer" target="_blank">arXiv [cs.AI]</a></span><span><a href="http://paperpile.com/b/FgFYMN/VfLjI" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>15. </span><span><a href="http://paperpile.com/b/FgFYMN/Q4Xhw" rel="noopener noreferrer" target="_blank">Liao, K. P. </a></span><span><a href="http://paperpile.com/b/FgFYMN/Q4Xhw" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/Q4Xhw" rel="noopener noreferrer" target="_blank"> Development of phenotype algorithms using electronic medical records and incorporating natural language processing. </a></span><span><a href="http://paperpile.com/b/FgFYMN/Q4Xhw" rel="noopener noreferrer" target="_blank">BMJ</a></span><span><a href="http://paperpile.com/b/FgFYMN/Q4Xhw" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/FgFYMN/Q4Xhw" rel="noopener noreferrer" target="_blank">350</a></sup></span><span><a href="http://paperpile.com/b/FgFYMN/Q4Xhw" rel="noopener noreferrer" target="_blank">, h1885 (2015).</a></span></p><p><span>16. </span><span><a href="http://paperpile.com/b/FgFYMN/ECcXS" rel="noopener noreferrer" target="_blank">Zhang, Y. </a></span><span><a href="http://paperpile.com/b/FgFYMN/ECcXS" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/ECcXS" rel="noopener noreferrer" target="_blank"> Comparison of Chest Radiograph Captions Based on Natural Language Processing vs Completed by Radiologists. </a></span><span><a href="http://paperpile.com/b/FgFYMN/ECcXS" rel="noopener noreferrer" target="_blank">JAMA Netw Open</a></span><span><a href="http://paperpile.com/b/FgFYMN/ECcXS" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/FgFYMN/ECcXS" rel="noopener noreferrer" target="_blank">6</a></sup></span><span><a href="http://paperpile.com/b/FgFYMN/ECcXS" rel="noopener noreferrer" target="_blank">, e2255113 (2023).</a></span></p><p><span>17. </span><span><a href="http://paperpile.com/b/FgFYMN/9Zs2K" rel="noopener noreferrer" target="_blank">Medori, J. &amp; Fairon, C. Machine learning and features selection for semi-automatic ICD-9-CM encoding. in </a></span><span><a href="http://paperpile.com/b/FgFYMN/9Zs2K" rel="noopener noreferrer" target="_blank">Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents</a></span><span><a href="http://paperpile.com/b/FgFYMN/9Zs2K" rel="noopener noreferrer" target="_blank"> 84–89 (Association for Computational Linguistics, Los Angeles, California, USA, 2010).</a></span></p><p><span>18. </span><span><a href="http://paperpile.com/b/FgFYMN/l2nGi" rel="noopener noreferrer" target="_blank">OpenAI API. </a></span><span><a href="http://platform.openai.com" rel="noopener noreferrer" target="_blank">http://platform.openai.com</a></span><span><a href="http://paperpile.com/b/FgFYMN/l2nGi" rel="noopener noreferrer" target="_blank">.</a></span></p><p><span>19. </span><span><a href="http://paperpile.com/b/FgFYMN/lfPHu" rel="noopener noreferrer" target="_blank">Li, Y., Wang, J. &amp; Yu, B. Detecting Health Advice in Medical Research Literature. in </a></span><span><a href="http://paperpile.com/b/FgFYMN/lfPHu" rel="noopener noreferrer" target="_blank">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</a></span><span><a href="http://paperpile.com/b/FgFYMN/lfPHu" rel="noopener noreferrer" target="_blank"> 6018–6029 (Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 2021).</a></span></p><p><span>20. </span><span><a href="http://paperpile.com/b/FgFYMN/uPu1p" rel="noopener noreferrer" target="_blank">Yu, B., Li, Y. &amp; Wang, J. Detecting Causal Language Use in Science Findings. in </a></span><span><a href="http://paperpile.com/b/FgFYMN/uPu1p" rel="noopener noreferrer" target="_blank">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</a></span><span><a href="http://paperpile.com/b/FgFYMN/uPu1p" rel="noopener noreferrer" target="_blank"> 4664–4674 (Association for Computational Linguistics, Hong Kong, China, 2019).</a></span></p><p><span>21. </span><span><a href="http://paperpile.com/b/FgFYMN/nut9V" rel="noopener noreferrer" target="_blank">Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. </a></span><span><a href="http://paperpile.com/b/FgFYMN/nut9V" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/FgFYMN/nut9V" rel="noopener noreferrer" target="_blank"> (2018).</a></span></p><p><span>22. </span><span><a href="http://paperpile.com/b/FgFYMN/QLAXk" rel="noopener noreferrer" target="_blank">Lee, J. </a></span><span><a href="http://paperpile.com/b/FgFYMN/QLAXk" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/QLAXk" rel="noopener noreferrer" target="_blank"> BioBERT: a pre-trained biomedical language representation model for biomedical text mining. </a></span><span><a href="http://paperpile.com/b/FgFYMN/QLAXk" rel="noopener noreferrer" target="_blank">Bioinformatics</a></span><span><a href="http://paperpile.com/b/FgFYMN/QLAXk" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/FgFYMN/QLAXk" rel="noopener noreferrer" target="_blank">36</a></sup></span><span><a href="http://paperpile.com/b/FgFYMN/QLAXk" rel="noopener noreferrer" target="_blank">, 1234–1240 (2020).</a></span></p><p><span>23. </span><span><a href="http://paperpile.com/b/FgFYMN/bHjK3" rel="noopener noreferrer" target="_blank">Wei, J. </a></span><span><a href="http://paperpile.com/b/FgFYMN/bHjK3" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/bHjK3" rel="noopener noreferrer" target="_blank"> Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. </a></span><span><a href="http://paperpile.com/b/FgFYMN/bHjK3" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/FgFYMN/bHjK3" rel="noopener noreferrer" target="_blank"> (2022).</a></span></p><p><span>24. </span><span><a href="http://paperpile.com/b/FgFYMN/DU3ir" rel="noopener noreferrer" target="_blank">Taylor, R. </a></span><span><a href="http://paperpile.com/b/FgFYMN/DU3ir" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/DU3ir" rel="noopener noreferrer" target="_blank"> Galactica: A Large Language Model for Science. </a></span><span><a href="http://paperpile.com/b/FgFYMN/DU3ir" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/FgFYMN/DU3ir" rel="noopener noreferrer" target="_blank"> (2022).</a></span></p><p><span>25. </span><span><a href="http://paperpile.com/b/FgFYMN/Ua2Fh" rel="noopener noreferrer" target="_blank">Brown, T. B. </a></span><span><a href="http://paperpile.com/b/FgFYMN/Ua2Fh" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/Ua2Fh" rel="noopener noreferrer" target="_blank"> Language Models are Few-Shot Learners. </a></span><span><a href="http://paperpile.com/b/FgFYMN/Ua2Fh" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/FgFYMN/Ua2Fh" rel="noopener noreferrer" target="_blank"> (2020).</a></span></p><p><span>26. </span><span><a href="http://paperpile.com/b/FgFYMN/95vn0" rel="noopener noreferrer" target="_blank">Wei, J. </a></span><span><a href="http://paperpile.com/b/FgFYMN/95vn0" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/95vn0" rel="noopener noreferrer" target="_blank"> Emergent Abilities of Large Language Models. </a></span><span><a href="http://paperpile.com/b/FgFYMN/95vn0" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/FgFYMN/95vn0" rel="noopener noreferrer" target="_blank"> (2022).</a></span></p><p><span>27. </span><span><a href="http://paperpile.com/b/FgFYMN/nrYzn" rel="noopener noreferrer" target="_blank">Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. &amp; Iwasawa, Y. Large Language Models are Zero-Shot Reasoners. </a></span><span><a href="http://paperpile.com/b/FgFYMN/nrYzn" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/FgFYMN/nrYzn" rel="noopener noreferrer" target="_blank"> (2022).</a></span></p><p><span>28. </span><span><a href="http://paperpile.com/b/FgFYMN/eKzvW" rel="noopener noreferrer" target="_blank">Shi, F. </a></span><span><a href="http://paperpile.com/b/FgFYMN/eKzvW" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/eKzvW" rel="noopener noreferrer" target="_blank"> Large Language Models Can Be Easily Distracted by Irrelevant Context. </a></span><span><a href="http://paperpile.com/b/FgFYMN/eKzvW" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/FgFYMN/eKzvW" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>29. </span><span><a href="http://paperpile.com/b/FgFYMN/ktchQ" rel="noopener noreferrer" target="_blank">Wang, X. </a></span><span><a href="http://paperpile.com/b/FgFYMN/ktchQ" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/ktchQ" rel="noopener noreferrer" target="_blank"> Self-Consistency Improves Chain of Thought Reasoning in Language Models. </a></span><span><a href="http://paperpile.com/b/FgFYMN/ktchQ" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/FgFYMN/ktchQ" rel="noopener noreferrer" target="_blank"> (2022).</a></span></p><p><span>30. </span><span><a href="http://paperpile.com/b/FgFYMN/4UG7r" rel="noopener noreferrer" target="_blank">Savova, G. K. </a></span><span><a href="http://paperpile.com/b/FgFYMN/4UG7r" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/4UG7r" rel="noopener noreferrer" target="_blank"> Use of Natural Language Processing to Extract Clinical Cancer Phenotypes from Electronic Medical Records. </a></span><span><a href="http://paperpile.com/b/FgFYMN/4UG7r" rel="noopener noreferrer" target="_blank">Cancer Res.</a></span><span><a href="http://paperpile.com/b/FgFYMN/4UG7r" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/FgFYMN/4UG7r" rel="noopener noreferrer" target="_blank">79</a></sup></span><span><a href="http://paperpile.com/b/FgFYMN/4UG7r" rel="noopener noreferrer" target="_blank">, 5463–5470 (2019).</a></span></p><p><span>31. </span><span><a href="http://paperpile.com/b/FgFYMN/tvjz" rel="noopener noreferrer" target="_blank">Beam, K. </a></span><span><a href="http://paperpile.com/b/FgFYMN/tvjz" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/FgFYMN/tvjz" rel="noopener noreferrer" target="_blank"> Performance of a Large Language Model on Practice Questions for the Neonatal Board Examination. </a></span><span><a href="http://paperpile.com/b/FgFYMN/tvjz" rel="noopener noreferrer" target="_blank">JAMA Pediatr.</a></span><span><a href="http://paperpile.com/b/FgFYMN/tvjz" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/FgFYMN/tvjz" rel="noopener noreferrer" target="_blank">177</a></sup></span><span><a href="http://paperpile.com/b/FgFYMN/tvjz" rel="noopener noreferrer" target="_blank">, 977–979 (2023).</a></span></p><p><span>32. </span><span><a href="http://paperpile.com/b/FgFYMN/K2Wb" rel="noopener noreferrer" target="_blank">Murk, W., Goralnick, E., Brownstein, J. S. &amp; Landman, A. B. Quality of Layperson CPR Instructions From Artificial Intelligence Voice Assistants. </a></span><span><a href="http://paperpile.com/b/FgFYMN/K2Wb" rel="noopener noreferrer" target="_blank">JAMA Netw Open</a></span><span><a href="http://paperpile.com/b/FgFYMN/K2Wb" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/FgFYMN/K2Wb" rel="noopener noreferrer" target="_blank">6</a></sup></span><span><a href="http://paperpile.com/b/FgFYMN/K2Wb" rel="noopener noreferrer" target="_blank">, e2331205 (2023).</a></span></p>