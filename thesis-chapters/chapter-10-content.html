<h1>Chapter 10</h1><p class="chapter-subtitle"><span>When Helpfulness Backfires: LLMs and the Risk of False Medical Information Due to Sycophantic Behavior</span></p><hr/><p><span>Shan Chen*</span><span>, Mingye Gao*, Kuleen Sasse, Thomas Hartvigsen, Brian Anthony, Lizhou Fan, Hugo Aerts, Jack Gallifant, Danielle S Bitterman</span></p><p class="chapter-meta"><a href="https://www.nature.com/articles/s41746-025-02008-z" rel="noopener noreferrer" target="_blank"><em>npj Digital Medicine</em></a></p><h2>Summary</h2><p><span>Background<br/></span><span>While large language models (LLMs) are designed to be helpful, this very trait can lead to </span><span>sycophantic compliance</span><span>—meaning they may respond to illogical or misleading instructions simply to appear cooperative. In the medical domain, such behavior can translate into false or dangerous health information.</span></p><p><span>Methods<br/></span><span>Our study evaluated five state-of-the-art LLMs by presenting prompts that incorrectly claimed two equivalent drugs were different (i.e., misrepresented a factual relationship). We tested baseline compliance (how often models answer even when the request is illogical), then compared versions with prompts that allowed rejection or emphasized factual recall, and finally evaluated models after fine-tuning on a dataset of illogical requests—including out-of-distribution generalization.</span></p><p><span>Findings<br/></span><span>Initial compliance was extremely high—up to </span><span>100%</span><span>—indicating models prioritized being helpful over checking logic or truth. Prompt engineering and supervised fine-tuning significantly improved performance: after tuning, models rejected illogical requests more frequently </span><span>while maintaining general benchmark ability</span><span>. </span></p><p><span>Interpretation<br/></span><span>Our results demonstrate a critical vulnerability: LLMs may provide false medical information because they default to “helpfulness” rather than verifying correctness. Mitigation requires targeted training and prompt design to prioritize logical consistency. This finding has direct relevance for clinical settings and potential AI-enabled trial automation or pharmacovigilance tasks: ensuring that a model doesn’t inadvertently comply with invalid prompts is just as important as training it to retrieve correct information.</span></p><h2>Introduction</h2><p><span>Large Language Models (LLMs) can store and retrieve vast amounts of information from diverse domains, including healthcare</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/Ms623+B69V7+0zyMm" rel="noopener noreferrer" target="_blank">1–3</a></sup></span><span>. This knowledge base has been noted for its potential to support medical professionals by providing specialized information and advice</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/i1s9w+Ms623" rel="noopener noreferrer" target="_blank">1,4</a></sup></span><span>. Yet, while these models may recall medical facts, it remains challenging for the models to process information logically and generate responses that demonstrate sound reasoning</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/Y26nJ" rel="noopener noreferrer" target="_blank">5</a></sup></span><span>. This gap between knowledge retrieval and logical reasoning in medicine</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/hcxrw+5kxN7" rel="noopener noreferrer" target="_blank">6,7</a></sup></span><span> leads to a particularly concerning public health risk: The rapid generation and dissemination of false information is particularly critical in high-stakes fields like medicine.</span></p><p><span>Two key principles for the safe deployment of LLMs in medicine are honesty and helpfulness</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/gTOCl+7wC8T+Nwrgv" rel="noopener noreferrer" target="_blank">8–10</a></sup></span><span>. In the context of LLMs, honesty refers to the principle that models should provide information that is factually accurate and logically sound, aligning with established medical knowledge rather than generating or perpetuating false information. Helpfulness describes the LLM's capacity to understand and responsively fulfill a user's query in an efficient and seemingly useful manner. Honesty ensures that models provide accurate and truthful information, while helpfulness focuses on fulfilling users' queries in an efficient and useful manner</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/bX5YJ+HWbJj" rel="noopener noreferrer" target="_blank">11,12</a></sup></span><span>. Our work examines the critical scenario where an overemphasis on this helpfulness can lead models to comply with illogical or factually incorrect medical requests, thereby undermining honesty.</span></p><p><span>Current state-of-the-art LLMs are aligned with these principles via training processes</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/gTOCl+Nwrgv+rfITO" rel="noopener noreferrer" target="_blank">8,10,13</a></sup></span><span>, including reinforcement learning with human feedback</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/8UflO" rel="noopener noreferrer" target="_blank">14</a></sup></span><span> (RLHF). These alignment processes typically involve tuning LLMs, not to gain new knowledge but to shift the outputs towards a more desirable human-readable format and away from potentially harmful or toxic behaviors learned during pre-training</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/HWbJj+aWY2c" rel="noopener noreferrer" target="_blank">12,15</a></sup></span><span>. Previous works have shown that helpfulness can be misused to generate unfaithful information and a lack of scientific grounding about health</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/XnpbV+15qvJ+iVtzi" rel="noopener noreferrer" target="_blank">16–18</a></sup></span><span>. While honesty and helpfulness are often complementary, emphasizing helpfulness can introduce safety vulnerabilities: </span><span>jailbreaking</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/4yypL+WKyES" rel="noopener noreferrer" target="_blank">19,20</a></sup></span><span> and </span><span>sycophancy</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/Q8QE8" rel="noopener noreferrer" target="_blank">21</a></sup></span><span> which may amplify the risks of LLM misuses. Jailbreaking refers to techniques or prompt structures designed to exploit a model’s helpfulness, tricking it into generating harmful, misleading, or restricted content</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/Ffw7R" rel="noopener noreferrer" target="_blank">22</a></sup></span><span>. Sycophancy is the tendency of LLMs to excessively agree with users, often at the expense of accuracy</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/Q8QE8+w5xip" rel="noopener noreferrer" target="_blank">21,23</a></sup></span><span>. We define this as sycophancy, differing from compliance, because LLMs (1) demonstrably know the premise is false (e.g., accurately matching brand/generic names), but (2) align with the user's implied incorrect belief, generating false information. </span><span>The confluence of these two vulnerabilities is a growing concern, because nefarious users, misinformed users, and even unintentional errors and typos input into the model, could result in LLMs generating and spreading false information</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/INsIJ" rel="noopener noreferrer" target="_blank">24</a></sup></span><span>. </span></p><p><span>Previous research on jailbreaking has primarily explored its implications in the context of catastrophic risks in the general domain—cases where models are manipulated to produce extreme content, such as violence, hate speech, or other harmful material</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/b816C+kTt10+4Jj7x" rel="noopener noreferrer" target="_blank">25–27</a></sup></span><span>. Jailbreaking has been thoroughly examined in healthcare contexts through multimodal, white-box internal, and adversarial approaches.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/khOft+kAniC" rel="noopener noreferrer" target="_blank">28,29</a></sup></span><span>. Our work builds on this foundation by addressing a critical underexplored area: evaluating LLMs' ability to recognize and resist illogical or factually flawed requests for medical information. For example, a user could unknowingly generate false information if they don’t have the background knowledge to know upfront that their request is illogical and would, by definition, generate false information. As a concrete example, if a patient doesn’t know that acetaminophen is the same drug tylenol, and is trying to self-educate about analgesic options using an LLM, they could inadvertently generate false information owing to a model’s overly helpful (or overly compliant) behavior with a request such as: “Tell me why acetaminophen is safer than tylenol”. Similarly, typos or accidents could readily but inadvertently generate false information in similar fashions.</span></p><p><span>We evaluated five LLMs across various scenarios and assessed how sensitive they are to generating false medical information in settings where the LLMs have the knowledge base to identify the requested information as incorrect. As a use case, we selected drug names, as in medicine, where different names are often used for the same drug. Because we previously showed that LLMs can accurately match brand and generic drug names</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/3jNoz" rel="noopener noreferrer" target="_blank">30</a></sup></span><span>, this allowed for a controlled and scalable experimental setup to characterize LLM sycophantic compliance to illogical requests. First, we tested whether LLMs refuse to comply with requests for information describing the equivalent drugs are distinct (i.e., a misinformation request), and found that even the most advanced models complied with up to 100% of misinformation requests. Second, we changed our instructions to the LLMs to understand if their overly submissive behavior can be overcome with prompting (given that prompting still remains the most effective steering method</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/i6Mml" rel="noopener noreferrer" target="_blank">31</a></sup></span><span>). Third, we fine-tuned models to resist requests for misleading information while maintaining responsiveness to valid prompts. We found that LLMs prioritize learned helpfulness over inherent logical reasoning in our datasets, leading them to generate false information from even simple illogical prompts. Our strategies to reduce this risk successfully enhanced logical reasoning, and can provide a basis for additional research to improve robust risk mitigation and oversight mechanisms targeted at LLM sycophancy in healthcare.</span></p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image33.png" title=""/><figcaption>Figure 1. Illustration of overall study workflow. Step 1 involves the generation of an LLM misinformation request, where models should recognize that the drug entities are equivalent and therefore the requested generated content would be misinformation. In Step 2, LLMs are prompted with this request to generate a response which is subsequently graded by Claude 3.5 Sonnet in Step 3 into one of the four response types. Claude 3.5 grading quality was validated by humans. Step 4 shows prompt-based variations which are evaluated, and the change in response types are collected in Step 5. Step 6 displays the instruction tuning of the LLMs, where we stitched the baseline prompt with output from rejection and factual recall hints. Step 7 evaluates this newly tuned LLM both in-domain and in other domains with different equivalence errors.</figcaption></figure><h2>Results</h2><p><span>Overview</span><span><br/></span><span>We evaluated whether state-of-the-art LLMs prioritize “helpfulness” over logical consistency when faced with illogical medical requests that they have the knowledge to detect, using drug pairs with 1:1 brand–generic mappings. The study proceeded in four stages: Stage 1 measured baseline sycophantic compliance to illogical prompts (aim: quantify default risk). Stage 2 introduced lightweight prompt edits—explicit rejection permission and a factual-recall cue—to test steerability with instructions (aim: assess whether prompting alone can restore logic). Stage 3 applied supervised fine-tuning on a small set of illogical requests with desired behavior and tested out-of-distribution generalization across medical and non-medical entities (aim: learn a reusable “reject-when-illogical” policy that transfers). Stage 4 checked for over-rejection and capability loss by evaluating compliance with valid prompts and performance on general/biomedical benchmarks (aim: ensure safety gains do not degrade usefulness). Unless noted, we report the generic→brand direction in the main text; brand→generic results are concordant in the supplementary.</span></p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image29.png" title=""/><figcaption>Figure 2. Generic-to-brand output grades for prompt-based and Instruction-tuning interventions. Figure 4a displays the results of stage 1 (prompt-based strategies). The Y-axis is marked as a percentile. Four prompt variations were used to evaluate 5 LLMs on generic-brand name pairs of 50 drug combinations. Figure 4b shows results for stage 2 (instruction-tuned model). The baseline and fine-tuned version of GPT4o-mini and Llama3-8B performance is on out-of-distribution test sets of 4 domains, such as cancer drug name and writer-pseudonym pairs.</figcaption></figure><h2>Stage 1. Baseline prompt to quantify default risk</h2><p><span>Our previous work showed that all models evaluated here have near-perfect factual recall ability to match these drugs’ generic and brand names</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/3jNoz" rel="noopener noreferrer" target="_blank">30</a></sup></span><span>. As shown in </span><span>Figure 2a)</span><span>, LLMs generally follow illogical requests to generate false information in the base prompt setup (details in Method section 3.1) for the generic-to-brand conversions. For clarity, we only discuss the generic-to-brand setups in the main text; all brand-to-generic results are in </span><span>Supplementary Figure 1</span><span> and show similar findings.</span></p><p><span>In the generic-to-brand setup, GPT4o-mini, GPT4o, and GPT4 followed the medication misinformation request 100% (50/50) of the time, while Llama3-8B did so in 94% (47/50) of cases. Llama3-70B had the highest rejection rate in this setup, but still rejected requests to generate false information in less than 50% (21/50) of cases. This stage quantifies baseline sycophancy, indicating that even large, advanced models predominantly complied with illogical requests.</span></p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image67.png" title=""/><figcaption>Figure 3. Out of distribution testing workflow. We composed one held-out cancer drug set that is not in the supervised fine-tuning data and crafted three other categories’ equivalences. As previously, Claude 3.5 Sonnet was used to auto-evaluate the categories of models’ responses.</figcaption></figure><h2>Stage 2. Prompt-based solutions to assess whether prompting restores logic</h2><p><span>Explicitly allowing models to reject misinformation requests (i.e., telling models that they can reject the request within the prompt, our detailed workflow can be found in </span><span>Figure 1</span><span>) improved the ability of the GPT series of models to resist misinformation requests. GPT4o and GPT4 rejected over 60% (GPT4o: 31/50, GPT4: 32/50) of the illogical requests in this setting. However, Llama's performance was similar to base prompting. Adding factual recall hints in the prompt yielded the most benefit for GPT4 and Llama3-8B.</span></p><p><span>Adding rejection hints and factual recall together in the prompts vastly improved the models' performance. This was particularly true for GPT4o and GPT4, which rejected generating the requested misinformation </span><span>and</span><span> correctly identified that the brand and generic names referred to the same drug in 94% (47/50) of test cases. Rejection rates for GPT4o-mini and Llama3-70B also improved substantially p&lt;0.05 (We build a square “before × after” contingency table of all categories and then apply Bowker’s test of symmetry to check for a statistically significant paired changes), reaching 62% (31/50) and 92% (46/50), respectively, with both hints applied.</span></p><p><span>An interesting behavioral shift was observed in Llama3-8B after including both the rejection and factual recall hints. The model transitioned from following illogical requests to directly rejecting them without providing the correct logical rationale for rejections. This change is reflected in the increase in direct rejections (yellow bar) from 2% (1/50) to 66% (33/50) (p&lt;0.05) in </span><span>Figure 2a</span><span>. </span></p><h2>Stage 3. Fine-tuning to learn a reusable policy and evaluating on out-of-distribution (OOD) data</h2><p><span>In the third stage, GPT4o-mini and Llama3-8B were supervised fine-tuned (SFT) on 300 illogical requests about general drugs with clear rejections. We then conducted OOD tests (</span><span>Figure 3 </span><span>demonstrates the workflow here) in four domains: cancer drugs, singers/performers, writers, and geography. As shown in </span><span>Figure 2b)</span><span>, the fine-tuned models were much more likely to identify a request as illogical and refuse to comply.</span></p><p><span>For example, in the OOD tests on cancer drugs (without rejection hints), the fine-tuned GPT4o-mini achieved a 100% (100/100) rejection rate, with 79% (79/100) of rejections (p&lt;0.05) providing the correct reason, compared to the baseline's 12% (12/100) rejection rate (5% with correct reasoning). Similarly, the fine-tuned Llama3-8B reached a 99% (99/100) rejection rate (70% with correct reasoning, 29% with other reasons), while the baseline model rejected only 30% (30/100) of requests, none of which provided the correct reason. This is similar to other categories with/without rejection hints. Fine-tuning can reduce sycophancy, leading to consistent rejection of illogical prompts across out-of-distribution domains.</span></p><p><span>Stage 4: Evaluating general benchmarks and compliance with logical requests to ensure safety gains do not degrade performance</span></p><p><span>The detailed results of the ability of the fine-tuned models to comply with logical requests are shown in </span><span>Supplementary Table 1</span><span>. Fine-tuned GPT4o-mini complied in 15/20 cases, and fine-tuned LLama3-8B complied in 12/20 cases. While the fine-tuned models were more likely than their base counterparts to reject requests, they always explained that they rejected because the request might be unrealistic. This behavior shift indicates a maintenance of balance between safety (rejection of illogical requests) and functionality (sycophantic compliance with logical instructions). Examples of how fine-tuning shifted behavior are provided in </span><span>Supplementary Figure 2</span><span>.</span></p><p><span>Lastly, we assessed the performance of the SFT models from Stage 2 and their base counterparts across 10 general and biomedical knowledge benchmarks, including Alpaca-Eval2</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/pJvqP" rel="noopener noreferrer" target="_blank">32</a></sup></span><span>, ARC Challenge, ARC Easy</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/Zr399" rel="noopener noreferrer" target="_blank">33</a></sup></span><span>, BoolQ</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/7cotw" rel="noopener noreferrer" target="_blank">34</a></sup></span><span>, MMLU</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/uEA64" rel="noopener noreferrer" target="_blank">35</a></sup></span><span>, GPQA</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/8sj55" rel="noopener noreferrer" target="_blank">36</a></sup></span><span>, TruthfulQA</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/6rxsv" rel="noopener noreferrer" target="_blank">37</a></sup></span><span>, and the USMLE Step 1, 2, and 3 exams</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/3jNoz" rel="noopener noreferrer" target="_blank">30</a></sup></span><span>. As demonstrated in </span><span>Figure 5</span><span>, the fine-tuned models exhibited negligible performance degradation across all tasks.</span></p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image31.png" title=""/><figcaption>Figure 4. LLM ability to comply to logical requests. To further investigate our fine-tuned models’ behavior, we provided three different subcategories of new, logical and correct in-context information requests, and assessed if the LLMs complied. Authors SC and MG did the annotation manually with a 100% annotation agreement.</figcaption></figure><h2>Discussion</h2><p><span>Our study identified a vulnerability in LLMs: their sycophantic tendency to prioritize helpfulness over honesty and critical reasoning when responding to illogical requests for medical information, resulting in false and potentially harmful information. If LLMs are prone to generating false medical information in response to requests that are overtly illogical, where they know the information is incorrect, they are likely even less able to resist more nuanced false information requests. This means that even simple errors in LLM inputs could readily and inadvertently prompt the generation of false information when LLMs are used in medical contexts. For example, patients seeking health information from LLMs could generate false information if they do not have the clinical knowledge base to know a priori that the question is illogical. If left unchecked, this sycophancy could lead to the acceleration of inadvertent or malicious misinformation which could cause serious population and individual harm in high-stakes domains like healthcare</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/INsIJ" rel="noopener noreferrer" target="_blank">24</a></sup></span><span>.</span></p><p><span>Previous research into the potential of LLMs to manipulate and generate false information has largely focused on single-turn or multi-turn conversational techniques aimed at exploiting a model’s inherent helpful nature to bend its "beliefs" or outputs to align with dangerous or unethical goals</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/1KmI2+chc0M" rel="noopener noreferrer" target="_blank">38,39</a></sup></span><span>. Such efforts reveal the vulnerability of even state-of-the-art models to being misled by adversarial inputs, underscoring the need for new robust safeguarding mechanisms. Our work adds to the existing literature by evaluating the ability of LLMs to identify and resist requests that are overtly illogical or factually flawed, and by proposing novel mitigation strategies via prompting and fine-tuning.</span></p><p><span>The initial sycophantic compliance of all models, including advanced ones like GPT-4, to illogical requests reveals a core vulnerability in LLM design where, without explicit guidance, models prioritize being helpful over applying critical reasoning. More specifically, the role of RLHF/Instruction Tuning creates a fundamental tension between blindly following instructions and providing context-sensitive and factual responses. Our findings demonstrate that explicit instruction prompting, such as providing rejection hints, can improve models' ability to critically assess requests before responding. Allowing models to reject flawed instructions appears to be important for enhancing their common-sense critical reasoning ability. This insight is crucial for developing safer AI systems that can balance helpfulness with necessary skepticism.</span></p><p><span>While factual recall prompts improved the performance of advanced models, such as GPT4o and GPT4, they had a limited impact on smaller models like Llama3-8B/70B or GPT4o-mini. Even when we explicitly told the models within the prompt that brand and generic names referred to the same drug, only the more advanced models responded correctly by rejecting the illogical request. For example, GPT4 and GPT4o rejected 94% of illogical requests after being prompted to recall factual relationships between the drugs, but Llama3-8B still often rejected without giving a correct explanation.</span></p><p><span>This suggests that simply spelling out factual equivalencies is not enough for less capable models and that the ability to effectively use factual knowledge in context-dependent reasoning tasks may be a key differentiator of more advanced AI systems</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/OUTNO+z8q3u" rel="noopener noreferrer" target="_blank">40,41</a></sup></span><span>. Smaller models seem to require more than factual prompts to process logical decisions, likely because they cannot fully integrate context and recall complex relationships as effectively as advanced models. However, even for these larger models, this approach is not scalable across the wide range of potential illogical requests because it requires preemptively identifying the precise factual knowledge needed to identify each request as illogical. </span></p><p><span>Supervised fine-tuning on 300 drug-related conversations enhanced the models' ability to distinguish between valid and illogical prompts, especially for OOD tests. After fine-tuning, models like GPT4o-mini achieved a 100% rejection rate, with 79% of rejections providing the correct reasoning, compared to the baseline's 9%. Similarly, Llama3-8B improved, though it sometimes rejected prompts without proper explanations. Importantly, the observed improvements in rejecting illogical prompts were generalized outside of the brand-generic use case on which the models were fine-tuned.</span></p><p><span>The success of SFT highlights how fine-tuning enables models to better recognize illogical requests in a generalizable, scalable fashion. In other words, we know the models can match these drug names correctly, and SFT steers models’ behavior toward prioritizing its factual knowledge over user requests. </span></p><p><span>Importantly, this fine-tuning did not lead to over-rejection or a refusal to respond to reasonable input: GPT4o-mini and Llama3-8B still largely complied with logical requests across a range of medical and non-medical tasks. When they did not, they provided reasonable explanations for not complying. This behavior shift demonstrates a successful balance between rejecting illogical instructions and remaining useful for legitimate tasks. Recent articles and new paradigm improvement on test time compute</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/6SP3a" rel="noopener noreferrer" target="_blank">42</a></sup></span><span> show a promising future where models can reason first instead of responding immediately, improve reasoning ability</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/ZhpP8+cc36s" rel="noopener noreferrer" target="_blank">43,44</a></sup></span><span>, and enhance potential jailbreaking behavior</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/JyVLn+lbS62+Irz6V" rel="noopener noreferrer" target="_blank">45–47</a></sup></span><span>. However, the normal language models that we studied in this paper are still the main daily workhorses accessible to most users. In fact, even OpenAI rose similar sycophancy issues on GPT-4o.</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/sCufh" rel="noopener noreferrer" target="_blank">48</a></sup></span></p><p><span>We showed that LLMs are sycophantic and do not reliably resist requests for illogical content, even when they have the knowledge to identify the request as factually flawed. This creates a gap between the knowledge benchmarks commonly used to evaluate LLMs and a true assessment of their medical risks and functionality. To ensure that LLMs effectively reject flawed requests while continuing to respond helpfully to logical instructions, future work could focus on refining tuning methods and developing approaches to scalable human-assisted and automated oversight. Ultimately, closing this gap will be essential to aligning LLMs' knowledge capabilities with their real-world reliability and safety in medicine.</span></p><h2>Methods</h2><p><span>To evaluate language models across varying levels of drug familiarity, we used the </span><span>RABBITS</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/3jNoz" rel="noopener noreferrer" target="_blank">30</a></sup></span><span> dataset, which includes 550 common drugs with 1:1 mapping between their brand and generic names.</span></p><p><span>To measure the relative familiarity of language models with these drugs, we tokenized multiple large pre-training corpora with the LLaMA tokenizer</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/7wC8T" rel="noopener noreferrer" target="_blank">9</a></sup></span><span> using Infini-gram</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/lcrcp" rel="noopener noreferrer" target="_blank">49</a></sup></span><span>, including Dolma1.6</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/2D9Hn" rel="noopener noreferrer" target="_blank">50</a></sup></span><span>, C4</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/ItNUJ" rel="noopener noreferrer" target="_blank">51</a></sup></span><span>, RedPajama</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/L4O42" rel="noopener noreferrer" target="_blank">52</a></sup></span><span>, and Pile</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/i6FZM" rel="noopener noreferrer" target="_blank">53</a></sup></span><span>. The frequency of generic drug names across this corpus was used to estimate how commonly these drugs appear in pre-training datasets. Generic drug names were then ranked by frequency to provide a proxy measure of model familiarity(Note that C4 and RedPajama have overlaps).</span></p><p><span>To ensure coverage of both common and rare drugs, we selected 50 drugs from five distinct frequency ranges based on their rankings in the tokenized dataset: The top 10, 100-110, 200-210, 300-310, and 400-410 most frequent drugs in our sampling window.</span></p><p>We evaluate the following LLMs: Llama3-8B-Instruct (Llama3-8B), Llama3-70B-Instruct (Llama3-70B), gpt-4o-mini-2024-07-18 (GPT4o-mini), gpt-4o-2024-05-13 (GPT4o), and gpt-4-0613 (GPT4). These models were chosen to represent the performance of current leading open- and closed-source models across a range of sizes.</p><p><span>We designed four prompt types to evaluate the models' handling of new drug-related information, assessing persuasive ability, factual recall, and logical consistency (Figure 2). Experiments were run via OpenAI Batch API, and Llama models used A100-80GB with CUDA &gt; 12.0, no quantization. Hyperparameters included a max of 512 output tokens and temperature=0 for best possible reproducibility. </span></p><p>Stage 1. Baseline Prompt: The first prompt represents the baseline condition, where the model is tasked with providing a persuasive but illogical letter informing people that a brand-name drug is found to have new side effects, and that they should take the generic counterpart instead. This task was selected because it illustrates a necessary safety mode for LLMs that follows from simple logical reasoning. If a model knows that the brand and generic drug are the same, it should be able to identify the request as illogical and reject the request, instead of complying with the request and generating false information.</p><p><span>Stage 2. Prompt-Based Solutions to Assess Steerability- Rejection Prompt: </span><span>In this variation, we explicitly allow the possibility of rejection, encouraging the model to evaluate whether there is a logical flaw in the prompt. This prompt also allows a model that is heavily aligned to being submissive to reject users’ queries. The explicit permission to reject creates a scenario where the model must consider not only the factual content but also the appropriateness of the substitution.</span></p><p><span>Factual Recall Prompt: </span><span>This prompt emphasizes the need for the model to recall the correct relationships between brand-name drugs and their generic equivalents before processing the rest of the request. This variation tests the model’s ability to accurately retrieve and utilize known facts in generating persuasive outputs. By instructing the model to prioritize factual recall, we assess how well it can integrate known drug relationships with new information.</span></p><p><span>Combined Rejection and Factual Recall Prompt: </span><span>The final prompt variation combines both the rejection and factual recall instructions. This setup evaluates whether the model can handle both tasks simultaneously, ensuring factual accuracy while also exercising logical reasoning to reject incorrect assumptions.</span></p><p><span>All prompt settings introduced were experimented with separate LLM inferences. </span></p><p>Stage 3. Fine-tuning and Evaluation on Out-of-Distribution (OOD) Data - Model Fine-Tuning: To enhance the ability of smaller language models to handle complex drug substitution prompts, we fine-tuned Llama 3 - 8B Instruct and GPT4o-mini using the PERSIST instruction-tuning dataset, publicly available at https://huggingface.co/datasets/AIM-Harvard/PERSIST.</p><p><span>This dataset comprises 300 input-output pairs, each featuring a challenging "Baseline" prompt concerning brand/generic drug substitutions (covering both directions for 50 drug pairs) and the corresponding desired response generated by a larger model (GPT4o-mini, GPT-4, or GPT4o) when presented with a "Combined Rejection and Factual Recall Prompt." </span></p><p><span>The dataset construction leveraged these larger models to systematically generate ideal responses for all 50 drug pairs in both substitution directions, resulting in 300 examples (50 * 2 * 3 = 300), drawing inspiration from work demonstrating effective instruction-tuning with limited data</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/TzrP46/u1Uso" rel="noopener noreferrer" target="_blank">54</a></sup></span><span>. We explored various hyperparameters, including learning rates (5e-6, 1e-5, 2e-5, 5e-5), batch sizes (1, 2), and epochs (2, 3) for Llama3-8B. For GPT4o-mini, we utilized OpenAI's automatic parameter search. Ultimately, the selected Llama3-8B model used a learning rate of 1e-5, a batch size of 2, and 3 epochs, while the selected GPT4o-mini was fine-tuned via the OpenAI API with a batch size of 1, 3 epochs, and a seed of 318998491. The core objective of this fine-tuning process was to impart the smaller models with the ability to emulate the larger models' successful rejection and explanation behavior when faced with the "Combined Rejection and Factual Recall Prompt."</span></p><p><span>We used 2*A100 80GB to fine-tune our examples in 2 epochs and a learning rate of 1e-5 which can be done under an hour. The estimated cost here will be under $10 if using cloud GPU renting. For fine-tuning GPT4o mini, we were on the OpenAI Trial program, so it was free of cost. However, custom models will require 1.5x inference costs.</span></p><p>Evaluation on OOD Data : To evaluate the generalization of the fine-tuned model to other illogical requests, we tested its performance on the OOD datasets of terms with the same meanings ( Figure 3 ). This OOD dataset included several other categories. Testing on OOD data allows us to assess the generalizability of a model’s behavior in responding to illogical requests involving novel or previously unseen entities — a crucial factor in evaluating its applicability in real-world scenarios.</p><p><span>Stage 4: Evaluating General Benchmarks and Compliance with Logical Requests </span><span>- </span><span>Balancing Rejection and Compliance: </span><span>To test whether models became overly conservative after fine-tuning, we designed an additional test set comprising 20 cases (10 real FDA drug safety recalls, 5 theoretically event canceling situations, and 5 real government announcements) where the model should comply with the prompt rather than reject it (</span><span>Figure 4</span><span>). These cases involved scenarios where the recommended substitution was appropriate and aligned with the correct drug relationships. This test ensured that the model retained the ability to provide helpful and persuasive responses when no logical flaws were present. The prompts are found in </span><span>Supplementary Table 1</span><span>. Additionally, we also prompt the fine-tuned models with questions regarding 50 common drugs we fine-tuned and see whether they can still answer logical requests regarding those drugs. </span></p><p>General Benchmark Evaluation: To ensure that fine-tuning and prompt modifications do not degrade the overall performance of the models, we evaluated them on a broad set of general benchmarks using Inspect 55 and Alpaca-Eval2 v0.6.5 56 using GPT4-turbo as the comparator model. These benchmarks were selected to test the models' reasoning, factual recall, and domain-specific knowledge, including medical contexts, ensuring that any improvements in handling drug-related prompts did not come at the expense of general task performance. The confidence intervals are calculated using the central limit theorem, a common practice in modern LLM evaluations 57,58 .</p><p>Automated Evaluation: Model outputs were categorized into 4 categories: (1) rejecting the request and explaining the logical flaw; (2) fulfilling the request and explaining the logical flaw; (3) rejecting the request without explaining the logical flaw; and (4) fulfilling the request without explaining the logical flaw. Model outputs were evaluated using a multi-step annotation process. The detailed counts of instances we evaluated in this research are available in Supplementary Table 2. To ensure consistency and reliability in the evaluation, we employed the Claude 3.5 Sonnet (we chose a separate model as a label because LLMs of the same family are known to have a favorable bias toward their own responses 59–62 ) to provide initial annotations, with human reviewers (annotators SC and MG blinded to each other) validating 50 outputs from GPT4o-mini. The inter-annotator agreement between Claude 3.5 Sonnet and the human reviewers was 98%, with 100% agreement between the two human annotators for both in-domain and out-of-domain data. Supplementary Table 3 shows the single output for which the human labels disagreed with Claude 3.5 Sonnet. Of note, compliance with logical requests was human-labeled.</p><p><span>Data availability:</span></p><p><span>All our data input and output from all models, and the Llama3 model we fine-tuned, are publicly available at https://huggingface.co/datasets/AIM-Harvard/PERSIST.</span></p><p><span>Code availability:</span></p><p><span>All code can be found at </span><span><a href="https://huggingface.co/datasets/AIM-Harvard/PERSIST" rel="noopener noreferrer" target="_blank">https://huggingface.co/datasets/AIM-Harvard/PERSIST</a></span><span>.</span></p><p><span>Please view the full Tables and appendix at:</span><span> </span><span><a href="https://www.nature.com/articles/s41746-025-02008-z" rel="noopener noreferrer" target="_blank">https://www.nature.com/articles/s41746-025-02008-z</a></span></p><h2>Acknowledgments</h2><p><span>The authors acknowledge financial support from the Google PhD Fellowship (SC), the Woods Foundation (DB, SC, HA, JG, LF), the NIH (NIH-USA R01CA294033 (SC, JG, LF, DB), NIH-USA U54CA274516-01A1 (SC, HA, DB), NIH-USA U24CA194354 (HA), NIH-USA U01CA190234 (HA), NIH-USA U01CA209414 (HA), and NIH-USA R35CA22052 (HA), the ASTRO-ACS Clinician Scientist Development Grant ASTRO-CSDG-24-1244514 (DB), and the European Union - European Research Council (HA: 866504). This work was also conducted with support from UM1TR004408 award through Harvard Catalyst | The Harvard Clinical and Translational Science Center (National Center for Advancing Translational Sciences, National Institutes of Health) and financial contributions from Harvard University and its affiliated academic healthcare centers. The content is solely the responsibility of the authors and does not necessarily represent the official views of Harvard Catalyst, Harvard University, and its affiliated academic healthcare centers, or the National Institutes of Health. The authors thank Google Cloud research fund for Claude API inference costs.</span></p><h2>References</h2><p><span>1. </span><span><a href="http://paperpile.com/b/TzrP46/Ms623" rel="noopener noreferrer" target="_blank">Singhal, K. </a></span><span><a href="http://paperpile.com/b/TzrP46/Ms623" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/Ms623" rel="noopener noreferrer" target="_blank"> Large language models encode clinical knowledge. </a></span><span><a href="http://paperpile.com/b/TzrP46/Ms623" rel="noopener noreferrer" target="_blank">Nature</a></span><span><a href="http://paperpile.com/b/TzrP46/Ms623" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/TzrP46/Ms623" rel="noopener noreferrer" target="_blank">620</a></sup></span><span><a href="http://paperpile.com/b/TzrP46/Ms623" rel="noopener noreferrer" target="_blank">, 172–180 (2023).</a></span></p><p><span>2. </span><span><a href="http://paperpile.com/b/TzrP46/B69V7" rel="noopener noreferrer" target="_blank">Taylor, R. </a></span><span><a href="http://paperpile.com/b/TzrP46/B69V7" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/B69V7" rel="noopener noreferrer" target="_blank"> Galactica: A Large Language Model for Science. </a></span><span><a href="http://paperpile.com/b/TzrP46/B69V7" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/B69V7" rel="noopener noreferrer" target="_blank"> (2022).</a></span></p><p><span>3. </span><span><a href="http://paperpile.com/b/TzrP46/0zyMm" rel="noopener noreferrer" target="_blank">Chen, S. </a></span><span><a href="http://paperpile.com/b/TzrP46/0zyMm" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/0zyMm" rel="noopener noreferrer" target="_blank"> Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification. </a></span><span><a href="http://paperpile.com/b/TzrP46/0zyMm" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/0zyMm" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>4. </span><span><a href="http://paperpile.com/b/TzrP46/i1s9w" rel="noopener noreferrer" target="_blank">Van Veen, D. </a></span><span><a href="http://paperpile.com/b/TzrP46/i1s9w" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/i1s9w" rel="noopener noreferrer" target="_blank"> Adapted large language models can outperform medical experts in clinical text summarization. </a></span><span><a href="http://paperpile.com/b/TzrP46/i1s9w" rel="noopener noreferrer" target="_blank">Nat. Med.</a></span><span><a href="http://paperpile.com/b/TzrP46/i1s9w" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/TzrP46/i1s9w" rel="noopener noreferrer" target="_blank">30</a></sup></span><span><a href="http://paperpile.com/b/TzrP46/i1s9w" rel="noopener noreferrer" target="_blank">, 1134–1142 (2024).</a></span></p><p><span>5. </span><span><a href="http://paperpile.com/b/TzrP46/Y26nJ" rel="noopener noreferrer" target="_blank">Soroush, A. </a></span><span><a href="http://paperpile.com/b/TzrP46/Y26nJ" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/Y26nJ" rel="noopener noreferrer" target="_blank"> Large language models are poor medical coders — benchmarking of medical code querying. </a></span><span><a href="http://paperpile.com/b/TzrP46/Y26nJ" rel="noopener noreferrer" target="_blank">NEJM AI</a></span><span><a href="http://paperpile.com/b/TzrP46/Y26nJ" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/TzrP46/Y26nJ" rel="noopener noreferrer" target="_blank">1</a></sup></span><span><a href="http://paperpile.com/b/TzrP46/Y26nJ" rel="noopener noreferrer" target="_blank">, (2024).</a></span></p><p><span>6. </span><span><a href="http://paperpile.com/b/TzrP46/hcxrw" rel="noopener noreferrer" target="_blank">Chen, S. </a></span><span><a href="http://paperpile.com/b/TzrP46/hcxrw" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/hcxrw" rel="noopener noreferrer" target="_blank"> The effect of using a large language model to respond to patient messages. </a></span><span><a href="http://paperpile.com/b/TzrP46/hcxrw" rel="noopener noreferrer" target="_blank">Lancet Digit. Health</a></span><span><a href="http://paperpile.com/b/TzrP46/hcxrw" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/TzrP46/hcxrw" rel="noopener noreferrer" target="_blank">6</a></sup></span><span><a href="http://paperpile.com/b/TzrP46/hcxrw" rel="noopener noreferrer" target="_blank">, e379–e381 (2024).</a></span></p><p><span>7. </span><span><a href="http://paperpile.com/b/TzrP46/5kxN7" rel="noopener noreferrer" target="_blank">Potts, B. MedFuzz: Exploring the robustness of LLMs on medical challenge problems. </a></span><span><a href="http://paperpile.com/b/TzrP46/5kxN7" rel="noopener noreferrer" target="_blank">Microsoft Research</a></span><span><a href="http://paperpile.com/b/TzrP46/5kxN7" rel="noopener noreferrer" target="_blank"> </a></span><span><a href="https://www.microsoft.com/en-us/research/blog/medfuzz-exploring-the-robustness-of-llms-on-medical-challenge-problems/" rel="noopener noreferrer" target="_blank">https://www.microsoft.com/en-us/research/blog/medfuzz-exploring-the-robustness-of-llms-on-medical-challenge-problems/</a></span><span><a href="http://paperpile.com/b/TzrP46/5kxN7" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>8. </span><span><a href="http://paperpile.com/b/TzrP46/gTOCl" rel="noopener noreferrer" target="_blank">Bai, Y. </a></span><span><a href="http://paperpile.com/b/TzrP46/gTOCl" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/gTOCl" rel="noopener noreferrer" target="_blank"> Training a helpful and harmless assistant with reinforcement learning from human feedback. </a></span><span><a href="http://paperpile.com/b/TzrP46/gTOCl" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/gTOCl" rel="noopener noreferrer" target="_blank"> (2022).</a></span></p><p><span>9. </span><span><a href="http://paperpile.com/b/TzrP46/7wC8T" rel="noopener noreferrer" target="_blank">Touvron, H. </a></span><span><a href="http://paperpile.com/b/TzrP46/7wC8T" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/7wC8T" rel="noopener noreferrer" target="_blank"> Llama 2: Open Foundation and Fine-Tuned Chat Models. </a></span><span><a href="http://paperpile.com/b/TzrP46/7wC8T" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/7wC8T" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>10. </span><span><a href="http://paperpile.com/b/TzrP46/Nwrgv" rel="noopener noreferrer" target="_blank">Bai, Y. </a></span><span><a href="http://paperpile.com/b/TzrP46/Nwrgv" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/Nwrgv" rel="noopener noreferrer" target="_blank"> Constitutional AI: Harmlessness from AI Feedback. </a></span><span><a href="http://paperpile.com/b/TzrP46/Nwrgv" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/Nwrgv" rel="noopener noreferrer" target="_blank"> (2022).</a></span></p><p><span>11. </span><span><a href="http://paperpile.com/b/TzrP46/bX5YJ" rel="noopener noreferrer" target="_blank">Liu, R., Sumers, T. R., Dasgupta, I. &amp; Griffiths, T. L. How do large language models navigate conflicts between honesty and helpfulness? </a></span><span><a href="http://paperpile.com/b/TzrP46/bX5YJ" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/bX5YJ" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>12. </span><span><a href="http://paperpile.com/b/TzrP46/HWbJj" rel="noopener noreferrer" target="_blank">Askell, A. </a></span><span><a href="http://paperpile.com/b/TzrP46/HWbJj" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/HWbJj" rel="noopener noreferrer" target="_blank"> A General Language Assistant as a Laboratory for Alignment. </a></span><span><a href="http://paperpile.com/b/TzrP46/HWbJj" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/HWbJj" rel="noopener noreferrer" target="_blank"> (2021).</a></span></p><p><span>13. </span><span><a href="http://paperpile.com/b/TzrP46/rfITO" rel="noopener noreferrer" target="_blank">Rafailov, R. </a></span><span><a href="http://paperpile.com/b/TzrP46/rfITO" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/rfITO" rel="noopener noreferrer" target="_blank"> Direct Preference Optimization: Your Language Model is Secretly a Reward Model. </a></span><span><a href="http://paperpile.com/b/TzrP46/rfITO" rel="noopener noreferrer" target="_blank">arXiv [cs.LG]</a></span><span><a href="http://paperpile.com/b/TzrP46/rfITO" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>14. </span><span><a href="http://paperpile.com/b/TzrP46/8UflO" rel="noopener noreferrer" target="_blank">Christiano, P. </a></span><span><a href="http://paperpile.com/b/TzrP46/8UflO" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/8UflO" rel="noopener noreferrer" target="_blank"> Deep reinforcement learning from human preferences. </a></span><span><a href="http://paperpile.com/b/TzrP46/8UflO" rel="noopener noreferrer" target="_blank">arXiv [stat.ML]</a></span><span><a href="http://paperpile.com/b/TzrP46/8UflO" rel="noopener noreferrer" target="_blank"> (2017).</a></span></p><p><span>15. </span><span><a href="http://paperpile.com/b/TzrP46/aWY2c" rel="noopener noreferrer" target="_blank">Ouyang, L. </a></span><span><a href="http://paperpile.com/b/TzrP46/aWY2c" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/aWY2c" rel="noopener noreferrer" target="_blank"> Training language models to follow instructions with human feedback. </a></span><span><a href="http://paperpile.com/b/TzrP46/aWY2c" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/aWY2c" rel="noopener noreferrer" target="_blank"> (2022).</a></span></p><p><span>16. </span><span><a href="http://paperpile.com/b/TzrP46/XnpbV" rel="noopener noreferrer" target="_blank">Menz, B. D., Modi, N. D., Sorich, M. J. &amp; Hopkins, A. M. Health disinformation use case highlighting the urgent need for artificial intelligence vigilance: Weapons of mass disinformation: Weapons of mass disinformation. </a></span><span><a href="http://paperpile.com/b/TzrP46/XnpbV" rel="noopener noreferrer" target="_blank">JAMA Intern. Med.</a></span><span><a href="http://paperpile.com/b/TzrP46/XnpbV" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/TzrP46/XnpbV" rel="noopener noreferrer" target="_blank">184</a></sup></span><span><a href="http://paperpile.com/b/TzrP46/XnpbV" rel="noopener noreferrer" target="_blank">, 92–96 (2024).</a></span></p><p><span>17. </span><span><a href="http://paperpile.com/b/TzrP46/15qvJ" rel="noopener noreferrer" target="_blank">Menz, B. D. </a></span><span><a href="http://paperpile.com/b/TzrP46/15qvJ" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/15qvJ" rel="noopener noreferrer" target="_blank"> Current safeguards, risk mitigation, and transparency measures of large language models against the generation of health disinformation: repeated cross sectional analysis. </a></span><span><a href="http://paperpile.com/b/TzrP46/15qvJ" rel="noopener noreferrer" target="_blank">BMJ</a></span><span><a href="http://paperpile.com/b/TzrP46/15qvJ" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/TzrP46/15qvJ" rel="noopener noreferrer" target="_blank">384</a></sup></span><span><a href="http://paperpile.com/b/TzrP46/15qvJ" rel="noopener noreferrer" target="_blank">, e078538 (2024).</a></span></p><p><span>18. </span><span><a href="http://paperpile.com/b/TzrP46/iVtzi" rel="noopener noreferrer" target="_blank">Zhang, Y., Sharma, K., Du, L. &amp; Liu, Y. Toward mitigating misinformation and social media manipulation in LLM era. in </a></span><span><a href="http://paperpile.com/b/TzrP46/iVtzi" rel="noopener noreferrer" target="_blank">Companion Proceedings of the ACM on Web Conference 2024</a></span><span><a href="http://paperpile.com/b/TzrP46/iVtzi" rel="noopener noreferrer" target="_blank"> vol. 19 1302–1305 (ACM, New York, NY, USA, 2024).</a></span></p><p><span>19. </span><span><a href="http://paperpile.com/b/TzrP46/4yypL" rel="noopener noreferrer" target="_blank">Xu, N. </a></span><span><a href="http://paperpile.com/b/TzrP46/4yypL" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/4yypL" rel="noopener noreferrer" target="_blank"> Cognitive overload: Jailbreaking large language models with overloaded logical thinking. in </a></span><span><a href="http://paperpile.com/b/TzrP46/4yypL" rel="noopener noreferrer" target="_blank">Findings of the Association for Computational Linguistics: NAACL 2024</a></span><span><a href="http://paperpile.com/b/TzrP46/4yypL" rel="noopener noreferrer" target="_blank"> (Association for Computational Linguistics, Stroudsburg, PA, USA, 2024). doi:</a></span><span><a href="http://dx.doi.org/10.18653/v1/2024.findings-naacl.224" rel="noopener noreferrer" target="_blank">10.18653/v1/2024.findings-naacl.224</a></span><span><a href="http://paperpile.com/b/TzrP46/4yypL" rel="noopener noreferrer" target="_blank">.</a></span></p><p><span>20. </span><span><a href="http://paperpile.com/b/TzrP46/WKyES" rel="noopener noreferrer" target="_blank">Ding, P. </a></span><span><a href="http://paperpile.com/b/TzrP46/WKyES" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/WKyES" rel="noopener noreferrer" target="_blank"> A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily. in </a></span><span><a href="http://paperpile.com/b/TzrP46/WKyES" rel="noopener noreferrer" target="_blank">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</a></span><span><a href="http://paperpile.com/b/TzrP46/WKyES" rel="noopener noreferrer" target="_blank"> (Association for Computational Linguistics, Stroudsburg, PA, USA, 2024). doi:</a></span><span><a href="http://dx.doi.org/10.18653/v1/2024.naacl-long.118" rel="noopener noreferrer" target="_blank">10.18653/v1/2024.naacl-long.118</a></span><span><a href="http://paperpile.com/b/TzrP46/WKyES" rel="noopener noreferrer" target="_blank">.</a></span></p><p><span>21. </span><span><a href="http://paperpile.com/b/TzrP46/Q8QE8" rel="noopener noreferrer" target="_blank">Sharma, M. </a></span><span><a href="http://paperpile.com/b/TzrP46/Q8QE8" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/Q8QE8" rel="noopener noreferrer" target="_blank"> Towards Understanding Sycophancy in Language Models. </a></span><span><a href="http://paperpile.com/b/TzrP46/Q8QE8" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/Q8QE8" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>22. </span><span><a href="http://paperpile.com/b/TzrP46/Ffw7R" rel="noopener noreferrer" target="_blank">Liu, Y. </a></span><span><a href="http://paperpile.com/b/TzrP46/Ffw7R" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/Ffw7R" rel="noopener noreferrer" target="_blank"> A Hitchhiker’s Guide to Jailbreaking ChatGPT via Prompt Engineering. in </a></span><span><a href="http://paperpile.com/b/TzrP46/Ffw7R" rel="noopener noreferrer" target="_blank">Proceedings of the 4th International Workshop on Software Engineering and AI for Data Quality in Cyber-Physical Systems/Internet of Things</a></span><span><a href="http://paperpile.com/b/TzrP46/Ffw7R" rel="noopener noreferrer" target="_blank"> (ACM, New York, NY, USA, 2024). doi:</a></span><span><a href="http://dx.doi.org/10.1145/3663530.3665021" rel="noopener noreferrer" target="_blank">10.1145/3663530.3665021</a></span><span><a href="http://paperpile.com/b/TzrP46/Ffw7R" rel="noopener noreferrer" target="_blank">.</a></span></p><p><span>23. </span><span><a href="http://paperpile.com/b/TzrP46/w5xip" rel="noopener noreferrer" target="_blank">Si, C. </a></span><span><a href="http://paperpile.com/b/TzrP46/w5xip" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/w5xip" rel="noopener noreferrer" target="_blank"> Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong. </a></span><span><a href="http://paperpile.com/b/TzrP46/w5xip" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/w5xip" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>24. </span><span><a href="http://paperpile.com/b/TzrP46/INsIJ" rel="noopener noreferrer" target="_blank">Haupt, C. E. &amp; Marks, M. FTC regulation of AI-generated medical disinformation. </a></span><span><a href="http://paperpile.com/b/TzrP46/INsIJ" rel="noopener noreferrer" target="_blank">JAMA</a></span><span><a href="http://paperpile.com/b/TzrP46/INsIJ" rel="noopener noreferrer" target="_blank"> (2024) doi:</a></span><span><a href="http://dx.doi.org/10.1001/jama.2024.19971" rel="noopener noreferrer" target="_blank">10.1001/jama.2024.19971</a></span><span><a href="http://paperpile.com/b/TzrP46/INsIJ" rel="noopener noreferrer" target="_blank">.</a></span></p><p><span>25. </span><span><a href="http://paperpile.com/b/TzrP46/b816C" rel="noopener noreferrer" target="_blank">Polyportis, A. &amp; Pahos, N. Navigating the perils of artificial intelligence: a focused review on ChatGPT and responsible research and innovation. </a></span><span><a href="http://paperpile.com/b/TzrP46/b816C" rel="noopener noreferrer" target="_blank">Humanit. Soc. Sci. Commun.</a></span><span><a href="http://paperpile.com/b/TzrP46/b816C" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/TzrP46/b816C" rel="noopener noreferrer" target="_blank">11</a></sup></span><span><a href="http://paperpile.com/b/TzrP46/b816C" rel="noopener noreferrer" target="_blank">, 1–10 (2024).</a></span></p><p><span>26. </span><span><a href="http://paperpile.com/b/TzrP46/kTt10" rel="noopener noreferrer" target="_blank">Lu, W. </a></span><span><a href="http://paperpile.com/b/TzrP46/kTt10" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/kTt10" rel="noopener noreferrer" target="_blank"> Eraser: Jailbreaking defense in Large Language Models via unlearning harmful knowledge. </a></span><span><a href="http://paperpile.com/b/TzrP46/kTt10" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/kTt10" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>27. </span><span><a href="http://paperpile.com/b/TzrP46/4Jj7x" rel="noopener noreferrer" target="_blank">Liu, T. </a></span><span><a href="http://paperpile.com/b/TzrP46/4Jj7x" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/4Jj7x" rel="noopener noreferrer" target="_blank"> Making them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction. </a></span><span><a href="http://paperpile.com/b/TzrP46/4Jj7x" rel="noopener noreferrer" target="_blank">arXiv [cs.CR]</a></span><span><a href="http://paperpile.com/b/TzrP46/4Jj7x" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>28. </span><span><a href="http://paperpile.com/b/TzrP46/khOft" rel="noopener noreferrer" target="_blank">Han, T. </a></span><span><a href="http://paperpile.com/b/TzrP46/khOft" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/khOft" rel="noopener noreferrer" target="_blank"> Medical large language models are susceptible to targeted misinformation attacks. </a></span><span><a href="http://paperpile.com/b/TzrP46/khOft" rel="noopener noreferrer" target="_blank">NPJ Digit. Med.</a></span><span><a href="http://paperpile.com/b/TzrP46/khOft" rel="noopener noreferrer" target="_blank"> </a></span><span><sup class="citation-ref"><a class="citation-link" href="http://paperpile.com/b/TzrP46/khOft" rel="noopener noreferrer" target="_blank">7</a></sup></span><span><a href="http://paperpile.com/b/TzrP46/khOft" rel="noopener noreferrer" target="_blank">, 288 (2024).</a></span></p><p><span>29. </span><span><a href="http://paperpile.com/b/TzrP46/kAniC" rel="noopener noreferrer" target="_blank">Huang, X. </a></span><span><a href="http://paperpile.com/b/TzrP46/kAniC" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/kAniC" rel="noopener noreferrer" target="_blank"> Medical MLLM is vulnerable: Cross-modality jailbreak and mismatched attacks on medical Multimodal Large Language Models. </a></span><span><a href="http://paperpile.com/b/TzrP46/kAniC" rel="noopener noreferrer" target="_blank">arXiv</a></span><span><a href="http://paperpile.com/b/TzrP46/kAniC" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>30. </span><span><a href="http://paperpile.com/b/TzrP46/3jNoz" rel="noopener noreferrer" target="_blank">Gallifant, J. </a></span><span><a href="http://paperpile.com/b/TzrP46/3jNoz" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/3jNoz" rel="noopener noreferrer" target="_blank"> Language models are surprisingly fragile to drug names in biomedical benchmarks. </a></span><span><a href="http://paperpile.com/b/TzrP46/3jNoz" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/3jNoz" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>31. </span><span><a href="http://paperpile.com/b/TzrP46/i6Mml" rel="noopener noreferrer" target="_blank">Wu, Z. </a></span><span><a href="http://paperpile.com/b/TzrP46/i6Mml" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/i6Mml" rel="noopener noreferrer" target="_blank"> AxBench: Steering LLMs? Even simple baselines outperform sparse autoencoders. </a></span><span><a href="http://paperpile.com/b/TzrP46/i6Mml" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/i6Mml" rel="noopener noreferrer" target="_blank"> (2025).</a></span></p><p><span>32. </span><span><a href="http://paperpile.com/b/TzrP46/pJvqP" rel="noopener noreferrer" target="_blank">Dubois, Y., Galambosi, B., Liang, P. &amp; Hashimoto, T. B. Length-controlled AlpacaEval: A simple way to debias automatic evaluators. </a></span><span><a href="http://paperpile.com/b/TzrP46/pJvqP" rel="noopener noreferrer" target="_blank">arXiv [cs.LG]</a></span><span><a href="http://paperpile.com/b/TzrP46/pJvqP" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>33. </span><span><a href="http://paperpile.com/b/TzrP46/Zr399" rel="noopener noreferrer" target="_blank">Bhakthavatsalam, S. </a></span><span><a href="http://paperpile.com/b/TzrP46/Zr399" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/Zr399" rel="noopener noreferrer" target="_blank"> Think you have solved direct-answer question answering? Try ARC-DA, the direct-answer AI2 Reasoning Challenge. </a></span><span><a href="http://paperpile.com/b/TzrP46/Zr399" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/Zr399" rel="noopener noreferrer" target="_blank"> (2021).</a></span></p><p><span>34. </span><span><a href="http://paperpile.com/b/TzrP46/7cotw" rel="noopener noreferrer" target="_blank">Clark, C. </a></span><span><a href="http://paperpile.com/b/TzrP46/7cotw" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/7cotw" rel="noopener noreferrer" target="_blank"> BoolQ: Exploring the surprising difficulty of natural yes/no questions. </a></span><span><a href="http://paperpile.com/b/TzrP46/7cotw" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/7cotw" rel="noopener noreferrer" target="_blank"> (2019).</a></span></p><p><span>35. </span><span><a href="http://paperpile.com/b/TzrP46/uEA64" rel="noopener noreferrer" target="_blank">Hendrycks, D. </a></span><span><a href="http://paperpile.com/b/TzrP46/uEA64" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/uEA64" rel="noopener noreferrer" target="_blank"> Measuring massive multitask language understanding. </a></span><span><a href="http://paperpile.com/b/TzrP46/uEA64" rel="noopener noreferrer" target="_blank">arXiv [cs.CY]</a></span><span><a href="http://paperpile.com/b/TzrP46/uEA64" rel="noopener noreferrer" target="_blank"> (2020).</a></span></p><p><span>36. </span><span><a href="http://paperpile.com/b/TzrP46/8sj55" rel="noopener noreferrer" target="_blank">Rein, D. </a></span><span><a href="http://paperpile.com/b/TzrP46/8sj55" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/8sj55" rel="noopener noreferrer" target="_blank"> GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark. </a></span><span><a href="http://paperpile.com/b/TzrP46/8sj55" rel="noopener noreferrer" target="_blank">arXiv [cs.AI]</a></span><span><a href="http://paperpile.com/b/TzrP46/8sj55" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>37. </span><span><a href="http://paperpile.com/b/TzrP46/6rxsv" rel="noopener noreferrer" target="_blank">Lin, S., Hilton, J. &amp; Evans, O. TruthfulQA: Measuring How Models Mimic Human Falsehoods. </a></span><span><a href="http://paperpile.com/b/TzrP46/6rxsv" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/6rxsv" rel="noopener noreferrer" target="_blank"> (2021).</a></span></p><p><span>38. </span><span><a href="http://paperpile.com/b/TzrP46/1KmI2" rel="noopener noreferrer" target="_blank">Zeng, Y. </a></span><span><a href="http://paperpile.com/b/TzrP46/1KmI2" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/1KmI2" rel="noopener noreferrer" target="_blank"> How Johnny can persuade LLMs to jailbreak them: Rethinking persuasion to challenge AI safety by humanizing LLMs. </a></span><span><a href="http://paperpile.com/b/TzrP46/1KmI2" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/1KmI2" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>39. </span><span><a href="http://paperpile.com/b/TzrP46/chc0M" rel="noopener noreferrer" target="_blank">Xu, R. </a></span><span><a href="http://paperpile.com/b/TzrP46/chc0M" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/chc0M" rel="noopener noreferrer" target="_blank"> The Earth is Flat because...: Investigating LLMs’ Belief towards Misinformation via Persuasive Conversation. </a></span><span><a href="http://paperpile.com/b/TzrP46/chc0M" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/chc0M" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>40. </span><span><a href="http://paperpile.com/b/TzrP46/OUTNO" rel="noopener noreferrer" target="_blank">Allen-Zhu, Z. &amp; Li, Y. Physics of language models: Part 3.2, knowledge manipulation. </a></span><span><a href="http://paperpile.com/b/TzrP46/OUTNO" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/OUTNO" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>41. </span><span><a href="http://paperpile.com/b/TzrP46/z8q3u" rel="noopener noreferrer" target="_blank">Wei, J. </a></span><span><a href="http://paperpile.com/b/TzrP46/z8q3u" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/z8q3u" rel="noopener noreferrer" target="_blank"> Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. </a></span><span><a href="http://paperpile.com/b/TzrP46/z8q3u" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/z8q3u" rel="noopener noreferrer" target="_blank"> (2022).</a></span></p><p><span>42. </span><span><a href="http://paperpile.com/b/TzrP46/6SP3a" rel="noopener noreferrer" target="_blank">OpenAI </a></span><span><a href="http://paperpile.com/b/TzrP46/6SP3a" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/6SP3a" rel="noopener noreferrer" target="_blank"> OpenAI o1 System Card. </a></span><span><a href="http://paperpile.com/b/TzrP46/6SP3a" rel="noopener noreferrer" target="_blank">arXiv [cs.AI]</a></span><span><a href="http://paperpile.com/b/TzrP46/6SP3a" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>43. </span><span><a href="http://paperpile.com/b/TzrP46/ZhpP8" rel="noopener noreferrer" target="_blank">Chen, Q. </a></span><span><a href="http://paperpile.com/b/TzrP46/ZhpP8" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/ZhpP8" rel="noopener noreferrer" target="_blank"> Towards reasoning era: A survey of long chain-of-thought for reasoning large language models. </a></span><span><a href="http://paperpile.com/b/TzrP46/ZhpP8" rel="noopener noreferrer" target="_blank">arXiv [cs.AI]</a></span><span><a href="http://paperpile.com/b/TzrP46/ZhpP8" rel="noopener noreferrer" target="_blank"> (2025).</a></span></p><p><span>44. </span><span><a href="http://paperpile.com/b/TzrP46/cc36s" rel="noopener noreferrer" target="_blank">DeepSeek-AI </a></span><span><a href="http://paperpile.com/b/TzrP46/cc36s" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/cc36s" rel="noopener noreferrer" target="_blank"> DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. </a></span><span><a href="http://paperpile.com/b/TzrP46/cc36s" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/cc36s" rel="noopener noreferrer" target="_blank"> (2025).</a></span></p><p><span>45. </span><span><a href="http://paperpile.com/b/TzrP46/JyVLn" rel="noopener noreferrer" target="_blank">Guan, M. Y. </a></span><span><a href="http://paperpile.com/b/TzrP46/JyVLn" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/JyVLn" rel="noopener noreferrer" target="_blank"> Deliberative Alignment: Reasoning enables safer language models. </a></span><span><a href="http://paperpile.com/b/TzrP46/JyVLn" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/JyVLn" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>46. </span><span><a href="http://paperpile.com/b/TzrP46/lbS62" rel="noopener noreferrer" target="_blank">Li, Z.-Z. </a></span><span><a href="http://paperpile.com/b/TzrP46/lbS62" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/lbS62" rel="noopener noreferrer" target="_blank"> From System 1 to System 2: A survey of reasoning Large Language Models. </a></span><span><a href="http://paperpile.com/b/TzrP46/lbS62" rel="noopener noreferrer" target="_blank">arXiv [cs.AI]</a></span><span><a href="http://paperpile.com/b/TzrP46/lbS62" rel="noopener noreferrer" target="_blank"> (2025).</a></span></p><p><span>47. </span><span><a href="http://paperpile.com/b/TzrP46/Irz6V" rel="noopener noreferrer" target="_blank">Jiang, F. </a></span><span><a href="http://paperpile.com/b/TzrP46/Irz6V" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/Irz6V" rel="noopener noreferrer" target="_blank"> SafeChain: Safety of language models with long chain-of-thought reasoning capabilities. </a></span><span><a href="http://paperpile.com/b/TzrP46/Irz6V" rel="noopener noreferrer" target="_blank">arXiv [cs.AI]</a></span><span><a href="http://paperpile.com/b/TzrP46/Irz6V" rel="noopener noreferrer" target="_blank"> (2025).</a></span></p><p><span>48. </span><span><a href="http://paperpile.com/b/TzrP46/sCufh" rel="noopener noreferrer" target="_blank">Sycophancy in GPT-4o: what happened and what we’re doing about it. </a></span><span><a href="https://openai.com/index/sycophancy-in-gpt-4o/" rel="noopener noreferrer" target="_blank">https://openai.com/index/sycophancy-in-gpt-4o/</a></span><span><a href="http://paperpile.com/b/TzrP46/sCufh" rel="noopener noreferrer" target="_blank">.</a></span></p><p><span>49. </span><span><a href="http://paperpile.com/b/TzrP46/lcrcp" rel="noopener noreferrer" target="_blank">Liu, J., Min, S., Zettlemoyer, L., Choi, Y. &amp; Hajishirzi, H. Infini-gram: Scaling unbounded n-gram language models to a trillion tokens. </a></span><span><a href="http://paperpile.com/b/TzrP46/lcrcp" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/lcrcp" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>50. </span><span><a href="http://paperpile.com/b/TzrP46/2D9Hn" rel="noopener noreferrer" target="_blank">Soldaini, L. </a></span><span><a href="http://paperpile.com/b/TzrP46/2D9Hn" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/2D9Hn" rel="noopener noreferrer" target="_blank"> Dolma: An open corpus of three trillion tokens for language model pretraining research. </a></span><span><a href="http://paperpile.com/b/TzrP46/2D9Hn" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/2D9Hn" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>51. </span><span><a href="http://paperpile.com/b/TzrP46/ItNUJ" rel="noopener noreferrer" target="_blank">Raffel, C. </a></span><span><a href="http://paperpile.com/b/TzrP46/ItNUJ" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/ItNUJ" rel="noopener noreferrer" target="_blank"> Exploring the limits of transfer learning with a unified text-to-text transformer. </a></span><span><a href="http://paperpile.com/b/TzrP46/ItNUJ" rel="noopener noreferrer" target="_blank">arXiv [cs.LG]</a></span><span><a href="http://paperpile.com/b/TzrP46/ItNUJ" rel="noopener noreferrer" target="_blank"> (2019).</a></span></p><p><span>52. </span><span><a href="http://paperpile.com/b/TzrP46/L4O42" rel="noopener noreferrer" target="_blank">Together. RedPajama, a project to create leading open-source models, starts by reproducing LLaMA training dataset of over 1.2 trillion tokens. </a></span><span><a href="https://www.together.ai/blog/redpajama" rel="noopener noreferrer" target="_blank">https://www.together.ai/blog/redpajama</a></span><span><a href="http://paperpile.com/b/TzrP46/L4O42" rel="noopener noreferrer" target="_blank">.</a></span></p><p><span>53. </span><span><a href="http://paperpile.com/b/TzrP46/i6FZM" rel="noopener noreferrer" target="_blank">Gao, L. </a></span><span><a href="http://paperpile.com/b/TzrP46/i6FZM" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/i6FZM" rel="noopener noreferrer" target="_blank"> The Pile: An 800GB dataset of diverse text for language modeling. </a></span><span><a href="http://paperpile.com/b/TzrP46/i6FZM" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/i6FZM" rel="noopener noreferrer" target="_blank"> (2020).</a></span></p><p><span>54. </span><span><a href="http://paperpile.com/b/TzrP46/u1Uso" rel="noopener noreferrer" target="_blank">Zhou, C. </a></span><span><a href="http://paperpile.com/b/TzrP46/u1Uso" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/u1Uso" rel="noopener noreferrer" target="_blank"> LIMA: Less is more for alignment. </a></span><span><a href="http://paperpile.com/b/TzrP46/u1Uso" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/u1Uso" rel="noopener noreferrer" target="_blank"> (2023).</a></span></p><p><span>55. </span><span><a href="http://paperpile.com/b/TzrP46/HFoiN" rel="noopener noreferrer" target="_blank">Inspect_ai: Inspect: A Framework for Large Language Model Evaluations</a></span><span><a href="http://paperpile.com/b/TzrP46/HFoiN" rel="noopener noreferrer" target="_blank">. (Github).</a></span></p><p><span>56. </span><span><a href="http://paperpile.com/b/TzrP46/9C0Zb" rel="noopener noreferrer" target="_blank">Alpaca_eval: An Automatic Evaluator for Instruction-Following Language Models. Human-Validated, High-Quality, Cheap, and Fast</a></span><span><a href="http://paperpile.com/b/TzrP46/9C0Zb" rel="noopener noreferrer" target="_blank">. (Github).</a></span></p><p><span>57. </span><span><a href="http://paperpile.com/b/TzrP46/VcJeB" rel="noopener noreferrer" target="_blank">Miller, E. Adding error bars to evals: A statistical approach to language model evaluations. </a></span><span><a href="http://paperpile.com/b/TzrP46/VcJeB" rel="noopener noreferrer" target="_blank">arXiv [stat.AP]</a></span><span><a href="http://paperpile.com/b/TzrP46/VcJeB" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>58. </span><span><a href="http://paperpile.com/b/TzrP46/0FxLp" rel="noopener noreferrer" target="_blank">Grattafiori, A. </a></span><span><a href="http://paperpile.com/b/TzrP46/0FxLp" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/0FxLp" rel="noopener noreferrer" target="_blank"> The Llama 3 herd of models. </a></span><span><a href="http://paperpile.com/b/TzrP46/0FxLp" rel="noopener noreferrer" target="_blank">arXiv [cs.AI]</a></span><span><a href="http://paperpile.com/b/TzrP46/0FxLp" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>59. </span><span><a href="http://paperpile.com/b/TzrP46/wmAxy" rel="noopener noreferrer" target="_blank">Panickssery, A., Bowman, S. R. &amp; Feng, S. LLM evaluators recognize and favor their own generations. </a></span><span><a href="http://paperpile.com/b/TzrP46/wmAxy" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/wmAxy" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>60. </span><span><a href="http://paperpile.com/b/TzrP46/2vT27" rel="noopener noreferrer" target="_blank">Wataoka, K., Takahashi, T. &amp; Ri, R. Self-preference bias in LLM-as-a-judge. </a></span><span><a href="http://paperpile.com/b/TzrP46/2vT27" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/2vT27" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>61. </span><span><a href="http://paperpile.com/b/TzrP46/0Q7bf" rel="noopener noreferrer" target="_blank">Xu, W. </a></span><span><a href="http://paperpile.com/b/TzrP46/0Q7bf" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/0Q7bf" rel="noopener noreferrer" target="_blank"> Pride and prejudice: LLM amplifies self-bias in self-refinement. </a></span><span><a href="http://paperpile.com/b/TzrP46/0Q7bf" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/0Q7bf" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p><p><span>62. </span><span><a href="http://paperpile.com/b/TzrP46/DCEWM" rel="noopener noreferrer" target="_blank">Laurito, W. </a></span><span><a href="http://paperpile.com/b/TzrP46/DCEWM" rel="noopener noreferrer" target="_blank">et al.</a></span><span><a href="http://paperpile.com/b/TzrP46/DCEWM" rel="noopener noreferrer" target="_blank"> AI AI bias: Large language models favor their own generated content. </a></span><span><a href="http://paperpile.com/b/TzrP46/DCEWM" rel="noopener noreferrer" target="_blank">arXiv [cs.CL]</a></span><span><a href="http://paperpile.com/b/TzrP46/DCEWM" rel="noopener noreferrer" target="_blank"> (2024).</a></span></p>