<h1>Appendices</h1><h2 class="appendix-breaker"><span class="appendix-index">A1</span><span class="appendix-title">MedAlpaca - An Open-Source Collection of Medical Conversation AI Models and Training Data</span></h2><p class="appendix-linkline"><span>Available here: </span><span><a href="https://github.com/kbressem/medAlpaca" rel="noopener noreferrer" target="_blank">https://github.com/kbressem/medAlpaca</a></span></p><p><span>Background </span><span>Large language models (LLMs) promise gains in clinical documentation, education, and patient communication, but closed APIs and privacy constraints limit deployment. MedAlpaca addresses this gap by releasing open-source medical instruction-tuned models and datasets that can run on-prem, enabling reproducible research and privacy-preserving use.</span></p><p><span>Approach </span><span>We compiled ‚ÄúMedical Meadow,‚Äù a &gt;160k-example corpus that reformats medical resources (e.g., flashcards, StackExchange Q&amp;A, WikiDoc chapters, public Medical QA benchmark) into instruction‚Äìresponse pairs. Models are fine-tuned from LLaMA 7B/13B with full-parameter FT and parameter-efficient variants (LoRA; 8-bit optimizer/matmul) using large effective batch sizes and standard schedulers. Zero-shot evaluations target USMLE Steps 1‚Äì3, excluding image items, with strict answer formatting for fair scoring.</span></p><p><span>Results </span><span>Fine-tuning consistently improves over base models on USMLE. MedAlpaca-13B attains the strongest accuracies (e.g., Step 1 ‚âà0.47, Step 2 ‚âà0.48, Step 3 ‚âà0.60). Parameter-efficient routes (LoRA/8-bit) trade some accuracy for much lower compute, supporting wider academic/clinical use. All models and data are released publicly to catalyze follow-on work.</span></p><p><span>Implications </span><span>Open instruction-tuned medical LLMs make it feasible to experiment locally, reduce privacy concerns, and accelerate tooling for structured reporting, retrieval/summarization, and training aids‚Äîwhile highlighting the need for guardrails against confabulation.</span></p><h2 class="appendix-breaker"><span class="appendix-index">A2</span><span class="appendix-title">Measuring Pointwise ùí±-Usable Information In-Context-ly</span></h2><p class="appendix-linkline"><span>Available here: </span><span><a href="https://aclanthology.org/2023.findings-emnlp.1054/" rel="noopener noreferrer" target="_blank">https://aclanthology.org/2023.findings-emnlp.1054/</a></span></p><p><span>Background</span><span> ‚ÄúPointwise ùí±-usable information‚Äù (PVI) is a hardness measure that tells you, for a fixed model ùí±, how much </span><span>usable</span><span> signal an individual example contains‚Äîlow PVI means the model has little leverage to solve that instance; high PVI means the signal is there and the model can exploit it. Prior work estimated PVI by training or fine-tuning probes, which is accurate but heavy-weight. This paper asks: can we bring PVI into the in-context learning (ICL) regime‚Äîi.e., diagnose instance-level difficulty purely by prompting, without any gradient updates? That would give practitioners a lightweight, model-specific difficulty score they can compute on the fly when they only have API access. </span></p><p><span>Approach </span><span>We adapt PVI to an in-context analogue (‚Äúin-context PVI‚Äù) by observing model predictions across </span><span>prompted</span><span> conditions rather than trained parameters. Concretely, we vary exemplars and shot counts in the prompt, read out the induced distribution of predictions, and compute a PVI-style quantity that reflects how much the model‚Äôs </span><span>prompt-conditioned behavior</span><span> uses the information present in the input. The key design goal is no fine-tuning and few exemplars, so the metric is cheap to compute and usable in settings where training is impossible. </span></p><p><span>We run a systematic reliability study: (i) Exemplar robustness‚Äîdoes the in-context PVI for a given instance change if you swap which few-shot examples you show? (ii) Shot robustness‚Äîdoes it change as you move from 1‚Üík shots? (iii) Behavioral parity‚Äîdoes in-context PVI preserve the characteristic relationships known from the original (training-based) PVI? Finally, they demonstrate practical utility: use in-context PVI to surface the ‚Äúhard tail‚Äù of a dataset for targeted evaluation or data curation. Across these lenses, the in-context measure behaves stably and mirrors the spirit of classical PVI, but at a fraction of the computational cost.</span></p><p><span>Results</span><span> First, you don‚Äôt need fine-tuning to get a useful PVI-like hardness signal: the prompted version tracks difficulty in ways that agree with the training-based formulation. Second, the number and identity of exemplars barely move the needle‚Äîvariance from exemplar choice is </span><span>insignificant</span><span>‚Äîand scaling the #shots does not destabilize the estimates. Third, it‚Äôs actionable: ranking by in-context PVI cleanly pulls out the cases your model will struggle with, which then lets you allocate annotation or evaluation budget where it matters most. In short, you get a trustworthy, ‚ÄúAPI-only‚Äù difficulty score that‚Äôs cheap and robust. </span></p><p><span>Implications</span><span> If you rely on ICL (or only have black-box API access), in-context PVI gives you a principled way to diagnose instance-level blind spots without training new heads or models. It‚Äôs useful for: (a) test-set triage‚Äîstress-testing on low-PVI (hard) examples; (b) data curation‚Äîprioritizing new labels where PVI is low; and (c) human-AI collaboration‚Äîrouting borderline cases to humans when the model‚Äôs usable information is small. It extends the PVI toolbox from the fine-tuning world into the prompt-engineering world, while staying faithful to the original theoretical framing of ùí±-usable information.</span></p><h2 class="appendix-breaker"><span class="appendix-index">A3</span><span class="appendix-title">Sparse Autoencoder Features for Classifications and Transferability</span></h2><p class="appendix-linkline"><span>Available here: </span><span><a href="https://aclanthology.org/2025.emnlp-main.1521/" rel="noopener noreferrer" target="_blank">https://aclanthology.org/2025.emnlp-main.1521/</a></span></p><p><span>Background</span><span> Hidden states in large language models are rich but notoriously entangled, which makes downstream use brittle and hard to audit. We turn to sparse autoencoders (SAEs) as a way to recover part-based, interpretable features from those states. Our question is pragmatic: if we replace raw hidden states with SAE activations, do we keep enough task signal to be competitive, and do those features transfer‚Äîacross models and across languages‚Äîso they‚Äôre actually useful beyond the lab settings?</span></p><p><span>Approach</span><span> We extract activations from released Gemma-scope SAEs (and keep the pipeline compatible with Neuropedia SAEs) and train simple linear probes for text classification. To stress-test usefulness, we run head-to-head comparisons of SAE features versus same-size slices of raw hidden states on 10+ challenging datasets. We then study two forms of transferability. (i) Model transfer: train on features from base or instruction-tuned LMs and apply them to LLaVA for classification without retraining the representation. (ii) Cross-lingual transfer: train a toxicity classifier on SAE features in one language and evaluate on other languages to see if the features carry over without translation. Throughout, we keep the recipe intentionally lightweight‚Äîno elaborate heads, minimal hyperparameter fuss‚Äîso any improvement is creditable to the representation rather than extra machinery.</span></p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image17.png" title=""/><figcaption>Figure: image17.</figcaption></figure><p><span>Results</span><span> Across difficult text classification tasks, SAE features consistently match or outpace baselines that use equivalently sized raw hidden-state slices, while remaining sparse and interpretable. For model transfer, features learned from base or instruction-tuned LMs plug into LLaVA and retain strong classification performance with no feature retraining. For cross-lingual toxicity, a probe trained on SAE features in a single language generalizes to other languages out of the box; in aggregate, it is competitive with ‚Äútranslate-to-English-then-classify‚Äù pipelines shown in our figures. Finally, because our pipeline treats any public SAE as a drop-in module, Neuropedia releases can be used immediately‚Äîno custom retraining required.</span></p><h2 class="appendix-breaker"><span class="appendix-index">A4</span><span class="appendix-title">When Models Reason in Your Language: Controlling Thinking Trace Language Comes at the Cost of Accuracy</span></h2><p class="appendix-linkline"><span>Available here: </span><span><a href="https://aclanthology.org/2025.findings-emnlp.1103/" rel="noopener noreferrer" target="_blank">https://aclanthology.org/2025.findings-emnlp.1103/</a></span></p><p><span>Background</span><span> As with many recent works‚Äîand our own prior results‚Äîwe see strong cross-lingual capabilities emerging in large language models. Test-time compute (longer sampling, multi-path CoT) is becoming mainstream for Large Reasoning Models (LRMs), yet the </span><span>language of the reasoning trace</span><span> itself remains underexplored outside English. For meaningful oversight and pedagogy, users need chains of thought in their own language. Our central question is whether enforcing in-language thinking traces improves transparency at a measurable cost in accuracy, and if so, how high that cost is across languages and model families.</span></p><p><span>Approach</span><span> We introduce XReasoning, a multilingual evaluation suite spanning 11 languages, and benchmark two leading LRM families. We measure: (i) how often models </span><span>actually</span><span> reason in the target language under ordinary prompting, (ii) how well they reason when we </span><span>explicitly push</span><span> them to keep the thinking trace in-language, and (iii) how various prompt interventions (e.g., stricter language constraints, formatting scaffolds) affect both language fidelity and task performance. To probe mitigations beyond prompting, we also run a small, targeted post-training (~500 examples) aimed at improving in-language reasoning without fully retraining the model.</span></p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image57.png" title=""/><figcaption>Figure: image57.</figcaption></figure><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image54.png" title=""/><figcaption>Figure: image54.</figcaption></figure><p><span>Results</span><span> In default conditions, SOTA LRMs frequently revert to English mid-reasoning or produce fragmented, mixed-language traces. When we force in-language CoT, readability and auditor access improve, but answer accuracy declines, revealing a persistent trade-off between language fidelity and task success. The small post-training top-up reduces (but does not eliminate) this gap, suggesting that lightweight tuning can reclaim part of the lost accuracy while keeping the reasoning trace aligned to the user‚Äôs language.</span></p><p><span>Implications </span><span>Practitioners should treat ‚Äúreason in the user‚Äôs language‚Äù as a distinct alignment objective with a non-zero accuracy tax‚Äîone that can be partially offset with judicious prompting and small, targeted post-training. In workflows where oversight, pedagogy, or localization matter, enforcing in-language thinking may be worth the modest loss in raw accuracy. In our follow-up (A5), we show techniques that move further toward getting both: higher in-language fidelity and stronger end-task performance.</span></p><h2 class="appendix-breaker"><span class="appendix-index">A5</span><span class="appendix-title">Budget Alignment: Making Models Reason in the User‚Äôs Language</span></h2><p class="appendix-linkline"><span>Available here: </span><span><a href="http://iclr.cc/virtual/2026/poster/10012111" rel="noopener noreferrer" target="_blank">http://iclr.cc/virtual/2026/poster/10012111</a></span></p><p><span>Background </span><span>Building directly on A4, we study the gap between answer language and reasoning language: many LLMs reply in the user‚Äôs language yet ‚Äúthink‚Äù in English/Chinese. This hurts instruction-following, user trust, multilingual evaluation, and oversight. Our aim is to align the </span><span>reasoning trace itself</span><span> to the user‚Äôs language without paying an accuracy tax.</span></p><p><span>Approach </span><span>We use a two-step recipe on DeepSeek-R1-Distill-Qwen-7B. First, a small SFT on 817 curated multilingual chains (LiMO-style) teaches </span><span>in-language chains of thought</span><span>‚Äîa minimal ‚Äúreprogramming‚Äù of inner monologue. Second, we apply Math500 training-only GRPO (RLVR/DAPO-like) with higher clip, rollout 24, LoRA r=8, lr=1e-5, and verifiable rewards that balance final-answer accuracy (1.0) with language-consistency (0.2) and format (0.2). We then evaluate JA/FR/ES on MMLU College Math, AIME‚Äô25, GPQA, and MMLU Pro Medicine, reporting pass@k (1/5/10, n=32) and language-consistency % (reasoning + final answer).</span></p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image35.png" title=""/><figcaption>Figure: image35.</figcaption></figure><p><span>Results </span><span>The SFT alone pushes language-consistency near ceiling in FR/ES and substantially lifts JA, with mixed or limited accuracy gains outside math. Adding GRPO after SFT typically Pareto-improves (higher accuracy </span><span>and</span><span> better in-language following) on AIME/GPQA, and in-domain SFT‚ÜíGRPO largely closes the accuracy gap between English-reasoning and in-language reasoning. The main exception is medicine, where math-only rewards under-specify biomedical style/recall and can cause regressions. Starting GRPO from the base model often exhibits smaller medical regressions, likely due to a more neutral prior than the SFT-steered one.</span></p><p><span>Why it works / where it fails</span><span> Under task difficulty, an entrenched EN/ZH reasoning prior ‚Äúwins‚Äù unless we intervene. The small multilingual SFT reliably locks the reasoning language, and GRPO then recovers or boosts accuracy. Failures trace to tokenization/numbering friction in JA, cue misalignment in ES, and especially reward misspecification for medicine‚Äînumeric correctness isn‚Äôt the same as calibrated clinical discourse.</span></p><p><span>Practical playbook </span><span>Use small multilingual SFT to secure the reasoning language, then GRPO to regain accuracy. When regressions appear, apply tokenizer-aware normalization, add tiny language-specific SFT top-ups, and switch to multi-objective GRPO that includes medical-style/lexicon rewards (and, when helpful, model merging). </span></p><p><span>In short: </span><span>lock the language cheaply, then train back the performance‚Äîwithout reverting to English under pressure.</span></p><h2 class="appendix-breaker"><span class="appendix-index">A6</span><span class="appendix-title">Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models</span></h2><p class="appendix-linkline"><span>Available here: </span><span><a href="https://neurips.cc/virtual/2025/loc/san-diego/poster/120231" rel="noopener noreferrer" target="_blank">https://neurips.cc/virtual/2025/loc/san-diego/poster/120231</a></span></p><p><span>Background </span><span>Modern Large Reasoning Models (LRMs) often generate a </span><span>thinking draft</span><span>‚Äîmulti-path chains of thought explored before the final answer. These drafts promise transparency and controllability, but only if they are </span><span>faithful</span><span>: intermediate steps should causally influence later ones, and the final answer should genuinely depend on the draft‚Äôs concluding logic. The paper argues that correctness alone is not enough; a model can get the right answer for the wrong reasons, undermining oversight and safety in high-stakes use. </span></p><p><span>We introduce a </span><span>systematic counterfactual intervention framework</span><span> to </span><span>measure</span><span> thinking-draft faithfulness along two complementary axes. (1) </span><span>Intra-Draft Faithfulness</span><span> tests whether earlier steps causally affect subsequent steps and the draft‚Äôs conclusion by inserting or perturbing intermediate steps and observing downstream changes. (2) </span><span>Draft-to-Answer Faithfulness</span><span> tests whether the final answer </span><span>logically depends on</span><span> the draft‚Äôs concluding logic by perturbing that conclusion and checking if the answer flips accordingly. The framework yields concrete, automatable tests rather than subjective judgments, enabling apples-to-apples comparisons across models. </span></p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image59.png" title=""/><figcaption>Figure: image59.</figcaption></figure><p><span>Results </span><span>Current LRMs display </span><span>selective step faithfulness</span><span>‚Äîsome intermediate steps matter while others can be altered with little effect‚Äîand </span><span>frequent mismatches</span><span> between a draft‚Äôs final logic and the model‚Äôs final answer. In other words, models often </span><span>ignore or contradict their own draft conclusions</span><span>, highlighting a gap between visible reasoning and the causal process that produced the answer. This reinforces prior concerns that chain-of-thought style traces can be unfaithful unless explicitly tested, and shows that the problem persists even for recent LRMs. </span></p><p><span>Implications</span><span> For practitioners, the takeaway is straightforward: monitoring should not stop at accuracy or surface-level coherence. You need </span><span>causal tests</span><span> that probe whether the model‚Äôs answer would change when the draft is perturbed in meaningful ways. The proposed interventions provide a practical blueprint for model evaluation suites, policy audits, and training-time regularizers that target faithfulness (complementing related work on counterfactual sensitivity and earlier CoT-faithfulness probes). The result is a more honest picture of when drafts can be trusted‚Äîand a path to improve them.</span></p><h2 class="appendix-breaker"><span class="appendix-index">A7</span><span class="appendix-title">KScope: A Framework for Characterizing the Knowledge Status of Language Models</span></h2><p class="appendix-linkline"><span>Available here: </span><span><a href="https://neurips.cc/virtual/2025/loc/san-diego/poster/119139" rel="noopener noreferrer" target="_blank">https://neurips.cc/virtual/2025/loc/san-diego/poster/119139</a></span></p><p><span>Background</span><span> LLMs can answer a question in multiple, sometimes conflicting ways. Traditional accuracy alone can‚Äôt reveal whether the model is certain, conflicted, or simply guessing. KScope introduces a principled way to label an LLM‚Äôs </span><span>knowledge status</span><span> from its response distribution, enabling targeted diagnosis and context design.</span></p><p><span>Approach</span><span> KScope samples an empirical distribution of model answers (stabilizing by ~100 samples using 20 paraphrases per question) and assigns one of five statuses using three axes: </span><span>consistency</span><span> (single vs. multi-mode beliefs), </span><span>correctness</span><span> (does any mode match gold), and </span><span>absent knowledge</span><span> (refusal, hallucination, or uniform guessing). The framework then studies how different </span><span>contexts</span><span> update status.</span></p><p><span>So what changes the knowledge status?</span></p><ul><li><span>Gold supporting context</span><span> reliably increases </span><span>consistent-correct</span><span> status across datasets and model families (Llama, Qwen, Gemma); larger models lead but gaps narrow with good context.</span></li><li><span>Noisy context</span><span> and </span><span>open-ended questions</span><span> reduce successful updates.</span></li><li><span>On model family quirks:</span><span> without context, Gemma shows more absent knowledge on open-ended than multi-choice; Llama/Qwen show the opposite.</span></li></ul><p><span><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image75.png" title=""/><figcaption>Figure: image75.</figcaption></figure></span><span>Which context features matter?</span><span> KScope evaluates 11 features in three groups‚Äî</span><span>difficulty</span><span> (length, readability, unique tokens), </span><span>relevance</span><span> (embedding similarity, ROUGE-2 R/P/F), and </span><span>familiarity</span><span> (question/context perplexity &amp; entropy)‚Äîand finds models prioritize features similarly when partially correct or in conflict; overturning strong false beliefs may need distinct signals.</span></p><p><span>What context strategies work? </span><span>A two-part recipe: (1) </span><span>Credibility</span><span> ‚Äî prefer sources with credible metadata; (2) </span><span>Constrained summarization</span><span> ‚Äî shorten while preserving semantics, token-level overlap, and fluency. Combined, this improves update success by </span><span>‚âà4.3% on average</span><span> and generalizes to GPT-4o.</span></p><p><span>Implications </span><span>KScope gives practitioners an interpretable lens on </span><span>why</span><span> an LLM succeeded or failed and </span><span>how</span><span> to nudge it‚Äîby picking contexts that are credible, relevant, concise, and familiar‚Äîrather than relying on accuracy alone.</span></p>