<h1>Chapter 3</h1><p class="chapter-subtitle"><span>The Effect of Using a Large Language Model to Respond to Patient Messages</span></p><hr/><p><span>Shan Chen</span><span>, Marco Guevara, Shalini Moningi, Frank Hoebers, Hesham Elhalawani, Benjamin H Kann, Fallon E Chipidza, Jonathan Leeman, Hugo JWL Aerts, Timothy Miller, Guergana K Savova, Jack Gallifant, Leo A Celi, Raymond H Mak, </span></p><p><span>Maryam Lustberg, Majid Afshar, Danielle S Bitterman</span></p><p class="chapter-meta"><a href="https://doi.org/10.1016/S2589-7500(24)00060-8" rel="noopener noreferrer" target="_blank"><em>Lancet Digital Health</em></a></p><h2>Background</h2><p><span>The integration of generalist large language models (LLMs) into clinical care promises to transform clinical workflows, potentially reducing the growing documentation burden facing healthcare practitioners. Such LLMs, including GPT-4, are currently being integrated into widely used electronic health record systems such as Epic, and are now being used to draft responses to patient messages. However, their efficiency, utility, safety, and human factors concerns have yet to be fully evaluated by physicians beyond current question-answer benchmarks.</span></p><h2>Methods</h2><p><span>This two-stage observational study was conducted in 2023 at Brigham and Women’s Hospital, Boston, MA. Six board-certified oncologists used GPT-4 to draft responses to 100 patient messages centered around realistic cancer patient scenarios. Responses were analyzed before and after physician editing for length and content. Physician surveys assessed the acceptability, utility, and safety of the LLM-drafted responses.</span></p><h2>Findings</h2><p><span>Physicians provided more concise responses compared to the LLM drafts and AI-assisted responses, averaging 34, 169, and 160 words, respectively. LLM drafts were deemed acceptable in 58.3% of cases, improved documentation efficiency in 76.9%, and were safe in 82.1%. However, there was a 7.7% risk of severe harm or death if used unedited. LLM-generated harmful content largely arose from incorrect assessment or communication of acuity, which has direct implications for patient outcomes if practitioners rely on LLM responses. Using an LLM draft significantly altered the content of messages compared to manual responses. </span></p><h2>Interpretation</h2><p><span>Our findings highlight the potential for using LLMs in patient communication workflows to improve physician efficiency and, potentially, patient education. Nonetheless, the risk of harm from unedited responses and dilution of direct clinical recommendations when LLMs are used with a human-in-the-loop highlight the need for careful oversight and continued evaluation of clinical and administrative workflows.</span></p><h2>Introduction</h2><p><span>Large language models (LLMs), celebrated for their transformative potential in various clinical settings, are not without their share of controversies, particularly concerns regarding bias and hallucination errors. These apprehensions have tempered their widespread adoption in high-stakes clinical decision-making. In contrast, less had been said of the quiet integration of LLMs into electronic health records (EHR) for supposedly lower-risk tasks that aim to address the clinician burnout crisis fueled by the extensive documentation burden.</span><span>1</span><span> This subtle integration, exemplified by partnerships like that between Microsoft/OpenAI and Epic, has positioned GPT-4 at the forefront of physician support tools such as care summarisation and patient communications. Such integration, while seemingly innocuous, carries implications not only for workforce well-being and retention, but also for patient care and safety.</span><span>2 </span></p><p><span>The relentless rise in administrative responsibilities, amplified by EHR systems, has diverted clinician attention from direct patient care, fueling burnout.</span><span>3</span><span> In response, LLMs are being adopted to streamline clinical and administrative tasks, and to enhance operational and financial efficiencies. Notably, Epic is currently leveraging OpenAI’s ChatGPT models, including GPT-4, for electronic messaging via online patient portals, a significant contributor to clinician burnout.</span><span>3–6</span><span> The volume of patient portal messaging has escalated in recent years,</span><span>7</span><span> and general-purpose LLMs, trained to respond to questions and participate in conversations fluently,</span><span>8</span><span> are being deployed to manage this growing burden. Their use in drafting responses to patient messages is one of the earliest applications in EHR systems.</span><span>5</span><span> Other current and near-future uses of LLMs in the clinic workflow include automated note creation, EHR navigation, billing support, and operations.</span><span>5,9,10</span><span> </span></p><p><span>Yet, the ability of LLMs to improve efficiency and reduce cognitive burden as part of the clinical workflow has not been established, and their impact on clinical decision-making and risks are unknown. Previous works have compared LLMs to human experts in answering questions about biomedical and clinical knowledge,</span><span>11–13</span><span> and also raised concerns about its varying responses accounting for social factors like race, insurance status, and gender.</span><span>14,15</span><span> Patient portal messaging is a form of patient care,</span><span>16</span><span> and represents one of medicine’s first encounters with the transformative potential of generative AI. While these LLMs are often being integrated with a human-in-the-loop, human factors such as automation bias and situational awareness could impact outcomes by altering clinicians’ cognitive processes in unexpected ways.</span><span>17–20</span></p><p><span>These critical questions remain unaddressed: Do these 'lower risk' administrative-focused applications of LLMs harbor similar pitfalls as their clinical counterparts? The hastened, large-scale deployment of LLMs in clinical settings, sans comprehensive evaluations of their performance, utility, and, crucially, safety, is a matter of significant concern. Indeed, we should learn from prior experience with disappointing and potentially harmful effects of rapid deployment of proprietary models within EHR systems: Epic widely implemented a sepsis prediction algorithm that was subsequently found on external validation to have low sensitivity compared to standard clinical practice and a large burden of alert fatigue.</span><span>21</span></p><p><span>This study seeks to bridge this knowledge gap, rigorously assessing the impact and safety of LLM-assisted patient messaging on healthcare delivery in an observational end-user study. We demonstrate that existing evaluations of LLM biomedical knowledge are insufficient to understand their clinical utility and risks because LLMs alter clinical decision-making in unexpected ways, and inappropriate LLM content arises largely from inadequate clinical acumen, not insufficient biomedical knowledgebase. This study serves as a call to action for a more measured approach to implementing LLMs within EHRs, including evaluations that reflect how they will actually be used in clinical settings and considerations of human factors.</span></p><h3>Overview of Study Design</h3><p><span>Figure 1 illustrates the overall study schema. We sought to understand how LLM assistance for electronic patient portal messaging as it is being implemented in EHRs, </span><span>i.e.</span><span> using an LLM to draft a response for a clinician to edit, impacts subjective efficiency, clinical recommendations, and potential harms. We therefore simulated how ChatGPT family models are being integrated into Epic systems for portal messaging. We simulated Epic’s GPT-4 implementation</span><span>5</span><span> due to Epic’s prominence: Epic is the leading EHR vendor with the most hospital-years in the United States,</span><span>22</span><span> and holds health records of more than 3% of people globally.</span><span>23</span><span> Because clinicians, and potentially the LLM depending on prompting approaches, would always have access to the patient’s EHR records to provide context, we included background clinical information along with the question. </span></p><p><span>Board-certified attending oncologists were first asked to respond to the patient messages as they normally would in clinical practice (“manual responses”), and then asked to edit GPT-4 responses (“LLM drafts”) so that they were a clinically acceptable response to send a patient (“LLM-assisted responses”). Surveys and content analysis of responses were used to evaluate the impact of LLM assistance on patient messaging. All datasets, including physician-written responses and evaluations, generated in this study are made publicly available. This study was approved by the Dana-Farber/Harvard Cancer Center IRB.</span></p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image76.png" title=""/><figcaption>Figure 1. Example of a scenario, patient message pair, and prompting methods used in this study. A) Prompt examples for creating scenario/message pairs. B) Prompt examples for creating responses from GPT4.</figcaption></figure><h3>Simulating Patient Portal Messages</h3><p><span>Cancer patient scenario/message pairs were generated to mirror realistic EHR system inbox messages. The scenario provided background information about a patient that is standardly available in the EHR and relevant to responding to cancer patient messages: age, gender, cancer diagnosis, past medical history, prior and current cancer treatments, current medications, and a summary of the most recent oncology visit (Figure 2). As above, these scenarios were needed for responses reflective of the real clinical setting where physicians have access to clinical information beyond just the patient’s question. They also served to encourage more substantive responses. While these exact summaries of a patient’s clinical history are not available in EHR messaging systems, in practice a clinician would have access to that patient’s full EHR records. In addition, depending on prompting approaches which vary across institutions, clinical data could be provided in the prompt the LLM.</span></p><p><span>Exemplars were written by a practicing oncologist (DSB), and the LLM GPT-4 was prompted via the OpenAI Application Programming Interface to provide similar examples (Figure 1A). To encourage diversity of patients' situations, exemplars were written to generate 50 scenario/question pairs of patients on active treatment, and 50 scenario/question pairs of patients not on active treatment. DSB then manually reviewed and revised each scenario to reflect clinically realistic and logical scenarios aligned with patient information needs. Messages were adjusted to reflect a range of common and high-priority questions received in patient portals. In a new instance, GPT-4 was prompted to respond to the message (Figure 1B). We chose to use GPT-4 to generate responses because this is the model being integrated to respond to patient messages per Epic announcements.</span><span>5</span><span> Appendix A provides more details on scenario development.</span></p><h3>Clinical Evaluation</h3><p><span>As shown in Figure 2, using the dataset of scenario/question pairs, we carried out a 2-stage observational study to investigate the impact and utility of LLM assistance for answering patient questions. Six board-certified attending oncologists were recruited to participate, and informed consent was obtained. In both stages, each participant evaluated 26 scenario/message pairs, yielding 56 dual-annotated cases and 44 single-annotated cases. During both stages, each physician received a different group of patient scenarios, and each of the 26 partitions were separated evenly.</span></p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image19.png" title=""/><figcaption>Figure 2. Schematic of study design. An oncologist generated and verified with GPT-4 assistance 100 patient clinical scenarios with paired patient questions in the format of portal messages. Participants manually wrote responses to scenarios in Stage I, and edited AI-assisted responses in Stage II. 56 scenario and message pairs were dual-annotated by 2 oncologists in both stages to assess inter-clinician variability.</figcaption></figure><p><span>In Stage I, physicians responded to messages manually, each followed by a 2-question survey on the perceived difficulty in crafting the response and the medical severity of each scenario. In Stage II, physicians received GPT-4 generated draft responses and were instructed to edit in any way they would like to generate a final LLM-assisted response. Physicians were not informed of how the draft responses were generated. Participants completed a 7-question survey after each response. Questions addressed: perceived difficulty in crafting responses, medical severity, acceptability of drafts, the potential harm of drafts, and perceived efficiency. Table 1 shows the survey questions. Examples of how the scenarios and surveys were presented to participants, along with instructions and real responses, are included in Appendix A.</span></p><h3>Assessing response pairs</h3><p><span>Word count was calculated and compared. Edit distance (measured using Levenshtein distance; details in Appendix A), which quantifies how much a response was altered, was calculated for each LLM draft/LLM-assisted response pair. </span></p><p><span>We then developed a framework for evaluating the content of responses to clinical questions. Responses were iteratively reviewed for clinically relevant content types based on clinical expertise and pilot annotations by an oncologist (DSB) until no additional content types were identified, resulting in 10 content categories (Appendix A). 50 (12%) responses were dual-annotated by content-based categorical evaluation by two physicians who did not participate in the 2-stage study (DSB and MA); Cohen’s kappa was </span><span>≥</span><span> 0.75 for all categories (Appendix A). The remaining responses were single-annotated by DSB.</span></p><h3>Statistical Analysis</h3><p><span>Statistical analyses were carried out using the statistical Python package in Scipy (Scipy.org). All pairwise comparisons were done using the Mann–Whitney U test.</span></p><h2>Results</h2><p><span>Manual responses were on average shorter than the LLM draft and LLM-assisted responses (34 vs. 169 and 160 words, respectively, p&lt;0.05 for all comparisons). Table 1 shows the Stage I and II survey results. Scenarios were considered neutral in terms of challenge in 85/156 (54.5%) and 82/156 (52.6%) of survey responses in Stage I and II, respectively. Scenarios were felt to describe severe medical events in 52/156 (33.3%) and 34/156 (21.8%) of survey responses in Stage I and II, respectively. LLM drafts were believed to be drafted by a human 48/156 (30.8%) of the time when, in reality, all drafted responses were drafted by the LLM. </span></p><p><span>Notably, 11/156 (7.1%) and 1/156 (0.6%) survey responses indicated that the LLM draft could lead to severe harm or death, respectively, if unedited. Appendix C shows these drafts, along with the error mode that led to the harmful content. The majority of harmful responses were due to incorrectly determining and/or conveying the acuity of the scenario and recommended actions. For example, in Appendix Example 12 where a patient describes new back pain and falls, the LLM does not identify that the provided information is sufficiently concerning for malignant spinal cord compression that requires urgent evaluation. In Example 7, where a patient describes new bloody diarrhea, the LLM appropriately recommends the patient reach out to their healthcare team. However, this recommendation appears at the end of a list of technically correct but lower priority, potentially distracting content, and the timing and specific details of who the patient should contact is vague. By contrast, the LLM-assisted response removes the extraneous content and recommends a phone or in-person visit to further assess. Other error modes included a lack of definiteness and clarity and incorrect diagnostic reasoning, largely arising from a failure to identify risks common in an oncology population but not a general population.</span></p><figure class="chapter-figure"><img alt="Thesis figure" decoding="async" loading="lazy" src="thesis-assets/images/image46.png" title=""/><figcaption>Figure 3. Distribution of content categories present in the manual, GPT-4 drafted, and AI-assisted responses to patient messages. Because 56/100 questions were dual-annotated, here we present results including one response from each of the dual-annotated scenarios. Results are similar when the other set of responses for dual-annotated scenarios are used (Supplemental Figure B2). Pairwise comparisons are done using Mann-Whitney U tests.</figcaption></figure><p><span>Edit distances were increased when the scenario was considered more challenging, the LLM draft was viewed as unacceptable, the LLM draft was associated with higher harm, and the LLM draft provided lower efficiency gain, indicating more extensive revisions of the draft in these settings (Table 1). Edit distance was higher when the draft was thought to be written by AI compared to when it was thought to be written by a human.</span></p><p><span>Inter-rater agreement between physicians for the content categories present in responses to the same scenario was lower for manual responses compared to LLM-assisted responses (mean Cohen’s kappa 0.10 vs. 0.52), indicating more consistent clinical content with the use of LLM assistance (Appendix B, Supplementary Table 1). </span></p><p><span>Figure 3 compares the content of manual, LLM draft, and LLM-assisted responses for the 100 scenarios. We present here comparisons that include only one set of manual responses and LLM-assisted responses for the 56 questions that were dual annotated. Results were very similar when the other set of responses were used (Appendix B, Supplementary Figure 1), and there was no difference in overall content distribution between these groups (p=0.77 and p=0.74 for manual and LLM-assisted responses, respectively). The overall content distribution of manual responses was significantly different than that of LLM drafts (p&lt;0.05) and LLM-assisted responses (p&lt;0.05); there was no difference in the content distribution of LLM drafts and LLM-assisted responses (p=0.81). </span></p><p><span>The most common category was Any Education for all responses. Compared to manual responses, LLM drafts were less likely to include content on direct clinical action, including instructing patients to present urgently or non-urgently for evaluation, and to describe an action the clinician will take in response to the question (p&lt;0.05 for all); but more likely to provide extensive education, self-management recommendations, and a contingency plan (p&lt;0.05 for all). These differences in content were similar when comparing manual versus LLM-assisted responses. LLM-assisted responses were less likely to recommend urgent evaluation, but this difference was significant when only one of the two sets of dual-annotated responses were analyzed. Appendix B, Supplementary Tables 2-3 show full details of the statistical analyses. The unigrams, bigrams, and trigrams that were most commonly added and removed from LLM drafts are illustrated in Appendix B, Supplementary Figure 2, and suggest that physicians were most likely to remove passive phrases such as “it is important” and more likely to add in actions such as “to come in”.</span></p><h2>Discussion</h2><p><span>The use of LLM assistance in patient portal messaging led to differences in response style and content compared to manual responses from board-certified attending physicians. Manual responses were nearly 5-fold shorter, while LLM-assisted responses reflected the longer, more verbose LLM drafts. Additionally, physicians frequently believed that LLM assistance improved efficiency of writing responses. The majority of LLM drafts were felt to have a low risk of harm to patients, although a clinically relevant minority were deemed to present a risk of severe harm or death if unedited. Our findings suggest that LLM assistance for patient messaging is a promising avenue to reduce clinician documentation burden through improved efficiency, but the significant differences in final message could have implications for patient safety if not appropriately safeguarded. </span></p><p><span>Our investigation contributes significantly to the burgeoning field of LLMs in clinical applications, notably by exploring their performance and impact on clinical decision-making within a human-machine collaborative framework. This study is pioneering in its evaluation of LLMs as assistive devices in real clinical workflows, specifically in the context of responding to patient portal messages. Our findings suggest that, when utilized appropriately, LLM assistance could offer a "best of both worlds" scenario, simultaneously reducing physician workload and enhancing the informativeness and educational value of responses. We discovered that LLM assistance has the potential to augment the quality of responses, furnishing detailed educational content about patient conditions and self-management plans, a task often infeasible for time-pressed clinicians. The high quality of the additional LLM-generated content is noteworthy, as our analysis indicates that drafts were generally acceptable and posed minimal risk of harm. Promisingly, we found that edit distances correlated with LLM draft quality, indicating physicians identified and were more likely to correct incorrect or harmful responses.</span></p><p><span>We also found that physicians tended to preserve the informative content generated by GPT-4, which was typically absent in manually crafted responses, while adding more direct instructions for patient evaluation. This approach reduced the variability in response content among different physicians, which could potentially elevate overall care quality. While previous studies have yielded mixed results when assessing unedited LLM responses to patient inquiries—some demonstrating acceptability akin to human responses</span><span>12</span><span> and others indicating high error rates</span><span>11</span><span>—these evaluations do not fully capture the dynamics of LLMs functioning in tandem with human clinicians. Our study fills this gap by examining LLMs in their intended role as a support tool for clinicians, mirroring their actual deployment in healthcare settings. This is crucial as other AI systems in healthcare have demonstrated varying performance levels when used in conjunction with human operators compared to autonomous operations.</span><span>24,25 </span><span>This highlights the need for careful monitoring, especially if and when trust in LLMs builds, and clinicians become less vigilant and more reliant on their responses.</span><span>20</span><span> Edit distance could be a promising metric to monitor both LLM quality and shifts in human-machine interactions. Clinician education, along with clear communication and reiteration of LLM error modes, will also be central to safe clinical implementation.</span></p><p><span>While the performance of LLMs was promising, we also validate concerns that LLM assistance carries real risks in its impact on clinical decision-making that need to be mitigated and monitored. Concerningly, when using LLM assistance, physicians were less likely to recommend a patient seek urgent care and say they would take a direct clinical action, such as prescribing a medication or diagnostic study. These differences in content are arguably the most immediately impactful for clinical outcomes, especially for high-acuity situations. </span></p><p><span>Our findings also underscore the influence of human factors such as automation bias and anchoring in human-in-the-loop systems,</span><span>17,18</span><span> illustrating unanticipated consequences that can arise and why these factors must be rigorously studied under their intended use.</span><span>26</span><span> Although most LLM-generated drafts were acceptable and posed low risk of harm, a minority of them, if left unedited, could lead to severe harm or even fatality. Intriguingly, the harmful content was mostly associated with inadequate recognition or communication of the scenario's acuity, rather than errors in biomedical knowledge. This indicates that while biomedical knowledge is crucial, it is insufficient on its own for clinical tasks. The nuanced and advanced clinical reasoning required for effective triage and communication, often acquired during post-graduate training, remains a formidable challenge for LLMs. This gap is likely due to the nature of LLM pre-training, which does not adequately capture this type of experiential knowledge. Thus, we posit that general-purpose LLMs like GPT-4 may continue to underperform in these areas without further refinement. Furthermore, assessments of encoded biomedical knowledge, such as performance on medical board exams are a first step toward clinical applications,</span><span>27</span><span> but should not be used as surrogates for the clinical expertise needed to care for patients. Clinical end-users should be made aware of this “blind spot” in LLM training and should actively engage with developers to help bridge this gap through new benchmarks and evaluation tools. </span></p><p><span>Furthermore, our study revealed that when LLM drafts contained erroneous biomedical knowledge, it often pertained to differential diagnoses that did not align with the specific risk profiles of patients. For example, in a patient with metastatic prostate cancer with burning left arm pain, hypertension was provided as a differential diagnosis, but metastases with bone or neural structure involvement were not mentioned. Given the incidence of cardiovascular disease in the general population, this might be reasonable in a situation where no specific clinical details about a given patient are known. However, it is less reflective of this individual’s specific risk profile. This error mode suggests that general-purpose LLM reasoning may mirror the distribution of clinical risk in the general population, rather than patient-specific risks, underscoring the potential need for specialty-specific models or tailored prompting methods. </span></p><p><span>Limitations of our study include that this was not performed using real patient data or within a real EHR system, and the use of LLM assistance may differ when physicians are responding to real questions. The focus of this study is on clinician perceptions because clinicians are the immediate end-users in the current intended use, and are the appropriate users for early safety evaluations in this stage of implementation. Understanding patients’ preferences and LLMs' impact on the patient-physician relationship will be an essential next step,</span><span>16</span><span> but safety must be urgently established and prioritized at this point of rapid clinical uptake in an evidence vacuum. Our study was limited to cancer patient questions and included only oncologists. Follow-on efforts will be needed, but the human factors and efficiency concerns are informative regardless of the clinician's specialty. Results may be different based on prompting methods and the type of LLM used.</span><span>28,29</span><span> We chose to study GPT-4 because Epic, a leading EHR vendor,</span><span>22</span><span> is implementing ChatGPT-family models including GPT-4 for LLM-assisted patient portal messaging.</span><span>5</span><span> More transparency is needed from EHR vendors and healthcare institutions about prompting methods and model versioning to enable future research. We did not generate data that included race or demographic information based on the desire to eliminate the introduction of additional variables that could affect the consistency of the outcomes. Previous research has demonstrated that the inclusion of factors such as race, gender, and insurance information can influence the consistency of outputs from large language models (LLMs),</span><span>14,30</span><span> and bias assessments and mitigation in LLM-assisted patient messaging must be prioritized.</span></p><p><span>In conclusion, our study emphasizes the critical need for thorough evaluation of LLMs in their intended clinical contexts, reflecting the precise clinical task and level of human oversight.</span><span>26</span><span> LLM assistance is a promising avenue to reduce clinician workload, but has clinical implications that merit regulation as software as a medical device. This situation necessitates treating LLMs with the same rigor in evaluation as any other software as a medical device or intervention. Physicians and institutions alike must exercise caution as the healthcare industry embraces these advanced technologies, as it is imperative to balance their innovative potential with a commitment to patient safety and care quality.</span></p><h3>Data Sharing Statement</h3><p><span>All data collected and generated in this study, after de-identification, are available to anyone who wishes to access the data for any purpose indefinitely at </span><span><a href="https://github.com/AIM-Harvard/OncQA" rel="noopener noreferrer" target="_blank">https://github.com/AIM-Harvard/OncQA</a></span><span>.</span><hr/></p><h3>Tables</h3><p><span>Table 1. Distribution of survey responses for Stage I and II of end-user study.</span></p><table class="chapter-table"><tr><th colspan="1" rowspan="2"><span>Survey Question</span></th><th colspan="2" rowspan="1"><span>Stage I</span></th><th colspan="2" rowspan="1"><span>Stage II</span></th></tr><tr><th colspan="1" rowspan="1"><span>Count (N=156)</span></th><th colspan="1" rowspan="1"><span>Percentage</span></th><th colspan="1" rowspan="1"><span>Count (N=156)</span></th><th colspan="1" rowspan="1"><span>Percentage</span></th></tr><tr><td colspan="5" rowspan="1"><span>Q1: How challenging was it to respond to this message?</span></td></tr><tr><td colspan="1" rowspan="1"><span>Neutral</span></td><td colspan="1" rowspan="1"><span>85</span></td><td colspan="1" rowspan="1"><span>54</span><span>.</span><span>5%</span></td><td colspan="1" rowspan="1"><span>82</span></td><td colspan="1" rowspan="1"><span>52</span><span>.</span><span>6%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Trivial/Very trivial</span></td><td colspan="1" rowspan="1"><span>46</span></td><td colspan="1" rowspan="1"><span>29</span><span>.</span><span>5%</span></td><td colspan="1" rowspan="1"><span>39</span></td><td colspan="1" rowspan="1"><span>25</span><span>.</span><span>0%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Challenging/Very challenging</span></td><td colspan="1" rowspan="1"><span>25</span></td><td colspan="1" rowspan="1"><span>16</span><span>.</span><span>0%</span></td><td colspan="1" rowspan="1"><span>35</span></td><td colspan="1" rowspan="1"><span>22</span><span>.</span><span>4%</span></td></tr><tr><td colspan="5" rowspan="1"><span>Q2: Do you believe this patient is experiencing a severe medical event?</span></td></tr><tr><td colspan="1" rowspan="1"><span>Disagree/Strongly disagree</span></td><td colspan="1" rowspan="1"><span>53</span></td><td colspan="1" rowspan="1"><span>34</span><span>.</span><span>0%</span></td><td colspan="1" rowspan="1"><span>61</span></td><td colspan="1" rowspan="1"><span>39</span><span>.</span><span>1%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Neither agree nor disagree</span></td><td colspan="1" rowspan="1"><span>51</span></td><td colspan="1" rowspan="1"><span>32</span><span>.</span><span>7%</span></td><td colspan="1" rowspan="1"><span>61</span></td><td colspan="1" rowspan="1"><span>39</span><span>.</span><span>1%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Agree/Strongly disagree</span></td><td colspan="1" rowspan="1"><span>52</span></td><td colspan="1" rowspan="1"><span>33</span><span>.</span><span>3%</span></td><td colspan="1" rowspan="1"><span>34</span></td><td colspan="1" rowspan="1"><span>21</span><span>.</span><span>8%</span></td></tr><tr><td colspan="5" rowspan="1"><span>Q3: </span><span>How would you rate the acceptability of the draft response?</span></td></tr><tr><td colspan="1" rowspan="1"><span>Accept without modifications</span></td><td colspan="2" rowspan="3"></td><td colspan="1" rowspan="1"><span>91</span></td><td colspan="1" rowspan="1"><span>58</span><span>.</span><span>3%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Accept with modifications</span></td><td colspan="1" rowspan="1"><span>38</span></td><td colspan="1" rowspan="1"><span>24</span><span>.</span><span>4%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Unacceptable (Major revision)</span></td><td colspan="1" rowspan="1"><span>27</span></td><td colspan="1" rowspan="1"><span>17</span><span>.</span><span>3%</span></td></tr><tr><td colspan="5" rowspan="1"><span>Q4: </span><span>How likely is it that the unedited draft response could cause harm?</span></td></tr><tr><td colspan="1" rowspan="1"><span>Low</span></td><td colspan="2" rowspan="4"></td><td colspan="1" rowspan="1"><span>128</span></td><td colspan="1" rowspan="1"><span>82</span><span>.</span><span>1%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Medium</span></td><td colspan="1" rowspan="1"><span>23</span></td><td colspan="1" rowspan="1"><span>14</span><span>.</span><span>7%</span></td></tr><tr><td colspan="1" rowspan="1"><span>High</span></td><td colspan="1" rowspan="1"><span>3</span></td><td colspan="1" rowspan="1"><span>1</span><span>.</span><span>9%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Missing</span></td><td colspan="1" rowspan="1"><span>2</span></td><td colspan="1" rowspan="1"><span>1</span><span>.</span><span>3%</span></td></tr><tr><td colspan="5" rowspan="1"><span>Q5: </span><span>If the unedited draft does cause harm, what would be the extent, or clinical impact on the patient?</span></td></tr><tr><td colspan="1" rowspan="1"><span>No harm</span></td><td colspan="2" rowspan="6"></td><td colspan="1" rowspan="1"><span>62</span></td><td colspan="1" rowspan="1"><span>39</span><span>.</span><span>7%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Mild harm</span></td><td colspan="1" rowspan="1"><span>52</span></td><td colspan="1" rowspan="1"><span>33</span><span>.</span><span>3%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Moderate harm</span></td><td colspan="1" rowspan="1"><span>25</span></td><td colspan="1" rowspan="1"><span>16</span><span>.</span><span>0%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Severe harm</span></td><td colspan="1" rowspan="1"><span>11</span></td><td colspan="1" rowspan="1"><span>7</span><span>.</span><span>1%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Death</span></td><td colspan="1" rowspan="1"><span>1</span></td><td colspan="1" rowspan="1"><span>0</span><span>.</span><span>6%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Missing</span></td><td colspan="1" rowspan="1"><span>5</span></td><td colspan="1" rowspan="1"><span>3</span><span>.</span><span>2%</span></td></tr><tr><td colspan="5" rowspan="1"><span>Q6: </span><span>Do you believe the provided unedited draft response improved your documentation efficiency?</span></td></tr><tr><td colspan="1" rowspan="1"><span>Agree</span></td><td colspan="2" rowspan="4"></td><td colspan="1" rowspan="1"><span>120</span></td><td colspan="1" rowspan="1"><span>76</span><span>.</span><span>9%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Neither agree nor disagree</span></td><td colspan="1" rowspan="1"><span>18</span></td><td colspan="1" rowspan="1"><span>11</span><span>.</span><span>5%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Disagree</span></td><td colspan="1" rowspan="1"><span>17</span></td><td colspan="1" rowspan="1"><span>10</span><span>.</span><span>9%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Missing</span></td><td colspan="1" rowspan="1"><span>1</span></td><td colspan="1" rowspan="1"><span>0</span><span>.</span><span>6%</span></td></tr><tr><td colspan="5" rowspan="1"><span>Q7: </span><span>Do you believe the provided draft response was written by an AI or by a human?</span></td></tr><tr><td colspan="1" rowspan="1"><span>AI</span></td><td colspan="2" rowspan="3"></td><td colspan="1" rowspan="1"><span>100</span></td><td colspan="1" rowspan="1"><span>64</span><span>.</span><span>1%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Human</span></td><td colspan="1" rowspan="1"><span>48</span></td><td colspan="1" rowspan="1"><span>30</span><span>.</span><span>8%</span></td></tr><tr><td colspan="1" rowspan="1"><span>Missing</span></td><td colspan="1" rowspan="1"><span>8</span></td><td colspan="1" rowspan="1"><span>5</span><span>.</span><span>1%</span></td></tr></table><p><span>Stage I surveys included only questions 1 and 2.</span><hr/></p><h2>References</h2><p><span>1 </span><span>Hswen Y, Voelker R. Electronic Health Records Failed to Make Clinicians’ Lives Easier-Will AI Technology Succeed? JAMA. 2023; published online Oct 4. DOI:10.1001/jama.2023.19138.</span></p><p><span>2 National Academies of Sciences, Engineering, and Medicine, National Academy of Medicine, Committee on Systems Approaches to Improve Patient Care by Supporting Clinician Well-Being. Taking Action Against Clinician Burnout: A Systems Approach to Professional Well-Being. National Academies Press, 2019.</span></p><p><span>3 Adler-Milstein J, Zhao W, Willard-Grace R, Knox M, Grumbach K. Electronic health records and burnout: Time spent on the electronic health record after hours and message volume associated with exhaustion but not with cynicism among primary care clinicians. </span><span>J Am Med Inform Assoc</span><span> 2020; </span><span>27</span><span>: 531–8.</span></p><p><span>4 Lieu TA, Altschuler A, Weiner JZ, </span><span>et al.</span><span> Primary Care Physicians’ Experiences With and Strategies for Managing Electronic Messages. </span><span>JAMA Netw Open</span><span> 2019; </span><span>2</span><span>: e1918287.</span></p><p><span>5 Epic and Microsoft bring GPT-4 to EHRs. https://www.epic.com/epic/post/epic-and-microsoft-bring-gpt-4-to-ehrs/ (accessed Dec 26, 2023).</span></p><p><span>6 Akbar F, Mark G, Prausnitz S, </span><span>et al.</span><span> Physician Stress During Electronic Health Record Inbox Work: In Situ Measurement With Wearable Sensors. </span><span>JMIR Med Inform</span><span> 2021; </span><span>9</span><span>: e24014.</span></p><p><span>7 Nath B, Williams B, Jeffery MM, </span><span>et al.</span><span> Trends in Electronic Health Record Inbox Messaging During the COVID-19 Pandemic in an Ambulatory Practice Network in New England. </span><span>JAMA Netw Open</span><span> 2021; </span><span>4</span><span>: e2131490.</span></p><p><span>8 Ouyang L, Wu J, Jiang X, </span><span>et al.</span><span> Training language models to follow instructions with human feedback. In: NeurIPS Proceesings. 2022. https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a0</span><span>01731-</span><span>Abstract-Conference.html.</span></p><p><span>9 Miliard M. Oracle Cerner adds generative AI to its EHR platforms. Healthcare IT News. 2023; published online Sept 19. https://www.healthcareitnews.com/news/oracle-cerner-adds-generative-ai-its-ehr-platforms (accessed Dec 28, 2023).</span></p><p><span>10 Microsoft News Center. Microsoft and Epic expand strategic collaboration with integration of Azure OpenAI Service. Stories. 2023; published online April 17. https://news.microsoft.com/2023/04/17/microsoft-and-epic-expand-strategic-collaboration-with-integration-of-azure-openai-service/ (accessed Dec 28, 2023).</span></p><p><span>11 Chen S, Kann BH, Foote MB, </span><span>et al.</span><span> Use of Artificial Intelligence Chatbots for Cancer Treatment Information. JAMA Oncol. 2023; </span><span>9</span><span>: 1459–62.</span></p><p><span>12 Ayers JW, Poliak A, Dredze M, </span><span>et al.</span><span> Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum. </span><span>JAMA Intern Med</span><span> 2023; </span><span>183</span><span>: 589–96.</span></p><p><span>13 Kanjee Z, Crowe B, Rodman A. Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge. </span><span>JAMA</span><span> 2023; </span><span>330</span><span>: 78–80.</span></p><p><span>14 Zack T, Lehman E, Suzgun M, </span><span>et al.</span><span> Assessing the potential of GPT-4 to perpetuate racial and gender biases in health care: a model evaluation study. </span><span>Lancet Digit Health</span><span> 2024; </span><span>6</span><span>: e12–22.</span></p><p><span>15 Nastasi AJ, Courtright KR, Halpern SD, Weissman GE. A vignette-based evaluation of ChatGPT’s ability to provide appropriate and equitable medical advice across care contexts. </span><span>Sci Rep</span><span> 2023; </span><span>13</span><span>: 17885.</span></p><p><span>16 Alpert JM, Markham MJ, Bjarnadottir RI, Bylund CL. Twenty-first Century Bedside Manner: Exploring Patient-Centered Communication in Secure Messaging with Cancer Patients. </span><span>J Cancer Educ</span><span> 2021; </span><span>36</span><span>: 16–24.</span></p><p><span>17 Sujan M, Furniss D, Grundy K, </span><span>et al.</span><span> Human factors challenges for the safe use of artificial intelligence in patient care. </span><span>BMJ Health Care Inform</span><span> 2019; </span><span>26</span><span>. DOI:10.1136/bmjhci-2019-100081.</span></p><p><span>18 Gianfrancesco MA, Tamang S, Yazdany J, Schmajuk G. Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data. </span><span>JAMA Intern Med</span><span> 2018; </span><span>178</span><span>: 1544–7.</span></p><p><span>19 </span><span>Lyell D, Coiera E. Automation bias and verification complexity: a systematic review. </span><span>J Am Med Inform Assoc</span><span> 2017; </span><span>24</span><span>: 423–31.</span></p><p><span>20 Cabitza F, Rasoini R, Gensini GF. Unintended Consequences of Machine Learning in Medicine. </span><span>JAMA</span><span> 2017; </span><span>318</span><span>: 517–8.</span></p><p><span>21 Wong A, Otles E, Donnelly JP, </span><span>et al.</span><span> External Validation of a Widely Implemented Proprietary Sepsis Prediction Model in Hospitalized Patients. </span><span>JAMA Intern Med</span><span> 2021; </span><span>181</span><span>: 1065–70.</span></p><p><span>22 Yuan N, Dudley RA, Boscardin WJ, Lin GA. Electronic health records systems and hospital clinical performance: a study of nationwide hospital data. </span><span>J Am Med Inform Assoc</span><span> 2019; </span><span>26</span><span>: 999–1009.</span></p><p><span>23 Adams K. 31 numbers that show how big Epic, Cerner, Allscripts &amp; Meditech are in healthcare. https://www.beckershospitalreview.com/healthcare-information-technology/31-numbers-that-show-how-big-epic-cerner-allscripts-meditech-are-in-healthcare.html (accessed Dec 31, 2023).</span></p><p><span>24 Walker SC, French B, Moore RP, </span><span>et al.</span><span> Model-Guided Decision-Making for Thromboprophylaxis and Hospital-Acquired Thromboembolic Events Among Hospitalized Children and Adolescents: The CLOT Randomized Clinical Trial. </span><span>JAMA Netw Open</span><span> 2023; </span><span>6</span><span>: e2337789.</span></p><p><span>25 Hosny A, Bitterman DS, Guthier CV, </span><span>et al.</span><span> Clinical validation of deep learning algorithms for radiotherapy targeting of non-small-cell lung cancer: an observational study. </span><span>Lancet Digit Health</span><span> 2022; </span><span>4</span><span>: e657–66.</span></p><p><span>26 Bitterman DS, Aerts HJWL, Mak RH. Approaching autonomy in medical artificial intelligence. </span><span>Lancet Digit Health</span><span> 2020; </span><span>2</span><span>: e447–9.</span></p><p><span>27 Singhal K, Azizi S, Tu T, </span><span>et al.</span><span> Large language models encode clinical knowledge. </span><span>Nature</span><span> 2023; </span><span>620</span><span>: 172–80.</span></p><p><span>28 Nori H, Lee YT, Zhang S, </span><span>et al.</span><span> Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine. arXiv [cs.CL]. 2023; published online Nov 28. http://arxiv.org/abs/2311.16452.</span></p><p><span>29 Chen S, Li Y, Lu S, </span><span>et al.</span><span> Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification. arXiv [cs.CL]. 2023; published online April 5. http://arxiv.org/abs/2304.02496.</span></p><p><span>30 Guevara M, Chen S, Thomas S, </span><span>et al.</span><span> Large Language Models to Identify Social Determinants of Health in Electronic Health Records. arXiv [cs.CL]. 2023; published online Aug 11. </span><span><a href="http://arxiv.org/abs/2308.06354" rel="noopener noreferrer" target="_blank">http://arxiv.org/abs/2308.06354</a></span><span>.</span></p><h3>Appendix A: Supplemental Method</h3><h3>Dataset Creation and Curation</h3><p><span>Cancer patient scenario/message pairs were generated to mirror realistic EHR system inbox messages. The scenario provided background information about a patient that is standardly available in the EHR and relevant to responding to cancer patient messages: age, gender, cancer diagnosis, past medical history, prior and current cancer treatments, current medications, and a summary of the most recent oncology visit (Figure 1). </span></p><p><span>These scenarios were needed for responses to simulate a real clinical setting where physicians have access to clinical information beyond just the patient’s question, and to encourage more substantive responses. </span><span>Exemplars</span><span> were written by a practicing oncologist (DSB), and GPT-4 was prompted via the OpenAI Application Programming Interface</span><span><sup class="citation-ref"><a class="citation-link" href="https://paperpile.com/c/snC9bN/IJmny" rel="noopener noreferrer" target="_blank">26</a></sup></span><span> to provide similar examples (Supplemental Figure A1). Computational linguists (CS) and oncologists (ML, DSB) collaborated to iteratively design prompts and select models across multiple AI systems, including Llama, Claude, GPT-Turbo, and GPT-4, to finalize the prompt design. </span></p><p><span>To encourage diversity of patients' situations, exemplars were written to generate 25 scenario/message pairs of patients on active treatment with descriptions of specific named chemotherapies (e.g., “pembrolizumab”), 25 scenario/message pairs of patients on active treatment with descriptions of general chemotherapies (e.g., “immunotherapy”), 25 scenario/message pairs of patients not on active treatment with descriptions of specific named chemotherapies, and 25 scenario/message pairs of patients not on active treatment with descriptions of general chemotherapies. DSB manually reviewed and edited the output to reflect clinically realistic and logical scenarios aligned with patient information needs. </span></p><p><span>All data </span><span>are</span><span> available on the project github: </span><span><a href="https://github.com/AIM-Harvard/OncQA" rel="noopener noreferrer" target="_blank">https://github.com/AIM-Harvard/OncQA</a></span><span>.</span></p><p><span>To see full details of these examples, please check: </span><span><a href="https://www.thelancet.com/cms/10.1016/S2589-7500(24)00060-8/attachment/11021699-8b45-4dde-8d52-71fb88876cd1/mmc1.pdf" rel="noopener noreferrer" target="_blank">https://www.thelancet.com/cms/10.1016/S2589-7500(24)00060-8/attachment/11021699-8b45-4dde-8d52-71fb88876cd1/mmc1.pdf</a></span></p>